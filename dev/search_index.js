var documenterSearchIndex = {"docs":
[{"location":"plot_model/#Basic-Example","page":"Plot Model Process","title":"Basic Example","text":"","category":"section"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"The function plot_model illustrates the evidence accumulation process of a given model. The code block below illustrates the decision dynamics of the racing diffusion model (RDM). ","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(77)\n\ndist = RDM()\ndensity_kwargs=(;t_range=range(.20, 1.0, length=100),)\nplot_model(dist; n_sim=1, density_kwargs, xlims=(0,1.0))","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"On each trial, the starting point z of the evidence accummulation process follows a uniform distribution: ","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"z sim mathrmUniform(0A)","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"The starting point distribution is represented as the height of the rectangle located at the origin of the plot above. Non-decision time tau, a constant representing the sum of percetual and motor processes, is represented as the width of the rectangle. The dashed horizontal line represents the decision threshold, alpha, which is defined as ","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"alpha = A + k","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"where k is the distance between the maximum starting point A and the threshold. The black lines extending from the starting point rectangle represent the noisy accumulation of evidence for each option. The accumulator whose evidence reaches the theshold first determines which option is selected.   ","category":"page"},{"location":"plot_model/#Add-Density-Plot","page":"Plot Model Process","title":"Add Density Plot","text":"","category":"section"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"In some cases, it is desirable to include the implied probability density of reaction times. The probability density can be included by setting the keyword add_density=true. By default, the probability density is rescaled to have a maximum value equal to the threshold alpha = A + k. Setting the keyword density_scale=nothing via density_kwargs will prevent rescaling. You may also pass your own desired maximum density value. ","category":"page"},{"location":"plot_model/","page":"Plot Model Process","title":"Plot Model Process","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(77)\n\ndist = RDM()\ndensity_kwargs=(;t_range=range(.20, 1.0, length=100),)\nplot_model(dist; n_sim=1, add_density=true, density_kwargs, xlims=(0,1.0))","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"using LinearAlgebra\nusing Plots\nusing SequentialSamplingModels\nusing Plots \nusing Random\nusing Revise\n\nmodel = CDDM(;\n    ν=[5,5.0],\n    η = [.50,.50],\n    σ = 1.0,\n    α = 4.0,\n    τ = .30\n)\n\nRandom.seed!(5874)\n\nplot_model(model)\nsavefig(\"cddm_plot.png\")","category":"page"},{"location":"cddm/#Circular-Drift-Diffusion-Model","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The Circular Drift Diffusion Model (CDDM; Smith, 2016) is a sequential sampling model for continuous responding on a circular domain. The CDDM is often used to model visual working memory. In these visual working memory tasks, subjects are briefly presented with a variable number of squares of different colors. After the stimuli are removed, subjects are prompted to use a color wheel to judge the color of a randomly selected square. Currently, the model is restricted to a 2D disk, but future versions may support modeling diffusion processes in hyperspheres. ","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The figure below illustrates the evidence accumulation process of the CDDM. At the begining of the trial, the evidence accumulation process starts at the center of the circle. As time progresses, the state of the system moves towards the the decision threshold depicted by the circle. Each step is perturbed with some degree of randomness. Once the system reaches the decision threshold, a response based on the position on the circle is given.   (Image: )","category":"page"},{"location":"cddm/#Example","page":"Circular Drift Diffusion Model (CDDM)","title":"Example","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"In this example, we will demonstrate how to use the CDDM in a generic two alternative forced choice task. ","category":"page"},{"location":"cddm/#Load-Packages","page":"Circular Drift Diffusion Model (CDDM)","title":"Load Packages","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"using LinearAlgebra\nusing SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(5874)","category":"page"},{"location":"cddm/#Create-Model-Object","page":"Circular Drift Diffusion Model (CDDM)","title":"Create Model Object","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"In the code below, we will define parameters for the CDDM and create a model object to store the parameter values. ","category":"page"},{"location":"cddm/#Drift-Rates","page":"Circular Drift Diffusion Model (CDDM)","title":"Drift Rates","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The mean drift rates boldsymbolnu control the speed with which information accumulates in the x and y direction.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"ν = [5.5,5.0]","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The magnitude of the mean drift rate vector boldsymbolnu is interpreted as the mean accumulation rate.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"norm(ν)","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The average direction of the accumulation process is given by mathrmarctan(fracnu_2nu_1):","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"atan(ν[2], ν[1])","category":"page"},{"location":"cddm/#Drift-Rate-Standard-Deviation","page":"Circular Drift Diffusion Model (CDDM)","title":"Drift Rate Standard Deviation","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The standard deviation of the drift rate boldsymboleta is inteprpreted as variability in the evidence accumulation across trials. ","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"η = [.50,.50]","category":"page"},{"location":"cddm/#Threshold","page":"Circular Drift Diffusion Model (CDDM)","title":"Threshold","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Evidence starts at the center of a circle (00) and terminates at a threshold defined by the circumference of the circle. The distance between the starting point and any point on the circumference is given by the radius alpha:","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"α = 4.0","category":"page"},{"location":"cddm/#Diffusion","page":"Circular Drift Diffusion Model (CDDM)","title":"Diffusion","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Intra-trial variability in the accumulation process is governed by parameter sigma","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"σ = 1.0","category":"page"},{"location":"cddm/#Non-Decision-Time","page":"Circular Drift Diffusion Model (CDDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"τ = 0.30","category":"page"},{"location":"cddm/#CDDM-Constructor","page":"Circular Drift Diffusion Model (CDDM)","title":"CDDM Constructor","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Now that values have been asigned to the parameters, we will pass them to CDDM to generate the model object.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"dist = CDDM(; ν, η, σ, α, τ)","category":"page"},{"location":"cddm/#Simulate-Model","page":"Circular Drift Diffusion Model (CDDM)","title":"Simulate Model","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.  The simulated data is a 2D array in which the first column contains the observed angular responses and the second column contains the corresponding reaction times.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":" data = rand(dist, 10_000)","category":"page"},{"location":"cddm/#Compute-PDF","page":"Circular Drift Diffusion Model (CDDM)","title":"Compute PDF","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"pdf(dist, data)","category":"page"},{"location":"cddm/#Compute-Log-PDF","page":"Circular Drift Diffusion Model (CDDM)","title":"Compute Log PDF","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"logpdf(dist, data)","category":"page"},{"location":"cddm/#Plot-Simulation","page":"Circular Drift Diffusion Model (CDDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"The code below overlays the PDF on the marginal histograms for angle and reaction time.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"histogram(dist)\nplot!(dist)","category":"page"},{"location":"cddm/#References","page":"Circular Drift Diffusion Model (CDDM)","title":"References","text":"","category":"section"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Smith, P. L. (2016). Diffusion theory of decision making in continuous report. Psychological Review, 123(4), 425.","category":"page"},{"location":"cddm/","page":"Circular Drift Diffusion Model (CDDM)","title":"Circular Drift Diffusion Model (CDDM)","text":"Smith, P. L., Garrett, P. M., & Zhou, J. (2023). Obtaining Stable Predicted Distributions of Response Times and Decision Outcomes for the Circular Diffusion Model.  Computational Brain & Behavior, 1-13.","category":"page"},{"location":"turing_advanced/#Estimate-Effect-on-Drift-Rate","page":"Advanced Model Specification","title":"Estimate Effect on Drift Rate","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"This advanced example illustrates how to estimate the effect of an experimental condition on the drift rate parameter. The drift rate could be manipulated in various ways. For example, the drift rate could be manipulated by varying the similarity of visual stimuli, or emphasizing speed or accuracy in task instructions.","category":"page"},{"location":"turing_advanced/#Generate-Data","page":"Advanced Model Specification","title":"Generate Data","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this example, we will get closer to real use-cases by starting with the data stored in a DataFrame. This dataframe will be a combination of data generated from two different distributions with different parameters, corresponding to two experimental conditions (e.g., Speed vs. Accuracy).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing Distributions\nusing DataFrames\nusing StatsPlots\nusing StatsModels\nusing KernelDensity\n\n# Generate data with different drifts for two conditions A vs. B\nRandom.seed!(6)\n\nn_obs = 50\ndf1 = DataFrame(rand(LBA(ν=[1.5, 0.5], A=0.5, k=0.2, τ=0.3), n_obs))\ndf2 = DataFrame(rand(LBA(ν=[2.5, 1.5], A=0.5, k=0.2, τ=0.3), n_obs))\ndf = vcat(df1, df2)\ndf.condition = repeat([\"A\", \"B\"], inner=n_obs)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"These 2 conditions A and B differ on their drift rates ([1.5, 0.5] vs. [2.5, 1.5]). In other words, the effect of condition B over condition A (the baseline condition, i.e., the intercept) is [1, 1] (because both drift rates increase by 1 between condition A and B).","category":"page"},{"location":"turing_advanced/#Exclude-Outliers","page":"Advanced Model Specification","title":"Exclude Outliers","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we are going to remove outliers, i.e., implausible RTs (RTs that likely do not reflect the processes of interest). In our case, we consider that RTs shorter than 0.2 seconds are too short for the cognitive process of interest to unfold, and that RTs longer than 3 seconds are too long to carry meaningful information.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Remove outliers\ndf = df[(df.rt .> 0.2).&(df.rt .< 3), :]\nfirst(df)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Note that standard outlier detection methods, such as z-scores (mean +- SD), are not necessarily appropriate for RTs, given the skewed nature of their distribution. Their asymmetric distribution is in fact accounted for by the models that we use. The outlier exclusion done here is more theory-driven (i.e., excluding extreme trials that likely do not reflect well the cognitive processes of interest) than data-driven (to better fit the model). That said, outlier exclusion should always be explicitly documented and justified.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"note: Note\nFor users coming from other languages, note the usage of the vectorization dot . in front of the < and > symbols. This means that we want to apply the logical test for all individual elements of the rt vector.","category":"page"},{"location":"turing_advanced/#Visualize-Data","page":"Advanced Model Specification","title":"Visualize Data","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can visualize the RT distribution for each response choice by looping through the conditions.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Make histogram\nhistogram(layout=(2, 1), xlabel=\"Reaction Time\", ylims=(0, 60), xlims=(0, 2), legend=false)\nfor (i, cond) in enumerate([\"A\", \"B\"])\n    histogram!(df.rt[(df.choice.==1).&(df.condition.==cond)], subplot=1, color=[:blue, :red][i], alpha=0.5, bins=range(0, 3, length=25))\n    histogram!(df.rt[(df.choice.==2).&(df.condition.==cond)], subplot=2, color=[:blue, :red][i], alpha=0.5, bins=range(0, 2, length=25))\nend\nplot!()","category":"page"},{"location":"turing_advanced/#Format-Predictors","page":"Advanced Model Specification","title":"Format Predictors","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"One additional step that we need to do here is to transform the dataframe into an input suited for modelling with Turing. For that, we will leverage the features of StatsModels to build an input matrix based on a formula.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Format input data\nf = @formula(rt ~ 1 + condition)\nf = apply_schema(f, schema(f, df))\n\n_, predictors = coefnames(f)\nX = modelmatrix(f, df)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this case, the model matrix is pretty simple: the key part is the second column that is simply a binary vector indicating whenever condition == \"B\". However, using formulas is a good way of dealing with more complex model specifications.","category":"page"},{"location":"turing_advanced/#Specify-Turing-Model","page":"Advanced Model Specification","title":"Specify Turing Model","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this model, the priors for the parameters that we want to vary between conditions are split, with one prior for their intercept (condition A) and another for the effect of condition B (relative to the intercept).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Because the drift parameters is a vector of length 2, the priors for both the intercept and condition effect drifts have themselves to be a vector of 2 distributions, which is done via filldist(prior_distribution, 2).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we need to specify these parameters as the result of a (linear) equation. Note that:","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We have added a keyword argument, condition, to let the user pass the condition data vector.\nSince we're computing parameters as the results of an equation, we need to use a for loop that loops through all the observations.\nBecause the priors for the drift is a filldist (i.e., a vector of distributions), we need to broadcast the addition (.+ instead of +).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"@model function model_lba(data; min_rt=0.2, condition=nothing)\n    # Priors for auxiliary parameters\n    A ~ truncated(Normal(0.8, 0.4), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    tau ~ Uniform(0.0, min_rt)\n\n    # Priors for coefficients\n    drift_intercept ~ filldist(Normal(0, 1), 2)\n    drift_condition ~ filldist(Normal(0, 1), 2)\n\n    for i in 1:length(data)\n        drifts = drift_intercept .+ drift_condition * condition[i]\n        data[i] ~ LBA(; τ=tau, A=A, k=k, ν=drifts)\n    end\nend","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Importantly, although we have the data as a dataframe, we will need to convert to a tuple, as it is the shape that the LBA() distribution expects. However, since we're iterating on each observation, we need to come up with an indexable version of the data: a vector of tuples.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Format the data to match the input type\ndata = [(choice=df.choice[i], rt=df.rt[i]) for i in 1:nrow(df)]","category":"page"},{"location":"turing_advanced/#Prior-Predictive-Check","page":"Advanced Model Specification","title":"Prior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/#Sample-from-Priors","page":"Advanced Model Specification","title":"Sample from Priors","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Before we fit the model, we want to inspect our priors to make sure that they are okay. To do that, we sample the model parameters from priors only. Note that condition is supplied as the 2nd column of the model matrix.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"chain = sample(model_lba(data; min_rt=minimum(df.rt), condition=X[:, 2]), Prior(), 1000)\nplot(chain; size=(800, 1200))","category":"page"},{"location":"turing_advanced/#Plot-Prior-Predictive-Check","page":"Advanced Model Specification","title":"Plot Prior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"The next step is to generate predictions from this model (i.e., from the priors). For this, we need to pass a dataset with empty (missing) values. Since the data used above was a vector (of tuples) of length 1000, we will create a vector of (missing) of the same length.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can then use the predict() method to generate predictions from this model. However, because the most of SequentialSamplingModels distributions return a tuple (choice and RT), the predicted output has the two types of variables mixed together. We can delineate the two by taking every 2nd values to get the predicted choice and RTs, respectively.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"datamissing = [(missing) for i in 1:nrow(df)]\n\npred = predict(model_lba(datamissing; min_rt=minimum(df.rt), condition=X[:, 2]), chain)\n\npriorpred_choice = Array(pred)[:, 1:2:end]\npriorpred_rt = Array(pred)[:, 2:2:end]","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"These objects have arrays of size 10,000 x 1000 : with 10,000 draws for each of the 1000 observations.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can plot the predicted distributions by looping through a number of draws (e.g., 100), and then plotting the density for each condition and each choice.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"plot(layout=(2, 1), xlabel=\"Reaction Time\", xlims = (0, 3), ylim=(0, 5), legend = false)\nfor i in 1:100\n    choice = priorpred_choice[i, :]\n    rt = priorpred_rt[i, :]\n    for (j, cond) in enumerate([0, 1])\n        U1 = kde(rt[(choice .== 1) .& (X[:, 2] .== cond)], boundary=(0, 5))\n        plot!(U1.x, U1.density, subplot=1, color = [:red, :blue][j], alpha=0.1)\n        U2 = kde(rt[(choice .== 2) .& (X[:, 2] .== cond)], boundary=(0, 5))\n        plot!(U2.x, U2.density, subplot=2, color = [:red, :blue][j], alpha=0.1)\n    end\nend\nplot!()","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can see that the bulk of the predicted RTs fall within 0 - 1.5 seconds, which is realistic, but that the same time it's all over the place, which means that the priors are not too informative.","category":"page"},{"location":"turing_advanced/#Parameters-Estimation","page":"Advanced Model Specification","title":"Parameters Estimation","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"chain = sample(model_lba(data; min_rt=minimum(df.rt), condition=X[:, 2]), NUTS(), 1000)\n\nsummarystats(chain)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"plot(chain; size = (800,1200))","category":"page"},{"location":"turing_advanced/#Posterior-Predictive-Check","page":"Advanced Model Specification","title":"Posterior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we will run a posterior predictive check by first sampling from the posteriors. For that, we will re-use the code for the prior predictive check, including the datamissing empty data.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Sample from posteriors\npred = predict(model_lba(datamissing; min_rt=minimum(df.rt), condition=X[:, 2]), chain)\npred_choice = Array(pred)[:, 1:2:end]\npred_rt = Array(pred)[:, 2:2:end]","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we will plot the predicted distributions on top of the observed distribution of data (the thick black lines).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Observed density\nplot(layout=(2, 1), xlabel=\"Reaction Time\", xlims=(0, 2.5), legend=false)\nfor cond in [\"A\", \"B\"]\n    d_A = kde(df.rt[(df.choice.==1).&(df.condition.==cond)], boundary=(0, 5))\n    plot!(d_A.x, d_A.density, subplot=1, color=:black, linewidth=3)\n    d_B = kde(df.rt[(df.choice.==2).&(df.condition.==cond)], boundary=(0, 5))\n    plot!(d_B.x, d_B.density, subplot=2, color=:black, linewidth=3)\nend\n\n# Predicted densities\nfor i in 1:100\n    choice = pred_choice[i, :]\n    rt = pred_rt[i, :]\n    for (j, cond) in enumerate([0, 1])\n        U1 = kde(rt[(choice.==1).&(X[:, 2].==cond)], boundary=(0, 5))\n        plot!(U1.x, U1.density, subplot=1, color=[:red, :blue][j], alpha=0.05)\n        U2 = kde(rt[(choice.==2).&(X[:, 2].==cond)], boundary=(0, 5))\n        plot!(U2.x, U2.density, subplot=2, color=[:red, :blue][j], alpha=0.05)\n    end\nend\nplot!()","category":"page"},{"location":"bayes_factor/#Computing-the-Bayes-Factor","page":"Model Comparison","title":"Computing the Bayes Factor","text":"","category":"section"},{"location":"bayes_factor/#Overview","page":"Model Comparison","title":"Overview","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"In this tutorial, we will use the Bayes factor to compare the evidence for one model relative to another reference model. Computing the Bayes factor is challenging because it requires integrating the log likelihood over the model parameters. One method for approximating this complex integral is non-reversible parallel tempering (Bouchard-Côté et al., 2022) using  Pigeons.jl. ","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"In the tutorial below, we will compare two models which differ only in terms of assumptions about drift rate variability: the LBA and the RDM. The LBA assumes that the drift rate varies across trials and is otherwise deterministic, whereas the RDM assumes the drift rate varies within a trial as Gaussian noise, but not across trials. The difference between the models can be visualized with Plots.jl:","category":"page"},{"location":"bayes_factor/#RDM","page":"Model Comparison","title":"RDM","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"using Random\nusing SequentialSamplingModels\nusing Plots","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"using SequentialSamplingModels\nusing Plots\nusing Random\nRandom.seed!(77)\n\ndist = RDM()\ndensity_kwargs=(;t_range=range(.20, 1.0, length=100),)\nplot_model(dist; n_sim=1, density_kwargs, xlims=(0,1.0))","category":"page"},{"location":"bayes_factor/#LBA","page":"Model Comparison","title":"LBA","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\ndist = LBA()\ndensity_kwargs=(;t_range=range(.20, 1.0, length=100),)\nplot_model(dist; n_sim=1, density_kwargs, xlims=(0,1.0))","category":"page"},{"location":"bayes_factor/#Load-Packages","page":"Model Comparison","title":"Load Packages","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"Before proceeding, we will load the required packages.","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"using LinearAlgebra\nusing Pigeons\nusing Random\nusing SequentialSamplingModels\nusing Turing","category":"page"},{"location":"bayes_factor/#Data-Generating-Model","page":"Model Comparison","title":"Data-Generating Model","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"The next step is to generate simulated data for comparing the models. Here, we will assume that the LBA is the true data generating model:","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"Random.seed!(654)\ndist = LBA(ν=[3.0, 2.0], A=0.8, k=0.2, τ=0.3)\ndata = rand(dist, 100)","category":"page"},{"location":"bayes_factor/#Define-Models","page":"Model Comparison","title":"Define Models","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"The following code blocks define the models along with their prior distributions using Turing.jl. Notice that the models are identical except for the log likelihood function.","category":"page"},{"location":"bayes_factor/#RDM-2","page":"Model Comparison","title":"RDM","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"@model function rdm(data; min_rt=minimum(data.rt))\n    ν ~ MvNormal(fill(2.0, 2), I * 3)\n    A ~ truncated(Normal(0.8, 0.8), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    τ ~ Uniform(0.0, min_rt)\n    data ~ RDM(; ν, A, k, τ)\nend","category":"page"},{"location":"bayes_factor/#LBA-2","page":"Model Comparison","title":"LBA","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"@model function lba(data; min_rt=minimum(data.rt))\n    ν ~ MvNormal(fill(2.0, 2), I * 3)\n    A ~ truncated(Normal(0.8, 0.8), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    τ ~ Uniform(0.0, min_rt)\n    data ~ LBA(; ν, A, k, τ)\nend","category":"page"},{"location":"bayes_factor/#Estimate-Marginal-Log-Likelihood","page":"Model Comparison","title":"Estimate Marginal Log Likelihood","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"The next step is to run the pigeons function to estimate the marginal log likelihood for each model. ","category":"page"},{"location":"bayes_factor/#LBA-3","page":"Model Comparison","title":"LBA","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"pt_lba = pigeons(target=TuringLogPotential(lba(data)), record=[traces])","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"────────────────────────────────────────────────────────────────────────────\n  #scans       Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) \n────────── ────────── ────────── ────────── ────────── ────────── ──────────\n        2        3.3      -24.5   4.43e-56      0.634          1          1 \n        4       1.88       42.2      0.331      0.791          1          1 \n        8       3.05       40.2     0.0393      0.661          1          1 \n       16       3.33       41.1      0.364       0.63          1          1 \n       32       3.05       41.3      0.396      0.662          1          1 \n       64       3.52       40.6      0.423      0.609          1          1 \n      128       3.26       41.4       0.56      0.638          1          1 \n      256       3.45       40.8      0.564      0.617          1          1 \n      512       3.48       40.8      0.578      0.613          1          1 \n 1.02e+03       3.33       40.9      0.596      0.629          1          1 \n────────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"bayes_factor/#RDM-3","page":"Model Comparison","title":"RDM","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"pt_rdm = pigeons(target=TuringLogPotential(rdm(data)), record=[traces])","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"────────────────────────────────────────────────────────────────────────────\n  #scans       Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) \n────────── ────────── ────────── ────────── ────────── ────────── ──────────\n        2       4.73       31.6   0.000606      0.475          1          1 \n        4       2.83       43.1       0.42      0.686          1          1 \n        8       3.05       39.8    0.00128      0.661          1          1 \n       16       3.46       41.2      0.268      0.615          1          1 \n       32       3.81       40.9      0.328      0.577          1          1 \n       64       3.16       40.6      0.404      0.649          1          1 \n      128       3.26       41.3      0.569      0.638          1          1 \n      256        3.3       40.6       0.56      0.633          1          1 \n      512       3.38       40.9       0.55      0.625          1          1 \n 1.02e+03       3.45       40.7      0.589      0.617          1          1","category":"page"},{"location":"bayes_factor/#Extract-marginal-log-likelihood","page":"Model Comparison","title":"Extract marginal log likelihood","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"In the following code block, the function stepping_stone extracts that marginal log likelihood:","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"mll_lba = stepping_stone(pt_lba)\nmll_rdm = stepping_stone(pt_rdm)","category":"page"},{"location":"bayes_factor/#Compute-the-Bayes-Factor","page":"Model Comparison","title":"Compute the Bayes Factor","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"The bayes factor is obtained by exponentiating the difference between marginal log likelihoods. The value of 1.21 indicates that the LBA is 1.21 times more likely to have generated the data. ","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"bf = exp(mll_lba - mll_rdm)","category":"page"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"1.2070298459526883","category":"page"},{"location":"bayes_factor/#References","page":"Model Comparison","title":"References","text":"","category":"section"},{"location":"bayes_factor/","page":"Model Comparison","title":"Model Comparison","text":"Syed, S., Bouchard-Côté, A., Deligiannidis, G., & Doucet, A. (2022). Non-reversible parallel tempering: a scalable highly parallel MCMC scheme. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(2), 321-350.","category":"page"},{"location":"Ratcliff_DDM/#Ratcliff-Diffusion-Model","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The Ratcliff Diffusion Model (Ratcliff DDM; Ratcliff et al., 2016) is similar to the DDM. Like the DDM, the model assumes that evidence accumulates over time, starting from a certain position, until it crosses one of two boundaries and triggers the corresponding response (Ratcliff & McKoon, 2008; Ratcliff & Rouder, 1998; Ratcliff & Smith, 2004). The drift rate (ν) determines the rate at which the accumulation process approaches a decision boundary, representing the relative evidence for or against a specific response. The distance between the two decision boundaries (referred to as the evidence threshold, α) influences the amount of evidence required before executing a response. Non-decision-related components, including perceptual encoding, movement initiation, and execution, are accounted for in the DDM and reflected in the τ parameter. Lastly, the model incorporates a bias in the evidence accumulation process through the parameter z, affecting the starting point of the drift process in relation to the two boundaries. The z parameter in DDM is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"However, the model differs in the inclusion of across-trial variability parameters. These parameters were developed to explain specific discrepancies between the DDM and experimental data (Anderson, 1960; Laming, 1968; Blurton et al., 2017). The data exhibited a difference in mean RT between correct and error responses that could not be captured by the DDM. As a result, two parameters for across-trial variability were introduced to explain this difference: across-trial variability in the starting point to explain fast errors (Laming, 1968), and across-trial variability in drift rate to explain slow errors (Ratcliff, 1978; Ratcliff and Rouder, 1998). Additionally, the DDM also showed a sharper rise in the leading edge of the response time distribution than observed in the data. To capture this leading edge effect, across-trial variability in non-decision time was introduced. ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Previous work has validated predictions of these across-trial variability parameters (Wagenmakers et al., 2009). When compared to the DDM, the Ratcliff DDM improves the fit to the data. Researchers now often assume that the core parameters of sequential sampling models, such as drift rates, non-decision times, and starting points vary between trials.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"One last parameter is the within-trial variability in drift rate (σ), or the diffusion coefficient. The diffusion coefficient is the standard deviation of the evidence accumulation process within one trial. It is a scaling parameter and by convention it is kept fixed. Following Navarro & Fuss, (2009), we use the σ = 1 version.","category":"page"},{"location":"Ratcliff_DDM/#Example","page":"Ratcliff Diffusion Model","title":"Example","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"In this example, we will demonstrate how to use the DDM in a generic two alternative forced choice task.","category":"page"},{"location":"Ratcliff_DDM/#Load-Packages","page":"Ratcliff Diffusion Model","title":"Load Packages","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The first step is to load the required packages.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"Ratcliff_DDM/#Create-Model-Object","page":"Ratcliff Diffusion Model","title":"Create Model Object","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"In the code below, we will define parameters for the DDM and create a model object to store the parameter values. ","category":"page"},{"location":"Ratcliff_DDM/#Drift-Rate","page":"Ratcliff Diffusion Model","title":"Drift Rate","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of drift rate. Standard deviation of a normal distribution with mean v describing the distribution of actual drift rates from specific trials. Values different from 0 can predict slow errors. Typical range: 0 < η < 2. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"ν=1.0\nη = 0.16","category":"page"},{"location":"Ratcliff_DDM/#Boundary-Separation","page":"Ratcliff Diffusion Model","title":"Boundary Separation","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The amount of information that is considered for a decision. Large values indicates response caution. Typical range: 0.5 < α < 2","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"α = 0.80","category":"page"},{"location":"Ratcliff_DDM/#Non-Decision-Time","page":"Ratcliff Diffusion Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of non-decisional components. Range of a uniform distribution with mean τ + st/2 describing the distribution of actual τ values across trials. Accounts for response times below t0. Reduces skew of predicted RT distributions. Typical range: 0 < τ < 0.2. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"τ = 0.30\nst = 0.10","category":"page"},{"location":"Ratcliff_DDM/#Starting-Point","page":"Ratcliff Diffusion Model","title":"Starting Point","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"An indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of starting point. Range of a uniform distribution with mean z describing the distribution of actual starting points from specific trials. Values different from 0 can predict fast errors. Typical range: 0 < sz < 0.5. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"z = 0.25\nsz = 0.05","category":"page"},{"location":"Ratcliff_DDM/#Ratcliff-Diffusion-Model-Constructor","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model Constructor","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Now that values have been assigned to the parameters, we will pass them to RatcliffDDM to generate the model object.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"dist = DDM(ν, α, τ, z)","category":"page"},{"location":"Ratcliff_DDM/#Simulate-Model","page":"Ratcliff Diffusion Model","title":"Simulate Model","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"Ratcliff_DDM/#Compute-PDF","page":"Ratcliff Diffusion Model","title":"Compute PDF","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"Ratcliff_DDM/#Compute-Log-PDF","page":"Ratcliff Diffusion Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"Ratcliff_DDM/#Plot-Simulation","page":"Ratcliff Diffusion Model","title":"Plot Simulation","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"# rts for option 1\nrts1 = rts[choices .== 1]\n# rts for option 2 \nrts2 = rts[choices .== 2]\n# probability of choosing 1\np1 = length(rts1) / length(rts)\nt_range = range(.30, 2, length=100)\n# pdf for choice 1\npdf1 = pdf.(dist, (1,), t_range)\n# pdf for choice 2\npdf2 = pdf.(dist, (2,), t_range)\n# histogram of retrieval times\nhist = histogram(layout=(2,1), leg=false, grid=false,\n     xlabel=\"Reaction Time\", ylabel=\"Density\", xlims = (0,1.5))\nhistogram!(rts1, subplot=1, color=:grey, bins = 200, norm=true, title=\"Choice 1\")\nplot!(t_range, pdf1, subplot=1, color=:darkorange, linewidth=2)\nhistogram!(rts2, subplot=2, color=:grey, bins = 150, norm=true, title=\"Choice 2\")\nplot!(t_range, pdf2, subplot=2, color=:darkorange, linewidth=2)\n# weight histogram according to choice probability\nhist[1][1][:y] *= p1\nhist[2][1][:y] *= (1 - p1)\nhist","category":"page"},{"location":"Ratcliff_DDM/#References","page":"Ratcliff Diffusion Model","title":"References","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Navarro, D., & Fuss, I. (2009). Fast and accurate calculations for first-passage times in Wiener diffusion models. https://doi.org/10.1016/J.JMP.2009.02.003","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922. https://doi.org/10.1162/neco.2008.12-06-420","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347–356. https://doi.org/10.1111/1467-9280.00067","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & Smith, P. L. (2004). A comparison of sequential sampling models for two-choice reaction time. Psychological Review, 111 2, 333–367. https://doi.org/10.1037/0033-295X.111.2.333","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Wagenmakers, E.-J. (2009). Methodological and empirical developments for the Ratcliff diffusion model of response times and accuracy. European Journal of Cognitive Psychology, 21(5), 641-671.","category":"page"},{"location":"layout/#Changing-the-Layout","page":"Changing the Layout","title":"Changing the Layout","text":"","category":"section"},{"location":"layout/","page":"Changing the Layout","title":"Changing the Layout","text":"As the example below demonstrates, densities are paneled according choice by default:","category":"page"},{"location":"layout/","page":"Changing the Layout","title":"Changing the Layout","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(85)\n\nν = [1.0,0.50]\nk = 0.50\nA = 1.0\nτ = 0.30\n\ndist = RDM(;ν, k, A, τ)\nplot(dist; t_range=range(.301, 2.5, length=100))","category":"page"},{"location":"layout/","page":"Changing the Layout","title":"Changing the Layout","text":"In some cases, one might prefer combining both density lines in the same plot. The code block below demonstrates how this can be achieved:","category":"page"},{"location":"layout/","page":"Changing the Layout","title":"Changing the Layout","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(85)\n\nν = [1.0,0.50]\nk = 0.50\nA = 1.0\nτ = 0.30\n\ndist = RDM(;ν, k, A, τ)\nt_range=range(.301, 2.5, length=100)\nplot(dist; \n    t_range, \n    layout=(1,1), \n    leg=true, \n    label=[\"1\" \"2\"], \n    color=[:blue :red], \n    title=\"\", \n    labeltitle=\"choice\"\n)","category":"page"},{"location":"ex_gaussian/#Ex-Gaussian-Model","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Model","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The Ex-Gaussian is the convolution of a Gaussian and exponential distribution sometimes used to model reaction time distributions:","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"mathrmrt sim mathrmnormal(musigma) + mathrmexponential(tau)","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"When the Ex-Gaussian was initially developed, some researchers thought that the Gaussian and exponential components represented motor and decision processes, respectively. More recent evidence casts doubt on this interpretation and shows that the parameters do not have a simple mapping to psychologically distinct processes in the drift diffusion model (Matzke & Wagenmakers, 2009). Perhaps this is unsurprising given that the models do not have the same number of parameters. Although the Ex-Gaussian is not technically a sequential sampling model, it is included in the package due to its historical role in reaction time modeling and its simple implementation. ","category":"page"},{"location":"ex_gaussian/#Example","page":"Ex-Gaussian Distribution","title":"Example","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"In this example, we will demonstrate how to use the Ex-Gaussian for a simulated detection task in which a stimulus appears and the subject responds as quickly as possible.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"ex_gaussian/#Load-Packages","page":"Ex-Gaussian Distribution","title":"Load Packages","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The first step is to load the required packages.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(21095)","category":"page"},{"location":"ex_gaussian/#Create-Model-Object","page":"Ex-Gaussian Distribution","title":"Create Model Object","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values.","category":"page"},{"location":"ex_gaussian/#Mean-of-Gaussian-Component","page":"Ex-Gaussian Distribution","title":"Mean of Gaussian Component","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The parameter mu represents the mean processing time in log space.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"μ = .80","category":"page"},{"location":"ex_gaussian/#Standard-Deviation-of-Gaussian-Component","page":"Ex-Gaussian Distribution","title":"Standard Deviation of Gaussian Component","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The parameter sigma represents the standard deviation of the Gaussian component.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"σ = .20","category":"page"},{"location":"ex_gaussian/#Mean-of-Exponential-Component","page":"Ex-Gaussian Distribution","title":"Mean of Exponential Component","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The parameter tau represents the mean of the exponential component.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"τ = 0.30","category":"page"},{"location":"ex_gaussian/#Ex-Gaussian-Constructor","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Constructor","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"Now that values have been assigned to the parameters, we will pass them to ExGaussian to generate the model object.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"dist = ExGaussian(μ, σ, τ)","category":"page"},{"location":"ex_gaussian/#Simulate-Model","page":"Ex-Gaussian Distribution","title":"Simulate Model","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"rts = rand(dist, 10_000)","category":"page"},{"location":"ex_gaussian/#Compute-PDF","page":"Ex-Gaussian Distribution","title":"Compute PDF","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"pdf.(dist, rts)","category":"page"},{"location":"ex_gaussian/#Compute-Log-PDF","page":"Ex-Gaussian Distribution","title":"Compute Log PDF","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"logpdf.(dist, rts)","category":"page"},{"location":"ex_gaussian/#Compute-CDF","page":"Ex-Gaussian Distribution","title":"Compute CDF","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The cumulative probability density Pr(T leq t) is computed by passing the model and a value t to cdf.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"cdf(dist, .4)","category":"page"},{"location":"ex_gaussian/#Plot-Simulation","page":"Ex-Gaussian Distribution","title":"Plot Simulation","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 2.5, length=100))","category":"page"},{"location":"ex_gaussian/#References","page":"Ex-Gaussian Distribution","title":"References","text":"","category":"section"},{"location":"ex_gaussian/","page":"Ex-Gaussian Distribution","title":"Ex-Gaussian Distribution","text":"Matzke, D., & Wagenmakers, E. J. (2009). Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis. Psychonomic bulletin & review, 16, 798-817.","category":"page"},{"location":"lba/#Linear-Ballistic-Accumulator","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The Linear Ballistic Accumulator (LBA; Brown & Heathcote, 2008) is a sequential sampling model in which evidence for options races independently. The LBA makes an additional simplification that evidence accumulates in a linear and ballistic fashion, meaning there is no intra-trial noise. Instead, evidence accumulates deterministically and linearly until it hits the threshold.","category":"page"},{"location":"lba/#Example","page":"Linear Ballistic Accumulator (LBA)","title":"Example","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"In this example, we will demonstrate how to use the LBA in a generic two alternative forced choice task. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"lba/#Load-Packages","page":"Linear Ballistic Accumulator (LBA)","title":"Load Packages","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The first step is to load the required packages.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lba/#Create-Model-Object","page":"Linear Ballistic Accumulator (LBA)","title":"Create Model Object","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"lba/#Mean-Drift-Rates","page":"Linear Ballistic Accumulator (LBA)","title":"Mean Drift Rates","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The drift rates control the speed with which evidence accumulates for each option. In the standard LBA, drift rates vary across trials according to a normal distribution with mean nu:","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"ν = [2.75,1.75]","category":"page"},{"location":"lba/#Standard-Deviation-of-Drift-Rates","page":"Linear Ballistic Accumulator (LBA)","title":"Standard Deviation of Drift Rates","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The standard deviation of the drift rate distribution is given by sigma, which is commonly fixed to 1 for each accumulator.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"σ = [1.0,1.0]","category":"page"},{"location":"lba/#Maximum-Starting-Point","page":"Linear Ballistic Accumulator (LBA)","title":"Maximum Starting Point","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The starting point of each accumulator is sampled uniformly between 0A.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"A = 0.80","category":"page"},{"location":"lba/#Threshold-Maximum-Starting-Point","page":"Linear Ballistic Accumulator (LBA)","title":"Threshold - Maximum Starting Point","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Evidence accumulates until accumulator reaches a threshold alpha = k +A. The threshold is parameterized this way to faciliate parameter estimation and to ensure that A le alpha.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"k = 0.50","category":"page"},{"location":"lba/#Non-Decision-Time","page":"Linear Ballistic Accumulator (LBA)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"τ = 0.30","category":"page"},{"location":"lba/#LBA-Constructor","page":"Linear Ballistic Accumulator (LBA)","title":"LBA Constructor","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Now that values have been asigned to the parameters, we will pass them to LBA to generate the model object.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"dist = LBA(; ν, A, k, τ) ","category":"page"},{"location":"lba/#Simulate-Model","page":"Linear Ballistic Accumulator (LBA)","title":"Simulate Model","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lba/#Compute-PDF","page":"Linear Ballistic Accumulator (LBA)","title":"Compute PDF","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"lba/#Compute-Log-PDF","page":"Linear Ballistic Accumulator (LBA)","title":"Compute Log PDF","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"lba/#Compute-Choice-Probability","page":"Linear Ballistic Accumulator (LBA)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"lba/#Plot-Simulation","page":"Linear Ballistic Accumulator (LBA)","title":"Plot Simulation","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"histogram(dist)\nplot!(dist; t_range=range(.3,2.5, length=100), xlims=(0, 2.5))\n","category":"page"},{"location":"lba/#References","page":"Linear Ballistic Accumulator (LBA)","title":"References","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Brown, S. D., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive psychology, 57(3), 153-178.","category":"page"},{"location":"lca/#Leaky-Competing-Accumulator","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The Leaky Competing Accumulator (LCA; Usher & McClelland, 2001) is a sequential sampling model in which evidence for options races independently. The LCA is similar to the Linear Ballistic Accumulator (LBA), but additionally assumes an intra-trial noise and leakage (in contrast, the LBA assumes that evidence accumulates in a ballistic fashion, i.e., linearly and deterministically until it hits the threshold).","category":"page"},{"location":"lca/#Example","page":"Leaky Competing Accumulator (LCA)","title":"Example","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"In this example, we will demonstrate how to use the LCA in a generic two alternative forced choice task. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"lca/#Load-Packages","page":"Leaky Competing Accumulator (LCA)","title":"Load Packages","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The first step is to load the required packages.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lca/#Create-Model-Object","page":"Leaky Competing Accumulator (LCA)","title":"Create Model Object","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"lca/#Drift-Rates","page":"Leaky Competing Accumulator (LCA)","title":"Drift Rates","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The drift rates control the speed with which information accumulates. Typically, there is one drift rate per option. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"ν = [2.5,2.0]","category":"page"},{"location":"lca/#Threshold","page":"Leaky Competing Accumulator (LCA)","title":"Threshold","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The threshold alpha represents the amount of evidence required to make a decision.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"α = 1.5","category":"page"},{"location":"lca/#Lateral-Inhibition","page":"Leaky Competing Accumulator (LCA)","title":"Lateral Inhibition","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The parameter beta inhibits evidence of competing options proportionally to their evidence value.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"β = 0.20","category":"page"},{"location":"lca/#Leak-Rate","page":"Leaky Competing Accumulator (LCA)","title":"Leak Rate","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The parameter lambda controls the rate with which evidence decays or \"leaks\".","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"λ = 0.10 ","category":"page"},{"location":"lca/#Diffusion-Noise","page":"Leaky Competing Accumulator (LCA)","title":"Diffusion Noise","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Diffusion noise is the amount of within trial noise in the evidence accumulation process. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"σ = 1.0","category":"page"},{"location":"lca/#Non-Decision-Time","page":"Leaky Competing Accumulator (LCA)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"τ = 0.30","category":"page"},{"location":"lca/#LCA-Constructor","page":"Leaky Competing Accumulator (LCA)","title":"LCA Constructor","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Now that values have been asigned to the parameters, we will pass them to LCA to generate the model object.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"dist = LCA(; ν, α, β, λ, τ, σ)","category":"page"},{"location":"lca/#Simulate-Model","page":"Leaky Competing Accumulator (LCA)","title":"Simulate Model","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"In the code block above, rand has a keyword argument Δt which controls the precision of the discrete approximation. The default value is Δt = .001.","category":"page"},{"location":"lca/#Compute-Choice-Probability","page":"Leaky Competing Accumulator (LCA)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"lca/#Plot-Simulation","page":"Leaky Competing Accumulator (LCA)","title":"Plot Simulation","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The code below plots a histogram for each option.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"histogram(dist)","category":"page"},{"location":"lca/#References","page":"Leaky Competing Accumulator (LCA)","title":"References","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Usher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108 3, 550–592. https://doi.org/10.1037/0033-295X.108.3.550","category":"page"},{"location":"lnr/#Lognormal-Race-Model","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The Lognormal Race model (LNR) assumes evidence for each option races independently and that the first passage time for each option is lognormally distributed. One way in which the LNR has been used is to provide a likelihood function for the ACT-R cognitive architecture. An example of such an application can be found in ACTRModels.jl. We will present a simplified version below.","category":"page"},{"location":"lnr/#Example","page":"Lognormal Race Model (LNR)","title":"Example","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"In this example, we will demonstrate how to use the LNR in a generic two alternative forced choice task.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"lnr/#Load-Packages","page":"Lognormal Race Model (LNR)","title":"Load Packages","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The first step is to load the required packages.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lnr/#Create-Model-Object","page":"Lognormal Race Model (LNR)","title":"Create Model Object","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values.","category":"page"},{"location":"lnr/#Mean-Log-Time","page":"Lognormal Race Model (LNR)","title":"Mean Log Time","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The parameter nu represents the mean processing time of each accumulator in log space.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"ν = [-1,-1.5]","category":"page"},{"location":"lnr/#Log-Standard-Deviation","page":"Lognormal Race Model (LNR)","title":"Log Standard Deviation","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The parameter sigma represents the standard deviation of processing time in log space.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"σ = [0.50,0.50]","category":"page"},{"location":"lnr/#Non-Decision-Time","page":"Lognormal Race Model (LNR)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Non-decision time is an additive constant representing encoding and motor response time.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"τ = 0.30","category":"page"},{"location":"lnr/#LNR-Constructor","page":"Lognormal Race Model (LNR)","title":"LNR Constructor","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Now that values have been asigned to the parameters, we will pass them to LNR to generate the model object.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"dist = LNR(ν, σ, τ)","category":"page"},{"location":"lnr/#Simulate-Model","page":"Lognormal Race Model (LNR)","title":"Simulate Model","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lnr/#Compute-PDF","page":"Lognormal Race Model (LNR)","title":"Compute PDF","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"lnr/#Compute-Log-PDF","page":"Lognormal Race Model (LNR)","title":"Compute Log PDF","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"lnr/#Compute-Choice-Probability","page":"Lognormal Race Model (LNR)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"To compute the joint probability of choosing c within t seconds, i.e., Pr(T leq t wedge C=c), pass a third argument for t.","category":"page"},{"location":"lnr/#Plot-Simulation","page":"Lognormal Race Model (LNR)","title":"Plot Simulation","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 1, length=100))","category":"page"},{"location":"lnr/#References","page":"Lognormal Race Model (LNR)","title":"References","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Heathcote, A., & Love, J. (2012). Linear deterministic accumulator models of simple choice. Frontiers in psychology, 3, 292.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Rouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015). The lognormal race: A cognitive-process model of choice and latency with desirable psychometric properties. Psychometrika, 80, 491-513.","category":"page"},{"location":"DDM/#Diffusion-Decision-Model","page":"Drift Diffusion Model (DDM)","title":"Diffusion Decision Model","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The Diffusion Decision Model (DDM; Ratcliff et al., 2016) is a model of speeded decision-making in two-choice tasks. The DDM assumes that evidence accumulates over time, starting from a certain position, until it crosses one of two boundaries and triggers the corresponding response (Ratcliff & McKoon, 2008; Ratcliff & Rouder, 1998; Ratcliff & Smith, 2004). Like other Sequential Sampling Models, the DDM comprises psychologically interpretable parameters that collectively form a generative model for reaction time distributions of both responses.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The drift rate (ν) determines the rate at which the accumulation process approaches a decision boundary, representing the relative evidence for or against a specific response. The distance between the two decision boundaries (referred to as the evidence threshold, α) influences the amount of evidence required before executing a response. Non-decision-related components, including perceptual encoding, movement initiation, and execution, are accounted for in the DDM and reflected in the τ parameter. Lastly, the model incorporates a bias in the evidence accumulation process through the parameter z, affecting the starting point of the drift process in relation to the two boundaries. The z parameter in DDM is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"One last parameter is the within-trial variability in drift rate (σ), or the diffusion coefficient. The diffusion coefficient is the standard deviation of the evidence accumulation process within one trial. It is a scaling parameter and by convention it is kept fixed. Following Navarro & Fuss, (2009), we use the σ = 1 version.","category":"page"},{"location":"DDM/#Example","page":"Drift Diffusion Model (DDM)","title":"Example","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"In this example, we will demonstrate how to use the DDM in a generic two alternative forced choice task.","category":"page"},{"location":"DDM/#Load-Packages","page":"Drift Diffusion Model (DDM)","title":"Load Packages","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"DDM/#Create-Model-Object","page":"Drift Diffusion Model (DDM)","title":"Create Model Object","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"In the code below, we will define parameters for the DDM and create a model object to store the parameter values. ","category":"page"},{"location":"DDM/#Drift-Rate","page":"Drift Diffusion Model (DDM)","title":"Drift Rate","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"ν=1.0","category":"page"},{"location":"DDM/#Boundary-Separation","page":"Drift Diffusion Model (DDM)","title":"Boundary Separation","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The amount of information that is considered for a decision. Large values indicates response caution. Typical range: 0.5 < α < 2","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"α = 0.80","category":"page"},{"location":"DDM/#Non-Decision-Time","page":"Drift Diffusion Model (DDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 ","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"τ = 0.30","category":"page"},{"location":"DDM/#Starting-Point","page":"Drift Diffusion Model (DDM)","title":"Starting Point","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"An indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"z = 0.50","category":"page"},{"location":"DDM/#DDM-Constructor","page":"Drift Diffusion Model (DDM)","title":"DDM Constructor","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Now that values have been assigned to the parameters, we will pass them to DDM to generate the model object.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"dist = DDM(ν, α, τ, z)","category":"page"},{"location":"DDM/#Simulate-Model","page":"Drift Diffusion Model (DDM)","title":"Simulate Model","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"DDM/#Compute-PDF","page":"Drift Diffusion Model (DDM)","title":"Compute PDF","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"DDM/#Compute-Log-PDF","page":"Drift Diffusion Model (DDM)","title":"Compute Log PDF","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"DDM/#Compute-Choice-Probability","page":"Drift Diffusion Model (DDM)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"cdf(dist, 1, 10)","category":"page"},{"location":"DDM/#Plot-Simulation","page":"Drift Diffusion Model (DDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 1, length=100))","category":"page"},{"location":"DDM/#References","page":"Drift Diffusion Model (DDM)","title":"References","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Navarro, D., & Fuss, I. (2009). Fast and accurate calculations for first-passage times in Wiener diffusion models. https://doi.org/10.1016/J.JMP.2009.02.003","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922. https://doi.org/10.1162/neco.2008.12-06-420","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347–356. https://doi.org/10.1111/1467-9280.00067","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & Smith, P. L. (2004). A comparison of sequential sampling models for two-choice reaction time. Psychological Review, 111 2, 333–367. https://doi.org/10.1037/0033-295X.111.2.333","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007","category":"page"},{"location":"wald_mixture/#Wald-Mixture-Model","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The Wald mixture model is a sequential sampling model for single choice decisions. It extends the Wald model by allowing the drift rate to vary randomly across trials. ","category":"page"},{"location":"wald_mixture/#Example","page":"Wald Mixture Model","title":"Example","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"In this example, we will demonstrate how to use the Wald mixture model in a generic single choice decision task. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"wald_mixture/#Load-Packages","page":"Wald Mixture Model","title":"Load Packages","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The first step is to load the required packages.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"wald_mixture/#Create-Model-Object","page":"Wald Mixture Model","title":"Create Model Object","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"In the code below, we will define parameters for the WaldMixture and create a model object to store the parameter values. ","category":"page"},{"location":"wald_mixture/#Drift-Rate","page":"Wald Mixture Model","title":"Drift Rate","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter nu represents the evidence accumulation rate.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"ν = 3.0","category":"page"},{"location":"wald_mixture/#Drift-Rate-Variability","page":"Wald Mixture Model","title":"Drift Rate Variability","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter eta represents the standard deviation of the evidence accumulation rate across trials.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"η = 0.20","category":"page"},{"location":"wald_mixture/#Threshold","page":"Wald Mixture Model","title":"Threshold","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter alpha the amount of evidence required to make a decision.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"α = 0.50","category":"page"},{"location":"wald_mixture/#Non-Decision-Time","page":"Wald Mixture Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"τ = 0.130","category":"page"},{"location":"wald_mixture/#Wald-Constructor","page":"Wald Mixture Model","title":"Wald Constructor","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Now that values have been asigned to the parameters, we will pass them to WaldMixture to generate the model object.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"dist = WaldMixture(ν, η, α, τ)","category":"page"},{"location":"wald_mixture/#Simulate-Model","page":"Wald Mixture Model","title":"Simulate Model","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"rts = rand(dist, 1000)","category":"page"},{"location":"wald_mixture/#Compute-PDF","page":"Wald Mixture Model","title":"Compute PDF","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"pdf.(dist, rts)","category":"page"},{"location":"wald_mixture/#Compute-Log-PDF","page":"Wald Mixture Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"logpdf.(dist, rts)","category":"page"},{"location":"wald_mixture/#Compute-CDF","page":"Wald Mixture Model","title":"Compute CDF","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The cumulative probability density Pr(T leq t) is computed by passing the model and a value t to cdf.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"cdf(dist, .4)","category":"page"},{"location":"wald_mixture/#Plot-Simulation","page":"Wald Mixture Model","title":"Plot Simulation","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The code below overlays the PDF on reaction time histogram.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"histogram(dist)\nplot!(dist; t_range=range(.130, 1, length=100))","category":"page"},{"location":"wald_mixture/#References","page":"Wald Mixture Model","title":"References","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Steingroever, H., Wabersich, D., & Wagenmakers, E. J. (2021). Modeling across-trial variability in the Wald drift rate parameter. Behavior Research Methods, 53, 1060-1076.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [SequentialSamplingModels]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#SequentialSamplingModels.AbstractDDM","page":"API","title":"SequentialSamplingModels.AbstractDDM","text":"AbstractDDM <: SSM2D\n\nAn abstract type for the drift diffusion model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractLBA","page":"API","title":"SequentialSamplingModels.AbstractLBA","text":"AbstractLBA <: SSM2D\n\nAn abstract type for the linear ballistic accumulator model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractLCA","page":"API","title":"SequentialSamplingModels.AbstractLCA","text":"AbstractLCA <: SSM2D\n\nAn abstract type for the leaky competing accumulator model\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractLNR","page":"API","title":"SequentialSamplingModels.AbstractLNR","text":"AbstractLNR <: SSM2D\n\nAn abstract type for the lognormal race model\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractMDFT","page":"API","title":"SequentialSamplingModels.AbstractMDFT","text":"AbstractMDFT <: SSM2D\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractMLBA","page":"API","title":"SequentialSamplingModels.AbstractMLBA","text":"AbstractMLBA <: AbstractLBA\n\nAn abstract type for the multi-attribute linear ballistic accumulator\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractPoissonRace","page":"API","title":"SequentialSamplingModels.AbstractPoissonRace","text":"AbstractPoissonRace <: SSM2D\n\nAn abstract type for the Poisson race model.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractRDM","page":"API","title":"SequentialSamplingModels.AbstractRDM","text":"AbstractRDM <: SSM2D\n\nAn abstract type for the racing diffusion model.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractWald","page":"API","title":"SequentialSamplingModels.AbstractWald","text":"AbstractWald <: SSM1D\n\nAn abstract type for the Wald model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractaDDM","page":"API","title":"SequentialSamplingModels.AbstractaDDM","text":"AbstractaDDM <: SSM2D\n\nAn abstract type for the attentional drift diffusion model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractstDDM","page":"API","title":"SequentialSamplingModels.AbstractstDDM","text":"AbstractstDDM <: SSM2D\n\nAn abstract type for the starting-time diffusion decision model.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.CDDM","page":"API","title":"SequentialSamplingModels.CDDM","text":"CDDM{T<:Real} <: AbstractCDDM\n\nA circular drift diffusion model (CDDM) for continous responding. CCDM is typically applied to continous report of color in visual working memory tasks. Currently supports the 2D case. \n\nParameters\n\nν: a vector drift rates. ν₁ is the mean drift rate along the x-axis; ν₂ is the mean drift rate along the y-axis.\nσ: intra-trial drift rate variability \nη: a vector across-trial standard deviations of  drift rates. η₁ is the standard deviation of drift rate along the x-axis;    ν₂ is the standard deviation of drift rate along the y-axis\nα: response boundary as measured by the radious of a circle \nτ: mean non-decision time \n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nCDDM(ν, σ, η, α, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nCDDM(; ν=[1,.5], η=[1,1], σ=1, α=1.5, τ=0.30)\n\nExample\n\nusing SequentialSamplingModels\ndist = CDDM(;ν=[1,.5], η=[1,1], σ=1, α=1.5, τ=0.30)\ndata = rand(dist, 10)\nlike = pdf(dist, data)\nloglike = logpdf(dist, data)\n\nReferences\n\nSmith, P. L. (2016). Diffusion theory of decision making in continuous report. Psychological Review, 123(4), 425.\n\nSmith, P. L., Garrett, P. M., & Zhou, J. (2023). Obtaining Stable Predicted Distributions of Response Times and Decision Outcomes for the Circular Diffusion Model.  Computational Brain & Behavior, 1-13.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.ClassicMDFT","page":"API","title":"SequentialSamplingModels.ClassicMDFT","text":"ClassicMDFT{T <: Real} <: AbstractMDFT\n\nA model type for Multiattribute Decision Field Theory. \n\nParameters\n\nσ = 1.0: diffusion noise \nα = 15.0: evidence threshold \nτ = .30: non-decision time\nw::Vector{T}: attention weights vector where each element corresponds to the attention given to the corresponding dimension\nS::Array{T, 2}: feedback matrix allowing self-connections and interconnections between alternatives. Self-connections range from zero to 1, where sij < 1 represents decay. Interconnections     between options i and j  where i ≠ j are inhibatory if sij < 0.\nC::Array{T, 2}: contrast weight matrix where c_ij is the contrast weight when comparing options i and j.\n\nConstructors\n\nClassicMDFT(σ, α, τ, w, S, C)\n\nClassicMDFT(σ, α, τ, w, S, C = make_default_contrast(S))\n\nExample\n\nAn example of the similarity effect. When choosing between options 1 and 2, the model predicts equal preference  because the options fall along the diagonal of attribute space, signifying a 1 to 1 trade-off of equally weighted  attributes. Option 3 is introduced to the choice set, which is similar to (and competitive with) option 1 and disimilar to option 2. In this case, the model predicts an increase the choice probability for option 2 relative to option 1.\n\n# value matrix where rows correspond to alternatives, and columns correspond to attributes\nM = [\n    1.0 3.0\n    3.0 1.0\n    0.9 3.1\n]\n\nmodel = ClassicMDFT(;\n    # non-decision time \n    τ = 0.300,\n    # diffusion noise \n    σ = 1.0,\n    # decision threshold\n    α = 17.5,\n    # attribute attention weights \n    w = [0.5, 0.5],\n    # feedback matrix \n    S = [\n        0.9500000 -0.0122316 -0.04999996\n        -0.0122316 0.9500000 -0.00903030\n        -0.0499996 -0.0090303 0.95000000\n    ],\n)\nchoices, rts = rand(model, 10_000, M; Δt = 1.0)\nmap(c -> mean(choices .== c), 1:3)\n\nReferences\n\nRoe, Robert M., Jermone R. Busemeyer, and James T. Townsend. \"Multiattribute Decision Field Theory: A dynamic connectionst model of decision making.\" Psychological review 108.2 (2001): 370.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.ContinuousMultivariateSSM","page":"API","title":"SequentialSamplingModels.ContinuousMultivariateSSM","text":"ContinuousMultivariateSSM <: ContinuousMultivariateDistribution\n\nAn abstract type for continuous multivariate sequential sampling models e.g., a circular drift diffusion model.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.DDM","page":"API","title":"SequentialSamplingModels.DDM","text":"DDM{T<:Real} <: SSM2D\n\nModel object for the standard Drift Diffusion Model.\n\nParameters\n\nν: drift rate. Average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5\nα: boundary threshold separation. The amount of information that is considered for a decision. Typical range: 0.5 < α < 2\nz: starting point. Indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).\nτ: non-decision time. The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 \n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nDDM(ν, α, z, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nDDM(; ν = 1.00, α = 0.80, τ = 0.30, z = 0.50)\n\nExample\n\nusing SequentialSamplingModels\ndist = DDM(ν = 1.0, α = 0.8, τ = 0.3, z = 0.25) \nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nRatcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.ExGaussian","page":"API","title":"SequentialSamplingModels.ExGaussian","text":"ExGaussian{T<:Real} <: SSM1D\n\nThe Ex-Gaussian is a convolution of the Gaussian and exponential distribution sometimes used  to model reaction time distributions. Note that this is not technically a sequential sampling model. \n\nParameters\n\nμ: mean of Gaussian component\nσ: standard deviation of Gaussian component\nτ: mean of exponential component\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nExGaussian(μ, σ, τ)\n\nThe second constructor uses kewords, and is not order dependent: \n\nExGaussian(;μ=.5, σ=.20, τ=.20)\n\nExample\n\nusing SequentialSamplingModels\ndist = ExGaussian(;μ=.5, σ=.20, τ=.20) \nrt = rand(dist, 10)\nlike = pdf.(dist, rt)\nloglike = logpdf.(dist, rt)\n\nReferences\n\nMatzke, D., & Wagenmakers, E. J. (2009). Psychological interpretation of the ex-Gaussian and shifted Wald parameters:  A diffusion model analysis. Psychonomic bulletin & review, 16, 798-817.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LBA","page":"API","title":"SequentialSamplingModels.LBA","text":"LBA{T<:Real} <: AbstractLBA\n\nA model object for the linear ballistic accumulator.\n\nParameters\n\nν::Vector{T}: a vector of drift rates\nσ::Vector{T}: a vector of drift rate standard deviation\nA::T: max start point\nk::T: A + k = b, where b is the decision threshold\nτ::T: an encoding-response offset\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nLBA(ν, σ, A, k, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nLBA(;τ=.3, A=.8, k=.5, ν=[2.0,1.75], σ=[1.0,1.0])\n\nExample\n\nusing SequentialSamplingModels\ndist = LBA(ν=[3.0,2.0], A = .8, k = .2, τ = .3) \nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nBrown, S. D., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive psychology, 57(3), 153-178.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LCA","page":"API","title":"SequentialSamplingModels.LCA","text":"LCA{T<:Real} <: AbstractLCA\n\nA model type for the Leaky Competing Accumulator. \n\nParameters\n\nν: drift rates \nσ: diffusion noise \nβ: lateral inhabition \nλ: leak rate\nα: evidence threshold \nτ: non-decision time \n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nLCA(ν, σ, β, λ, α, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nLCA(; ν = [2.5, 2.0], σ = 1.0, β = 0.20, λ = 0.10, α = 1.5, τ = 0.30)\n\nExample\n\nusing SequentialSamplingModels \nν = [2.5,2.0]\nα = 1.5\nβ = 0.20\nλ = 0.10 \nσ = 1.0\nτ = 0.30\n\ndist = LCA(; ν, α, β, λ, τ, σ)\nchoices,rts = rand(dist, 500)\n\nReferences\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108 3, 550–592. https://doi.org/10.1037/0033-295X.108.3.550\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LNR","page":"API","title":"SequentialSamplingModels.LNR","text":"LNR{T<:Real} <: AbstractLNR\n\nParameters\n\nν: a vector of means in log-space\nσ: a vector of standard deviation parameter in log-space\nτ: a encoding-response offset\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nLNR(ν, σ, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nLNR(; ν = [-1, -2], σ = fill(1.0, length(ν)), τ = 0.20)\n\nExample\n\nusing SequentialSamplingModels\ndist = LNR(ν=[-2,-3], σ=[1.0,1.0], τ=.3)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nRouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015).  The lognormal race: A cognitive-process model of choice and latency with desirable  psychometric properties. Psychometrika, 80(2), 491-513.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.MDFT","page":"API","title":"SequentialSamplingModels.MDFT","text":"MDFT{T <: Real} <: AbstractMDFT\n\nA model type for simulating Multi-attribute Decision Field Theory (MDFT) as an Stochastic Differential Equation (SDE). \n\nParameters\n\nσ = 1.0: diffusion noise \nα = 15.0: evidence threshold \nτ = .30: non-decision time\nγ::T: scales the valance, CMW, functioning like a drift rate\nκ::Vector{T}: exponential rate parameters for switching attention between attributes. Currently, limited to two    attributes \nϕ1: controls the sensitivity of lateral inhibition to distance in the distance function for creating the feedback matrix, S\nϕ2: controls evidence decay and maximum inhibition in the distance function for creating the feedback matrix, S\nβ: controls the weight of the dominance dimension in the feedback matrix distance function. If β < 0, the indifference dimension    recieves more where. If β > 0, the dominance dimension recieves more weight\nS::Array{T, 2}: feedback matrix allowing self-connections and interconnections between alternatives. Self-connections range from zero to 1, where sij < 1 represents decay. Interconnections     between options i and j where i ≠ j are inhibitory if sij < 0.\nC::Array{T, 2}: contrast weight matrix for comparing attended alternative to other alternatives.   The element c_ij is the contrast weight when comparing options i and j.\n\nConstructors\n\nMDFT(σ, α, τ, γ, κ, ϕ1, ϕ2, β, C)\n\nMDFT(;\n    n_alternatives,\n    σ,\n    α,\n    τ,\n    γ,\n    κ,\n    ϕ1,\n    ϕ2,\n    β,\n    C = make_default_contrast(n_alternatives)\n)\n\nExample\n\nAn example of the similarity effect. When choosing between options 1 and 2, the model predicts equal preference  because the options fall along the diagonal of attribute space, signifying a 1 to 1 trade-off of equally weighted  attributes. Option 3 is introduced to the choice set, which is similar to (and competitive with) option 1 and disimilar to option 2. In this case, the model predicts a reversal of preference between options 1 and 2.\n\nusing SequentialSamplingModels\n\nmodel = MDFT(;\n    n_alternatives = 3,\n    σ = 0.1,\n    α = .50,\n    τ = 0.0,\n    γ = 1.0,\n    κ = [6.0, 5.0],\n    ϕ1 = 0.01,\n    ϕ2 = 0.10,\n    β = 10.0\n)\n# value matrix where rows correspond to alternatives, and columns correspond to attributes\nM = [\n    1.0 3.0\n    3.0 1.0\n    0.9 3.1\n]\n\nchoices, rts = rand(model, 10_000, M)\nprobs = map(c -> mean(choices .== c), 1:3)\n\nReferences\n\nEvans, N. J., Holmes, W. R., & Trueblood, J. S. (2019). Response-time data provide critical constraints on dynamic models of multi-alternative, multi-attribute choice. Psychonomic Bulletin & Review, 26, 901-933.\n\nHotaling, J. M., Busemeyer, J. R., & Li, J. (2010). Theoretical developments in decision field theory: Comment on tsetsos, usher, and chater (2010). Psychological Review, 117 , 1294-1298.\n\nRoe, Robert M., Jermone R. Busemeyer, and James T. Townsend. \"Multi-attribute Decision Field Theory: A dynamic connectionst model of decision making.\" Psychological review 108.2 (2001): 370.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.MLBA","page":"API","title":"SequentialSamplingModels.MLBA","text":"MLBA{T <: Real} <: AbstractMLBA\n\nFields\n\nν::Vector{T}: a vector of drift rates, which is a function of β₀, λₚ, λₙ, γ\nβ₀::T: baseline input for drift rate \nλₚ::T: decay constant for attention weights of positive differences\nλₙ::T: decay constant for attention weights of negative differences  \nγ::T: risk aversion exponent for subjective values\nσ::Vector{T}: a vector of drift rate standard deviation\nA::T: max start point\nk::T: A + k = b, where b is the decision threshold\nτ::T: an encoding-response offset\n\nConstructors\n\nMLBA(ν, β₀, λₚ, λₙ, γ, σ, A, k, τ)\n\nMLBA(;\n    n_alternatives = 3,\n    ν = fill(0.0, n_alternatives),\n    β₀ = 5.0,\n    λₚ = 0.20,\n    λₙ = 0.40,\n    γ = 5.0,\n    τ = 0.3,\n    A = 1.0,\n    k = 1.0,\n    σ = fill(1.0, n_alternatives)\n)\n\nReferences\n\nTrueblood, J. S., Brown, S. D., & Heathcote, A. (2014). The multiattribute linear ballistic accumulator model of context effects in multialternative choice. Psychological Review, 121(2), 179.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.PoissonRace","page":"API","title":"SequentialSamplingModels.PoissonRace","text":"PoissonRace{T<:Real} <: AbstractPoissonRace\n\nParameters\n\nν: gamma scale parameter\nα: threshold\nτ: a encoding-response offset\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nPoissonRace(ν, α, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nPoissonRace(;ν=[.05,.06], α=[5,5], τ=.3)\n\nExample\n\nusing SequentialSamplingModels\ndist = PoissonRace(ν=[.05,.06], α=[5,5], τ=.3)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nLaBerge, D. A. (1962). A recruitment model of simple behavior. Psychometrika, 27, 375-395.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.RDM","page":"API","title":"SequentialSamplingModels.RDM","text":"RDM{T<:Real} <: AbstractRDM\n\nAn object for the racing diffusion model.\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nRDM(ν, k, A, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nRDM(;ν=[1,2], k=.3, A=.7, τ=.2)\n\nParameters\n\nν: a vector of drift rates\nA: the maximum starting point diffusion process, sampled from Uniform distribution\nk: k = b - A where b is the decision threshold, and A is the maximum starting point\nτ: a encoding-motor time offset\n\nExample\n\nusing SequentialSamplingModels\ndist = RDM(;ν=[1,2], k=.3, A=.7, τ=.2)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nTillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability:  The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27, 911-936.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.SSM1D","page":"API","title":"SequentialSamplingModels.SSM1D","text":"SSM1D <: ContinuousUnivariateDistribution\n\nAn abstract type for sequential sampling models characterized by a single choice reaction time distribution. Sub-types of SSM1D output a vector of reaction times.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.SSM2D","page":"API","title":"SequentialSamplingModels.SSM2D","text":"SSM2D = Distribution{Multivariate, Mixed}\n\nAn abstract type for sequential sampling models characterized by a multivariate choice-reaction time distribution. Sub-types of SSM2D output a NamedTuple consisting of a vector of choices and reaction times. \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.Wald","page":"API","title":"SequentialSamplingModels.Wald","text":"Wald{T<:Real} <: AbstractWald\n\nA model object for the Wald model, also known as the inverse Gaussian model.\n\nParameters\n\nν: drift rate\nα: decision threshold\nτ: a encoding-response offset\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nWald(ν, α, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nWald(;ν=1.5, α=.50, τ=0.20)\n\nExample\n\nusing SequentialSamplingModels\ndist = Wald(ν=3.0, α=.5, τ=.130)\nrt = rand(dist, 10)\nlike = pdf.(dist, rt)\nloglike = logpdf.(dist, rt)\n\nReferences\n\nAnders, R., Alario, F., & Van Maanen, L. (2016). The shifted Wald distribution for response time data analysis. Psychological methods, 21(3), 309.\n\nFolks, J. L., & Chhikara, R. S. (1978). The inverse Gaussian distribution and its statistical application—a review. Journal of the Royal Statistical Society Series B: Statistical Methodology, 40(3), 263-275.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.WaldMixture","page":"API","title":"SequentialSamplingModels.WaldMixture","text":"WaldMixture{T<:Real} <: AbstractWald\n\nParameters\n\nυ: drift rate\nη: standard deviation of drift rate\nα: decision threshold\nτ: a encoding-response offset\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nWaldMixture(ν, η, α, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nWaldMixture(;ν=3.0, η=.2, α=.5, τ=.130)\n\nExample\n\nusing SequentialSamplingModels\ndist = WaldMixture(;ν=3.0, η=.2, α=.5, τ=.130)\nrt = rand(dist, 10)\nlike = pdf.(dist, rt)\nloglike = logpdf.(dist, rt)\n\nReferences\n\nSteingroever, H., Wabersich, D., & Wagenmakers, E. J. (2020).  Modeling across-trial variability in the Wald drift rate parameter.  Behavior Research Methods, 1-17.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.aDDM","page":"API","title":"SequentialSamplingModels.aDDM","text":"aDDM{T<:Real} <: AbstractaDDM\n\nAn object for the attentional diffusion model. \n\nParameters\n\nν: relative decision values (i.e., drift rates)\nσ: standard deviation of noise in evidence accumulation\nΔ: constant of evidence accumulation speed (evidence per ms)\nα: evidence threshold \nz: initial evidence \nθ: bias towards attended alternative (lower indicates more bias)\nτ: non-decision time\n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\naDDM(ν, σ, Δ, θ, α, z, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\naDDM(;ν=[5.0,4.0], α=1.0, z=α*.5, θ=.3, σ=.02, Δ=.0004, τ=0.0)\n\nExample\n\nusing SequentialSamplingModels\nusing StatsBase\n\nmutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\n function Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end\n \n function fixate(transition)\n     (;mat,n,state) = transition\n     w = @view mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end\n\n model = aDDM()\n \n tmat = Transition([.98 .015 .005;\n                    .015 .98 .005;\n                    .45 .45 .1])\n\n choices,rts = rand(model, 100, tmat; fixate)\n\nReferences\n\nKrajbich, I., Armel, C., & Rangel, A. (2010). Visual fixations and the computation and comparison of  value in simple choice. Nature neuroscience, 13(10), 1292-1298.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.maaDDM","page":"API","title":"SequentialSamplingModels.maaDDM","text":"maaDDM{T<:Real} <: AbstractaDDM\n\nAn object for the multi-attribute attentional drift diffusion model. \n\nConstructors\n\nTwo constructors are defined below. The first constructor uses positional arguments, and is therefore order dependent:\n\nmaaDDM(ν, σ, Δ, θ, ϕ, ω, α, z, τ)\n\nThe second constructor uses keywords with default values, and is not order dependent: \n\nmaaDDM(;\n    ν = [4.0 5.0; 5.0 4.0],\n    α = 1.0,\n    z = 0.0,\n    θ = 0.3,\n    ϕ = 0.50,\n    ω = 0.70,\n    σ = 0.02,\n    Δ = 0.0004,\n    τ = 0.0,\n)\n\nIn this version of the model, the non-attended attribute of the non-attended alternative is doubly discounted. For example, the mean drift rate for the attribute 1 of alternative 1 is given by:\n\n    Δ * (ω * (ν[1,1] - θ * ν[2,1]) + (1 - ω) * ϕ * (ν[1,2] - θ * ν[2,2]))\n\nKeywords\n\nν: drift rates where rows are alternatives and columns are attributes\nσ: standard deviation of noise in evidence accumulation\nΔ: constant of evidence accumulation speed (evidence per ms)\nθ: bias away from unattended alternative (lower indicates more bias)\nϕ: bias away from unattended attribute \nω: attribute weight\nα: evidence threshold \nz: initial evidence \nτ: non-decision time\n\nExample\n\nusing SequentialSamplingModels\nusing StatsBase\n\nmutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\n function Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end\n \n function attend(transition)\n     (;mat,n,state) = transition\n     w = @view mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end\n\nν = [4.0 5.0; 5.0 4.0]\nα = 1.0 \nz = 0.0\nθ = .3\nϕ = .50\nω = .70\nσ = .02\nΔ = .0004\nτ = 0.0\n\ndist = maaDDM(; ν, σ, Δ, θ, ϕ, ω, α, z, τ)\n\ntmat = Transition([.98 .015 .0025 .0025;\n                .015 .98 .0025 .0025;\n                .0025 .0025 .98 .015;\n                .0025 .0025 .015 .98])\n\n choices,rts = rand(dist, 100, attend, tmat)\n\nReferences\n\nYang, X., & Krajbich, I. (2023). A dynamic computational model of gaze and choice in multi-attribute decisions.  Psychological Review, 130(1), 52.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractLCA, Int64}","page":"API","title":"Base.rand","text":"rand(dist::AbstractLCA, n_sim::Int; Δt = 0.001)\n\nGenerate n_sim random choice-rt pairs for the Leaky Competing Accumulator.\n\nArguments\n\ndist: model object for the Leaky Competing Accumulator.\nn_sim::Int: the number of simulated choice-rt pairs \n\nKeywords\n\nΔt = 0.001: time step size\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractLCA}","page":"API","title":"Base.rand","text":"rand(dist::AbstractLCA; Δt = 0.001)\n\nGenerate a random choice-rt pair for the Leaky Competing Accumulator.\n\nArguments\n\ndist: model object for the Leaky Competing Accumulator. \nΔt = 0.001: time step size \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractMLBA, AbstractArray}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, d::AbstractMLBA, M::AbstractArray)\n\nGenerates a single choice-rt pair of simulated data from the Multi-attribute Linear Ballistic Accumulator.\n\nArguments\n\ndist::AbstractMLBA: an object for the multi-attribute linear ballistic accumulator\nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractMLBA, Int64, AbstractArray}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, d::AbstractMLBA, n_trials::Int, M::AbstractArray)\n\nGenerates n_trials choice-rt pair of simulated data from the Multi-attribute Linear Ballistic Accumulator.\n\nArguments\n\ndist::AbstractMLBA: an object for the multi-attribute linear ballistic accumulator\nn_trials::Int: the number of trials to simulate\nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractaDDM, Int64, Vararg{Any}}","page":"API","title":"Base.rand","text":"rand(\n    rng::AbstractRNG,\n    dist::AbstractaDDM,\n    n_sim::Int,\n    fixate::Function, \n    args...;\n    rand_state! = _rand_state!,\n    Δt = .001,\n    kwargs...\n)\n\nGenerate n_sim simulated trials from the attention diffusion model.\n\nArguments\n\nrng: a random number generator\ndist: an attentional diffusion model object\nn_sim::Int: the number of simulated trials\nfixate: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs...: optional positional arguments for the fixate function\n\nKeywords\n\nrand_state! = _rand_state!: initialize first state with equal probability \n\nkwargs...: optional keyword arguments for the fixate function\nΔt = .001: time step\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractaDDM, Vararg{Any}}","page":"API","title":"Base.rand","text":"rand(\n    rng::AbstractRNG, \n    dist::AbstractaDDM, \n    fixate::Function, args...; \n    rand_state! = _rand_state!, \n    Δt = .001,\n    kwargs...\n)\n\nGenerate a single simulated trial from the attentional diffusion model.\n\nArguments\n\nrng: a random number generator\ndist: an attentional diffusion model object\nfixate: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs...: optional positional arguments for the fixate function\n\nKeywords\n\nkwargs...: optional keyword arguments for the fixate function\nΔt = .001: time step\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractstDDM}","page":"API","title":"Base.rand","text":"rand(dist::AbstractstDDM)\n\nGenerate a random choice-rt pair for starting-time diffusion decision model.\n\nArguments\n\nrng: a random number generator\ndist: model object for the starting-time diffusion decision model. \nΔt: time-step for simulation\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, ClassicMDFT, Int64, AbstractArray}","page":"API","title":"Base.rand","text":"rand(\n    rng::AbstractRNG,\n    dist::AbstractMDFT,\n    n_sim::Int,\n    M::AbstractArray;\n    Δt = 0.001\n)\n\nGenerate n_sim random choice-rt pairs for the Multiattribute Decision Field Theory (MDFT).\n\nArguments\n\nrng::AbstractRNG: a random number generator which is a subtype of AbstractRNG\ndist::AbstractMDFT: model object for the Multiattribute Decision Field Theory (MDFT).\nn_sim::Int: the number of simulated choice-rt pairs\nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\nKeywords\n\nΔt = 0.001: time step size\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, DDM}","page":"API","title":"Base.rand","text":"rand(dist::DDM)\n\nGenerate a random rt for the Diffusion Decision Model (negative coding)\n\nArguments\n\ndist: model object for the Diffusion Decision Model. \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, MDFT, Int64, AbstractArray}","page":"API","title":"Base.rand","text":"rand(\n    rng::AbstractRNG,\n    dist::MDFT,\n    n_sim::Int,\n    M::AbstractArray;\n    Δt = 0.001\n)\n\nGenerate n_sim random choice-rt pairs for the Multi-attribute Decision Field Theory (MDFT).\n\nArguments\n\nrng::AbstractRNG: a random number generator which is a subtype of AbstractRNG\ndist::MDFT: model object for the Multi-attribute Decision Field Theory (MDFT).\nn_sim::Int: the number of simulated choice-rt pairs\nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\nKeywords\n\nΔt = 0.001: time step size\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, MultivariateDistribution{SequentialSamplingModels.Mixed}, Int64}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, d::SSM2D, N::Int; kwargs...)\n\nDefault method for Generating n_sim random choice-rt pairs from a sequential sampling model  with more than one choice option.\n\nArguments\n\nd::SSM2D: a 2D sequential sampling model.\nn_trials::Int: the number of simulated choices and rts  \n\nKeywords\n\nkwargs...: optional keyword arguments \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.cdf-Tuple{MultivariateDistribution{SequentialSamplingModels.Mixed}, Int64, Real, Vararg{Any}}","page":"API","title":"Distributions.cdf","text":"cdf(d::SSM2D, choice::Int, ub=10)\n\nComputes the cumulative density for a given choice. The cumulative density is based on  an analytic formula, a numeric integration of pdf, or Monte Carlo simulation, depending on which is  available for a given model. \n\nArguments\n\nd::SSM2D: a 2D sequential sampling model.\nchoice::Int: the number of simulated choices and rts  \nub::Real: upper bound of integration\nargs...: optional arguments passed to rand\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.cdf-Tuple{Random.AbstractRNG, AbstractaDDM, Int64, Any, Vararg{Any}}","page":"API","title":"Distributions.cdf","text":"cdf(\n    rng::AbstractRNG, \n    d::AbstractaDDM, \n    choice::Int, \n    fixate::Function, \n    ub, \n    args...; \n    n_sim=10_000, \n    kwargs...\n)\n\nComputes the approximate cumulative probability density of AbstractaDDM using Monte Carlo simulation.\n\nArguments\n\ndist: an attentional diffusion model object\nchoice: the choice on which the cumulative density is computed\nfixate: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nub::Int: the upper bound of the integral\nargs...: optional positional arguments for the fixate function\n\nKeywords\n\nn_sim::Int=10_000: the number of simulated trials\n\nrand_state! = _rand_state!: initialize first state with equal probability \n\nkwargs...: optional keyword arguments for the fixate function\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.cdf-Tuple{SSM1D, Real}","page":"API","title":"Distributions.cdf","text":"cdf(d::SSM1D, choice::Int, ub=10)\n\nComputes the cumulative density for a given choice. The cumulative density is based on  an analytic formula, a numeric integration of pdf, or Monte Carlo simulation, depending on which is  available for a given model. \n\nArguments\n\nd::SSM1D: a 1D sequential sampling model.\nub: upper bound of integration\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{AbstractMLBA, Int64, Real, AbstractArray}","page":"API","title":"Distributions.logpdf","text":"logpdf(d::AbstractMLBA, c::Int, rt::Real,  M::AbstractArray)\n\nComputes default log probability density for multi-alternative linear ballistic accumulator. \n\nArguments\n\ndist::AbstractMLBA: an object for the multi-attribute linear ballistic accumulator\nc::Int: choice index\nrt::Real: reaction time in seconds \nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple}","page":"API","title":"Distributions.logpdf","text":"logpdf(d::SSM2D, data::NamedTuple)\n\nComputes the likelihood for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{AbstractMLBA, Int64, Real, AbstractArray}","page":"API","title":"Distributions.pdf","text":"pdf(d::AbstractMLBA, c::Int, rt::Real,  M::AbstractArray)\n\nComputes default probability density for multi-alternative linear ballistic accumulator. \n\nArguments\n\ndist::AbstractMLBA: an object for the multi-attribute linear ballistic accumulator\nc::Int: choice index\nrt::Real: reaction time in seconds \nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple, Vararg{Any}}","page":"API","title":"Distributions.pdf","text":"pdf(d::SSM2D, data::NamedTuple)\n\nComputes the probability density for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.compute_choice_probs-Tuple{NamedTuple}","page":"API","title":"SequentialSamplingModels.compute_choice_probs","text":"compute_choice_probs(data::NamedTuple; choice_set=unique(data.choice))\n\nReturns the choice probabilities for a 2D SSM. \n\nArguments\n\ndata::NamedTuple: a data structure containing discrete choices in the key choice and corresponding \n\nreaction times in key rt\n\nKeywords\n\n`choice_set: a vector of possible choices. \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.compute_quantiles-Tuple{Matrix{<:Real}}","page":"API","title":"SequentialSamplingModels.compute_quantiles","text":"compute_quantiles(data::Array{<:Real,2}; percentiles=.1:.1:.90)\n\nReturns the marginal quantiles for a continous multivariate SSM. \n\ndata::Array{<:Real,2}: an array of continous observations\n\nKeywords\n\npercentiles=.1:.1:.90: percentiles at which to evaluate the quantiles \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.compute_quantiles-Tuple{NamedTuple}","page":"API","title":"SequentialSamplingModels.compute_quantiles","text":"compute_quantiles(data::NamedTuple; choice_set=unique(data.choice), percentiles=.1:.1:.90)\n\nReturns the quantiles for each choice of a 2D SSM. Note there is a chance that a given choice will  have no observations, and thus no quantiles. Such cases will need to be removed or handled in post processing.\n\nArguments\n\ndata::NamedTuple: a data structure containing discrete choices in the key choice and corresponding \n\nreaction times in key rt\n\nKeywords\n\npercentiles=.1:.1:.90: percentiles at which to evaluate the quantiles \nchoice_set=unique(choice): a vector of possible choices. \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.compute_quantiles-Tuple{Vector{<:Real}}","page":"API","title":"SequentialSamplingModels.compute_quantiles","text":"compute_quantiles(data::Vector{<:Real}; percentiles=.1:.1:.90)\n\nReturns the quantiles associated with a vector of reaction times for a single choice SSM.\n\ndata::Vector{<:Real}: a vector of reaction times     \n\nKeywords\n\npercentiles=.1:.1:.90: percentiles at which to evaluate the quantiles \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{DDM}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::DDM)\n\nReturns 2 for the number of choice options\n\nArguments\n\nd::DDM: a model object for the drift diffusion model\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{MultivariateDistribution{SequentialSamplingModels.Mixed}}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::SSM2D)\n\nReturns the number of choice options based on the length of the drift rate vector ν.\n\nArguments\n\nd::SSM2D: a sub-type of SSM2D\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{SSM1D}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::SSM1D)\n\nReturns 1 for the number of choice options\n\nArguments\n\nd::SSM1D: a sub-type of SSM1D\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractCDDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractCDDM; Δt=.001)\n\nReturns a matrix containing evidence samples of the racing diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractCDDM;: a circular drift diffusion model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractLCA}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractLCA; _...)\n\nReturns a matrix containing evidence samples of the LCA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstrctLCA: an LCA model object\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractPoissonRace}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractPoissonRace; _...)\n\nReturns a matrix containing evidence samples of the LBA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractLBA: a subtype of AbstractLBA\n\nKeywords\n\nn_steps=100: number of time steps at which evidence is recorded\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{MDFT, AbstractArray}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::MDFT, M::AbstractArray; Δt = 0.001, _...)\n\nReturns a matrix containing evidence samples of the MDFT decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::MDFT: an MDFT model object\nM::AbstractArray: an alternative × attribute value matrix representing the value of the stimuli \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, AbstractLBA}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractLBA; n_steps=100)\n\nReturns a matrix containing evidence samples of the LBA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractLBA: a subtype of AbstractLBA\n\nKeywords\n\nn_steps=100: number of time steps at which evidence is recorded\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, AbstractMLBA, AbstractArray}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(rng::AbstractRNG, model::AbstractMLBA, M::AbstractArray; n_steps = 100)\n\nReturns a matrix containing evidence samples of the MLBA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractMLBA: a subtype of AbstractMLBA\n\nKeywords\n\nn_steps=100: number of time steps at which evidence is recorded\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, AbstractRDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractRDM)\n\nReturns a matrix containing evidence samples of the racing diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractRDM: an racing diffusion model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, AbstractaDDM, Vararg{Any}}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(\n    rng::AbstractRNG, \n    model::AbstractaDDM; \n    fixate, \n    args=(), \n    kwargs=(), \n    Δt = .001,\n    rand_state! = _rand_state!\n)\n\nReturns a matrix containing evidence samples from a subtype of an attentional drift diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nrng::AbstractRNG: random number generator \nmodel::AbstractaDDM: an drift diffusion  model object\n\nKeywords\n\nfixate: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs=(): a set of optional positional arguments for the attend function \nkwargs=(): a set of optional keyword arguments for the attend function \nΔt = .001: time step \n\nrand_state! = _rand_state!: initialize first state with equal probability \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, AbstractstDDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(rng::AbstractRNG, model::AbstractstDDM; Δt)\n\nReturns a matrix containing evidence samples of the stDDM decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nrng: a random number generator\nmodel::AbstractstDDM: a starting-time diffusion decision model diffusion model object\nΔt: time-step for simulation\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Random.AbstractRNG, DDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::DDM; Δt=.001)\n\nReturns a matrix containing evidence samples of the drift diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::DDM: an drift diffusion  model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{WaldMixture}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::WaldMixture; Δt=.001)\n\nReturns a matrix containing evidence samples of the Wald mixture decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::Wald: an Wald mixture model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Wald}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::Wald; Δt=.001)\n\nReturns a matrix containing evidence samples of the Wald decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::Wald: an Wald model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.loglikelihood-Tuple{MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple}","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(d::SSM2D, data::NamedTuple)\n\nComputes the summed log likelihood for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"wald/#Wald-Model","page":"Wald Model","title":"Wald Model","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The Wald model, also known as the inverse Gaussian, a sequential sampling model for single choice decisions. It is formally equivalent to a drift diffusion model with one decision threshold and no starting point or across Plots drift rate variability.","category":"page"},{"location":"wald/#Example","page":"Wald Model","title":"Example","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"In this example, we will demonstrate how to use the Wald model in a generic single choice decision task. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"wald/#Load-Packages","page":"Wald Model","title":"Load Packages","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The first step is to load the required packages.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"wald/#Create-Model-Object","page":"Wald Model","title":"Create Model Object","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"wald/#Drift-Rate","page":"Wald Model","title":"Drift Rate","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The parameter nu represents the evidence accumulation rate.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"ν = 3.0","category":"page"},{"location":"wald/#Threshold","page":"Wald Model","title":"Threshold","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The parameter alpha the amount of evidence required to make a decision.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"α = 0.50","category":"page"},{"location":"wald/#Non-Decision-Time","page":"Wald Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"τ = 0.130","category":"page"},{"location":"wald/#Wald-Constructor","page":"Wald Model","title":"Wald Constructor","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Now that values have been asigned to the parameters, we will pass them to Wald to generate the model object.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"dist = Wald(ν, α, τ)","category":"page"},{"location":"wald/#Simulate-Model","page":"Wald Model","title":"Simulate Model","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"rts = rand(dist, 1000)","category":"page"},{"location":"wald/#Compute-PDF","page":"Wald Model","title":"Compute  PDF","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"pdf.(dist, rts)","category":"page"},{"location":"wald/#Compute-Log-PDF","page":"Wald Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"logpdf.(dist, rts)","category":"page"},{"location":"wald/#Compute-CDF","page":"Wald Model","title":"Compute CDF","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The cumulative probability density Pr(T leq t) is computed by passing the model and a value t to cdf.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"cdf(dist, .4)","category":"page"},{"location":"wald/#Plot-Simulation","page":"Wald Model","title":"Plot Simulation","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The code below overlays the PDF on reaction time histogram.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"histogram(dist)\nplot!(dist; t_range=range(.130, 1, length=100))","category":"page"},{"location":"wald/#References","page":"Wald Model","title":"References","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Anders, R., Alario, F., & Van Maanen, L. (2016). The shifted Wald distribution for response time data analysis. Psychological methods, 21(3), 309.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Folks, J. L., & Chhikara, R. S. (1978). The inverse Gaussian distribution and its statistical application—a review. Journal of the Royal Statistical Society: Series B (Methodological), 40(3), 263-275.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Steingroever, H., Wabersich, D., & Wagenmakers, E. J. (2021). Modeling across-Plots variability in the Wald drift rate parameter. Behavior Research Methods, 53, 1060-1076.","category":"page"},{"location":"maaDDM/#Attentional-Drift-Diffusion-Model","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Attentional Drift Diffusion Model","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The multi-attribute attentional drift diffusion model (MAADDM; Yang & Krajbich, 2023) describes how attentional processes drive drive decision making. Much like the ADDM, in the MAADDM preference for the currently attended option accrues faster than preference for non-attended options. However, the MAADDM has been extended to model shifts in attention for alternatives with two attributes. As with other sequential sampling models, the first option to hit a decision threshold determines the resulting choice and reaction time.","category":"page"},{"location":"maaDDM/#Example","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Example","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"using SequentialSamplingModels\nusing StatsBase\nusing Plots \nusing Random","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"In this example, we will develope a MAADDM for binary choice and generate its predictions. Unlike many other sequential sampling models, it is necessary to specify the attentional process, or supply fixation patterns from eye tracking data. ","category":"page"},{"location":"maaDDM/#Load-Packages","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Load Packages","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The first step is to load the required packages.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"using SequentialSamplingModels\nusing StatsBase\nusing Plots \n\nRandom.seed!(9854)","category":"page"},{"location":"maaDDM/#Define-Transition-Type","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Define Transition Type","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"To represent the transition of attention from one option to the other, we will definite a Transition type and constructor. The fields of the Transition type are:","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"state: an index for the current state\nn: the number of states\nmat: an ntimes n transition matrix","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The constructor accepts a transition matrix, extracts the number of states, and initializes the first state randomly with equal probability.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"mutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\nfunction Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end","category":"page"},{"location":"maaDDM/#Define-Transition-Matrix","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Define Transition Matrix","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The transition matrix is defined below in the constructor for Transition. As shown in the table below, the model's attention can be in one of three states: option 1, option 2, or non-option, which is any area except the two options.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"  Option 1  Option 1 \n  Attribute 1 Attribute 2 Attribute 1 Attribute 2\nOption 1 Attribute 1 0.980 0.015 0.0025 0.0025\n Attribute 2 0.015 0.980 0.0025 0.0025\nOption 1 Attribute 1 0.0025 0.0025 0.980 0.015\n Attribute 2 0.0025 0.0025 0.015 0.980","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The transition matrix above embodies the following assumptions:","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Once the model attends to an option, it dwells on the option for some time.\nThere is not a bias for one option over the other.\nThere is a larger chance of transitioning between attributes within the same alternative than transitioning between alternatives\nTransitions are Markovian in that they only depend on the previous state.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":" tmat = Transition([.98 .015 .0025 .0025;\n                    .015 .98 .0025 .0025;\n                    .0025 .0025 .98 .015;\n                    .0025 .0025 .015 .98])\n","category":"page"},{"location":"maaDDM/#Fixate-Function","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Fixate Function","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The function below generates the next attention location based on the previous location. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":" function fixate(transition)\n     (;mat,n,state) = transition\n     w = @view mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end","category":"page"},{"location":"maaDDM/#Create-Model-Object","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Create Model Object","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The code snippets assign values to parameters of the MAADDM and create a model object.","category":"page"},{"location":"maaDDM/#Drift-Rate-Components","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Drift Rate Components","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"In the decision making task, there are two alternatives with two attributes each. This leads to four components of the drift rates: nu_11 nu_12nu_21nu_22 where the first index corresponds to alternative and the second index corresponds to attribute.  To form the drift rate, each component is weighted by non-attention bias and then a difference is computed.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"ν = [4.0 5.0; 5.0 4.0]","category":"page"},{"location":"maaDDM/#Threshold","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Threshold","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The threshold hold represents the amount of evidence required to make a decision. This parameter is typically fixed at alpha = 1.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"α = 1.0","category":"page"},{"location":"maaDDM/#Starting-Point","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Starting Point","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The starting point of the evidence accumulation process is denoted z and is typically fixed to 0.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"z = 0.0","category":"page"},{"location":"maaDDM/#Non-Attend-Bias-Alternative","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Non-Attend Bias Alternative","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The non-attend bias parameter theta determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, theta=1. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"θ = 0.30","category":"page"},{"location":"maaDDM/#Non-Attend-Bias-Attribute","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Non-Attend Bias Attribute","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The non-attend bias parameter psi determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, psi=1. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"ϕ = .50","category":"page"},{"location":"maaDDM/#Attribute-Weight","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Attribute Weight","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The parameter omega denotes the weight of the first attribute.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"ω = .70","category":"page"},{"location":"maaDDM/#Diffusion-Noise","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Diffusion Noise","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Diffusion noise, sigma represents intra-trial noise during the evidence accumulation process.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"σ = 0.02","category":"page"},{"location":"maaDDM/#Drift-Rate-Scalar","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Drift Rate Scalar","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"The drift rate scalar controls how quickly evidence accumulates for each option. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Δ = 0.0004 ","category":"page"},{"location":"maaDDM/#Model-Object","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Model Object","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Finally, we pass the parameters to the maaDDM constructor to initialize the model.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"dist = maaDDM(; ν, α, z, θ, ϕ, ω, σ, Δ)","category":"page"},{"location":"maaDDM/#Simulate-Model","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Simulate Model","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. The rand function accepts the model object, the number of simulated trials, the fixate function, and the transition matrix object. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":" choices,rts = rand(dist, 10_000, tmat; fixate)","category":"page"},{"location":"maaDDM/#Plot-Simulation","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Plot Simulation","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Finally, we can generate histograms of the reaction times for each decision option. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"histogram(dist; model_args=(;tmat), model_kwargs=(;fixate))\nplot!(dist; model_args=(;tmat), model_kwargs=(;fixate), t_range=range(.130, 5, length=100), xlims=(0,7))","category":"page"},{"location":"maaDDM/#References","page":"Muti-attribute Attentional Drift Diffusion Model","title":"References","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Yang, X., & Krajbich, I. (2023). A dynamic computational model of gaze and choice in multi-attribute decisions. Psychological Review, 130(1), 52.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute Attentional Drift Diffusion Model","title":"Muti-attribute Attentional Drift Diffusion Model","text":"Fisher, G. (2021). A multiattribute attentional drift diffusion model. Organizational Behavior and Human Decision Processes, 165, 167-182.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"using SequentialSamplingModels\nusing Plots \nusing Random\nM = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n    0.9 3.1 # S\n]","category":"page"},{"location":"mlba/#Multi-attribute-Linear-Ballistic-Accumulator","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Multi-attribute Linear Ballistic Accumulator (MLBA; Trueblood et al., 2014) is an extention of the LBA for multi-attribute deicisions. Alternatives in multi-attribute decisions vary along multiple dimensions. For example, jobs may differ in terms of benefits, salary, flexibility, and work-life balance. As with other sequential sampling models, MLBA assumes that evidence (or preference) accumulates dynamically until the evidence for one alternative reaches a threshold, and triggers the selection of the winning alternative. MLBA incorporates three additional core assumptions:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Drift rates based on a weighted sum of pairwise comparisons\nThe comparison weights are an inverse function of similarity of two alternatives on a given attribute.\nObjective values are mapped to subjective values, which can display extremeness aversion","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"As with MDFT, the MLBA excells in  accounting for context effects in preferential decision making. A context effect occurs when the preference relationship between two alternatives changes when a third alternative is included in the choice set. In such cases, the preferences may reverse or the decision maker may violate rational choice principles.  ","category":"page"},{"location":"mlba/#Similarity-Effect","page":"Multi-attribute Linear Ballistic Accumulator","title":"Similarity Effect","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"In what follows, we will illustrate the use of MLBA with a demonstration of the similarity effect.  Consider the choice between two jobs, A and B. The main criteria for evaluating the two jobs are salary and flexibility. Job A is high on salary but low on flexibility, whereas job B is low on salary. In the plot below, jobs A and B are located on the line of indifference, y = 3 - x. However, because salary recieves more attention, job A is slightly prefered over job B.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"scatter(\n    M[:, 1],\n    M[:, 2],\n    grid = false,\n    leg = false,\n    lims = (0, 4),\n    xlabel = \"Flexibility\",\n    ylabel = \"Salary\",\n    markersize = 6,\n    markerstrokewidth = 2\n)\nannotate!(M[1, 1] + 0.10, M[1, 2] + 0.25, \"A\")\nannotate!(M[2, 1] + 0.10, M[2, 2] + 0.25, \"B\")\nannotate!(M[3, 1] + 0.10, M[3, 2] + 0.25, \"S\")\nplot!(0:0.1:4, 4:-0.1:0, color = :black, linestyle = :dash)","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Suppose an job S, which is similar to A is added to the set of alternatives. Job S inhibits job A more than job B because S and A are close in attribute space. As a result, the preference for job A over job B is reversed. Formally, this is stated as:  ","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Pr(A mid AB)  Pr(B mid AB)","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Pr(A mid ABS)  Pr(B mid ABS)","category":"page"},{"location":"mlba/#Load-Packages","page":"Multi-attribute Linear Ballistic Accumulator","title":"Load Packages","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The first step is to load the required packages.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"mlba/#Create-Model-Object","page":"Multi-attribute Linear Ballistic Accumulator","title":"Create Model Object","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"In the code below, we will define parameters for the MLBA and create a model object to store the parameter values. ","category":"page"},{"location":"mlba/#Drift-Rate-Parameters","page":"Multi-attribute Linear Ballistic Accumulator","title":"Drift Rate Parameters","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"In sequential sampling models, the drift rate is the average speed with which evidence accumulates towards a decision threshold. In the MLBA, the drift rate is determined by comparing attributes between alternatives. The drift rate for alternative i is defined as:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"nu_i = beta_0 + sum_ine j v_ij","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"where beta_0 is the basline additive constant, and v_ij is the comparative value between alternatives i and j. A given alternative i is compared to another alternative j ne i as a weighted sum of differences across attributes k in 12dots n_a:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"v_ij = sum_k=1^n_a w_ijk (u_ik - u_jk)","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The attention weight between alternatives i and j on attribute k is an inverse function of similarity, and decays exponentially with distance:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"w_ijk = e^-lambda u_ik - u_jk","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Similarity between alternatives is not necessarily symmetrical, giving rise to two decay rates:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"lambda = begincases \n      lambda_p  u_ik geq u_jk\n      lambda_n  mathrm otherwise\nendcases","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The subjective value mathbfu = u_1u_2 is found by bending the line of indifference passing through the objective stimulus mathbfs in attribute space, such that (fracxa)^gamma + (fracxb)^gamma = 1. When gamma  1, the model produces extremeness aversion. ","category":"page"},{"location":"mlba/#Baseline-Drift-Rate","page":"Multi-attribute Linear Ballistic Accumulator","title":"Baseline Drift Rate","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The baseline drift rate parameter beta_0 is a constant added to drift rate:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"β₀ = 5.0","category":"page"},{"location":"mlba/#Similarity-Based-Weighting","page":"Multi-attribute Linear Ballistic Accumulator","title":"Similarity-Based Weighting","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Attention weights are an inverse function of similarity between alternatives on a given attribute. The decay rate for positive differences is:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"λₚ = 0.20","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The decay rate for negative differences is:","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"λₙ = 0.40","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The comparisons are asymmetrical when lambda_p ne lambda_n. ","category":"page"},{"location":"mlba/#Extremeness-Aversion","page":"Multi-attribute Linear Ballistic Accumulator","title":"Extremeness Aversion","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The parameter for extremeness aversion is: ","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"γ = 5","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"gamma = 1 indicates objective treatment of stimuli, whereas gamma  1 indicates extreness aversion, i.e. 22 is prefered to 31 even though both fall along the line indifference.","category":"page"},{"location":"mlba/#Standard-Deviation-of-Drift-Rates","page":"Multi-attribute Linear Ballistic Accumulator","title":"Standard Deviation of Drift Rates","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The standard deviation of the drift rate distribution is given by sigma, which is commonly fixed to 1 for each accumulator.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"σ = [1.0,1.0]","category":"page"},{"location":"mlba/#Maximum-Starting-Point","page":"Multi-attribute Linear Ballistic Accumulator","title":"Maximum Starting Point","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The starting point of each accumulator is sampled uniformly between 0A.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"A = 1.0","category":"page"},{"location":"mlba/#Threshold-Maximum-Starting-Point","page":"Multi-attribute Linear Ballistic Accumulator","title":"Threshold - Maximum Starting Point","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Evidence accumulates until accumulator reaches a threshold alpha = k +A. The threshold is parameterized this way to faciliate parameter estimation and to ensure that A le alpha.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"k = 1.0","category":"page"},{"location":"mlba/#Non-Decision-Time","page":"Multi-attribute Linear Ballistic Accumulator","title":"Non-Decision Time","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"τ = 0.30","category":"page"},{"location":"mlba/#MLBA-Constructor","page":"Multi-attribute Linear Ballistic Accumulator","title":"MLBA Constructor","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Now that values have been asigned to the parameters, we will pass them to MLBA to generate the model object. We will begin with the choice between job A and job B.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"dist = MLBA(;\n    n_alternatives = 2,\n    β₀,\n    λₚ,\n    λₙ,\n    γ,\n    τ,\n    A,\n    k,\n)","category":"page"},{"location":"mlba/#Simulate-Model","page":"Multi-attribute Linear Ballistic Accumulator","title":"Simulate Model","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Now that the model is defined, we will generate 10,000 choices and reaction times using rand. ","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"M₂ = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n]\n    \nchoices,rts = rand(dist, 10_000, M₂)\nprobs2 = map(c -> mean(choices .== c), 1:2)","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Here, we see that job A is prefered over job B.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Next, we will simulate the choice between jobs A, B, and S.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"dist = MLBA(;\n    n_alternatives = 3,\n    β₀,\n    λₚ,\n    λₙ,\n    γ,\n    τ,\n    A,\n    k,\n)\n\nM₃ = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n    0.9 3.1 # S\n]\n\nchoices,rts = rand(dist, 10_000, M₃)\nprobs3 = map(c -> mean(choices .== c), 1:3)","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"In this case, the preferences have reversed: job B is now preferred over job A. ","category":"page"},{"location":"mlba/#Compute-Choice-Probability","page":"Multi-attribute Linear Ballistic Accumulator","title":"Compute Choice Probability","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"cdf(dist, 1, Inf, M₃)","category":"page"},{"location":"mlba/#Plot-Simulation","page":"Multi-attribute Linear Ballistic Accumulator","title":"Plot Simulation","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"The code below plots a histogram for each alternative.","category":"page"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"histogram(dist; model_args = (M₃,))","category":"page"},{"location":"mlba/#References","page":"Multi-attribute Linear Ballistic Accumulator","title":"References","text":"","category":"section"},{"location":"mlba/","page":"Multi-attribute Linear Ballistic Accumulator","title":"Multi-attribute Linear Ballistic Accumulator","text":"Trueblood, J. S., Brown, S. D., & Heathcote, A. (2014). The multiattribute linear ballistic accumulator model of context effects in multialternative choice. Psychological Review, 121(2), 179.","category":"page"},{"location":"rdm/#Racing-Diffusion-Model","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The Racing Diffusion Model (RDM; Tillman, Van Zandt, & Logan, 2020) is a sequential sampling model in which evidence for options races independently. The RDM is similar to the Linear Ballistic Accumulator (LBA), except it assumes that noise occurs during the within-trial evidence accumulation process (but the drift rate is constant across trials).","category":"page"},{"location":"rdm/#Example","page":"Racing Diffusion Model (RDM)","title":"Example","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"In this example, we will demonstrate how to use the RDM in a generic two alternative forced choice task.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"using SequentialSamplingModels\nusing Plots\nusing Random","category":"page"},{"location":"rdm/#Load-Packages","page":"Racing Diffusion Model (RDM)","title":"Load Packages","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"rdm/#Create-Model-Object","page":"Racing Diffusion Model (RDM)","title":"Create Model Object","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"In the code below, we will define parameters for the RDM and create a model object to store the parameter values.","category":"page"},{"location":"rdm/#Drift-Rates","page":"Racing Diffusion Model (RDM)","title":"Drift Rates","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The drift rates control the speed with which information accumulates. Typically, there is one drift rate per option.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"ν = [1.0,0.50]","category":"page"},{"location":"rdm/#Maximum-Starting-Point","page":"Racing Diffusion Model (RDM)","title":"Maximum Starting Point","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The starting point of each accumulator is sampled uniformly between 0A.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"A = 0.80","category":"page"},{"location":"rdm/#Threshold-Maximum-Starting-Point","page":"Racing Diffusion Model (RDM)","title":"Threshold - Maximum Starting Point","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Evidence accumulates until accumulator reaches a threshold alpha = k +A. The threshold is parameterized this way to faciliate parameter estimation and to ensure that A le alpha.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"k = 0.50","category":"page"},{"location":"rdm/#Non-Decision-Time","page":"Racing Diffusion Model (RDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Non-decision time is an additive constant representing encoding and motor response time.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"τ  = 0.30","category":"page"},{"location":"rdm/#RDM-Constructor","page":"Racing Diffusion Model (RDM)","title":"RDM Constructor","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Now that values have been assigned to the parameters, we will pass them to RDM to generate the model object.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"dist = RDM(;ν, k, A, τ)","category":"page"},{"location":"rdm/#Simulate-Model","page":"Racing Diffusion Model (RDM)","title":"Simulate Model","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":" choices,rts = rand(dist, 10_000)\n ","category":"page"},{"location":"rdm/#Compute-PDF","page":"Racing Diffusion Model (RDM)","title":"Compute PDF","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"rdm/#Compute-Log-PDF","page":"Racing Diffusion Model (RDM)","title":"Compute Log PDF","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"rdm/#Compute-Choice-Probability","page":"Racing Diffusion Model (RDM)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"rdm/#Plot-Simulation","page":"Racing Diffusion Model (RDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"histogram(dist; xlims=(0,2.5))\nplot!(dist; t_range=range(.301, 2.5, length=100))","category":"page"},{"location":"rdm/#References","page":"Racing Diffusion Model (RDM)","title":"References","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Tillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability: The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27, 911-936.","category":"page"},{"location":"turing_hierarchical/#Hierarchical-Models","page":"Hierarchical Models","title":"Hierarchical Models","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"In this example, we will fit a model with random factors and estimate individual parameters. This tutorial will build on the previous ones, so make sure you have followed them first. Let's start by loading all the packages and setting a reproducible seed.","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing Distributions\nusing DataFrames\nusing StatsPlots\nusing StatsModels\nusing CSV\nusing Optim\n\nRandom.seed!(6)","category":"page"},{"location":"turing_hierarchical/#Generate-Data","page":"Hierarchical Models","title":"Generate Data","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We will use the LBA distribution to simulate data for 10 participants in two conditions with 100 trials per condition (repeated measures design). The drift rates for condition A are sampled from normal distributions, and the drift rates for condition B are set by sampling a departure from (i.e., the difference with) condition A. In other words, each participant has different drift rates for condition A (the intercept, i.e., the baseline condition) and a different \"effect\" magnitude of condition B (the offset from condition A to condition B).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"# Generate data with different drifts for two conditions A vs. B\ndf = DataFrame()\nparams = DataFrame()\nfor participant in 1:10\n    # Intercept (condition A)\n    drifts = [rand(Normal(1.5, 0.2)), rand(Normal(0.5, 0.1))]\n    param = join(round.(drifts, digits=2), \", \")  # Format and save params\n    df1 = DataFrame(rand(LBA(ν=drifts, A=0.5, k=0.5, τ=0.3), 100))\n    df1[!, :condition] = repeat([\"A\"], nrow(df1))\n    df1[!, :participant] = repeat([participant], nrow(df1))\n\n    # Effect of condition B\n    drifts2 = [rand(Normal(0.5, 0.15)), rand(Normal(0.5, 0.05))]\n    param = [param, join(round.(drifts2, digits=2), \", \")]\n    df2 = DataFrame(rand(LBA(ν=drifts .+ drifts2, A=0.5, k=0.5, τ=0.3), 100))\n    df2[!, :condition] = repeat([\"B\"], nrow(df2))\n    df2[!, :participant] = repeat([participant], nrow(df1))\n\n    # Assemble and store parameters (to compare with estimation)\n    df = vcat(df, df1, df2)\n    params = vcat(params, DataFrame(permutedims(param), [:drift_intercept, :drift_condition]))\nend","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We can visualize the individual distributions for the two type of responses and for the conditions (condition A in red and B in blue).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"density(layout=(2, 1), ylims=(0, 5), xlims=(0, 3), legend=false)\nfor p in unique(df.participant)\n    for (i, cond) in enumerate([\"A\", \"B\"])\n        density!(df.rt[(df.choice.==1).&(df.condition.==cond).&(df.participant.==p)],\n            subplot=1, color=[:blue, :red][i], title=\"Choice = 1\")\n        density!(df.rt[(df.choice.==2).&(df.condition.==cond).&(df.participant.==p)],\n            subplot=2, color=[:blue, :red][i], title=\"Choice = 2\", xlabel=\"Reaction Time (s)\")\n    end\nend\nplot!()","category":"page"},{"location":"turing_hierarchical/#Model-Specification","page":"Hierarchical Models","title":"Model Specification","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"First, we will transform our predictor data into an model matrix. This essentially transform our favor column with \"A\" and \"B\" to a binary vector.","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We will also transform our outcome data (RTs and choice) into a list of tuples (see this example for more explanation).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"# Format input data\nf = @formula(rt ~ 1 + condition)\nf = apply_schema(f, schema(f, df))\n_, predictors = coefnames(f)\nX = modelmatrix(f, df)\n\n# Format the data to match the input type\ndata = [(choice=df.choice[i], rt=df.rt[i]) for i in 1:nrow(df)]","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"Now, the model is a bit more complex:","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"@model function model_lba(data; min_rt=0.2, condition=nothing, participant=nothing)\n\n    # Priors for auxiliary parameters\n    A ~ truncated(Normal(0.8, 0.4), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    tau ~ Uniform(0.0, min_rt)\n\n    # Priors for population-level coefficients\n    drift_intercept_1 ~ Normal(0, 1)\n    drift_intercept_2 ~ Normal(0, 1)\n    drift_condition_1 ~ Normal(0, 1)\n    drift_condition_2 ~ Normal(0, 1)\n\n    # Prior for random intercepts (requires thoughtful specification)\n    # Group-level intercepts' SD\n    drift_intercept_random_sd ~ truncated(Cauchy(0, 0.1), 0.0, Inf)\n    # Group-level intercepts\n    drift_intercept_random_1 ~ filldist(\n        Normal(0, drift_intercept_random_sd),\n        length(unique(participant))\n    )\n    drift_intercept_random_2 ~ filldist(\n        Normal(0, drift_intercept_random_sd),\n        length(unique(participant))\n    )\n\n    for i in 1:length(data)\n        # Formula for intercept\n        drifts_intercept_1 = drift_intercept_1 .+ drift_intercept_random_1[participant[i]]\n        drifts_intercept_2 = drift_intercept_2 .+ drift_intercept_random_2[participant[i]]\n\n        # Combine with condition\n        drifts_1 = drift_intercept_1 + drift_condition_1 * condition[i]\n        drifts_2 = drift_intercept_2 + drift_condition_2 * condition[i]\n        data[i] ~ LBA(; τ=tau, A=A, k=k, ν=[drifts_1, drifts_2])\n    end\nend","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"Note that for now, these types of model are very slow to run in Turing.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"using SequentialSamplingModels\nusing Plots \nusing Random\nM = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n    0.9 3.1 # S\n]","category":"page"},{"location":"mdft/#Multi-attribute-Decision-Field-Theory","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Multi-attribute Decision Field Theory (MDFT; Roe, Busemeyer, & Townsend, 2001) models how people choose between alternatives with multiple dimensions, such as cars, phones, or jobs. As an example, jobs may differ in terms of benefits, salary, flexibility, and work-life balance. As with other sequential sampling models, MDFT assumes that evidence (or preference) accumulates dynamically until the evidence for one alternative reaches a threshold, and triggers the selection of the winning alternative. MDFT incorporates three additional core assumptions:","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Attention switches between attributes, and alternatives are compared on the currently attended attribute\nAs two alternatives become closer to each other in attribute space, their mutual inhibition increases\nEvidence for each alternative gradually decays across time","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"One of MDFT's strong suits is accounting for context effects in preferential decision making. A context effect occurs when the preference relationship between two alternatives changes when a third alternative is included in the choice set. In such cases, the preferences may reverse or the decision maker may violate rational choice principles.  ","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Note that this version of MDFT uses stochastic differential equations (see Evans et al., 2019). For the random walk version, see ClassicMDFT. ","category":"page"},{"location":"mdft/#Similarity-Effect","page":"Multi-attribute Decision Field Theory","title":"Similarity Effect","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In what follows, we will illustrate the use of MDFT with a demonstration of the similarity effect.  Consider the choice between two jobs, A and B. The main criteria for evaluating the two jobs are salary and flexibility. Job A is high on salary but low on flexibility, whereas job B is low on salary. In the plot below, jobs A and B are located on the line of indifference, y = 3 - x. However, because salary recieves more attention, job A is slightly prefered over job B.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"scatter(\n    M[:, 1],\n    M[:, 2],\n    grid = false,\n    leg = false,\n    lims = (0, 4),\n    xlabel = \"Flexibility\",\n    ylabel = \"Salary\",\n    markersize = 6,\n    markerstrokewidth = 2\n)\nannotate!(M[1, 1] + 0.10, M[1, 2] + 0.25, \"A\")\nannotate!(M[2, 1] + 0.10, M[2, 2] + 0.25, \"B\")\nannotate!(M[3, 1] + 0.10, M[3, 2] + 0.25, \"S\")\nplot!(0:0.1:4, 4:-0.1:0, color = :black, linestyle = :dash)","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Suppose an job S, which is similar to A is added to the set of alternatives. Job S inhibits job A more than job B because S and A are close in attribute space. As a result, the preference for job A over job B is reversed. Formally, this is stated as:  ","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Pr(A mid AB)  Pr(B mid AB)","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Pr(A mid ABS)  Pr(B mid ABS)","category":"page"},{"location":"mdft/#Load-Packages","page":"Multi-attribute Decision Field Theory","title":"Load Packages","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The first step is to load the required packages.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"mdft/#Create-Model-Object","page":"Multi-attribute Decision Field Theory","title":"Create Model Object","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In the code below, we will define parameters for the MDFT and create a model object to store the parameter values. ","category":"page"},{"location":"mdft/#Drift-Rate-Scalar","page":"Multi-attribute Decision Field Theory","title":"Drift Rate Scalar","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In MDFT, the drift rate is determined by the contrast between alternatives along the attended attribute. These evaluations are scaled by the parameter gamma:","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"γ = 1.0","category":"page"},{"location":"mdft/#Threshold","page":"Multi-attribute Decision Field Theory","title":"Threshold","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The threshold alpha represents the amount of evidence required to make a decision.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"α = .50","category":"page"},{"location":"mdft/#Dominance-Weight","page":"Multi-attribute Decision Field Theory","title":"Dominance Weight","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In MDFT, alternatives are compared along the dominance dimension (diagonal) and indifference dimension (off-diagonal) in attribute space. The relative weight of the dominance dimension is controlled by parameter beta","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"β = 10","category":"page"},{"location":"mdft/#Lateral-Inhibition","page":"Multi-attribute Decision Field Theory","title":"Lateral Inhibition","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In MDFT, alternatives inhibit each other as an inverse function of thier distance in attribute space: the closer they are, the more inhibititory the relationship. Lateral inhibition is controled via a alternative times alternative feedback matrix in which the diagonal elements (e.g., self-inhibition) represents decay or leakage, and the non-diagonal elements represent lateral inhibition between different alternatives. The values of the feedback matrix are controled by a Gaussian distance function with two parameters: phi_1 and phi_2. ","category":"page"},{"location":"mdft/#Inhibition-Strength","page":"Multi-attribute Decision Field Theory","title":"Inhibition Strength","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The distance gradient parameter phi_1 controls the strength of lateral inhibition between alternatives:","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"ϕ1 = .01","category":"page"},{"location":"mdft/#Maximum-Inhibition","page":"Multi-attribute Decision Field Theory","title":"Maximum Inhibition","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Maximimum inhibition and decay is controlled by parameter phi_2:","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"ϕ2 = .10","category":"page"},{"location":"mdft/#Diffusion-Noise","page":"Multi-attribute Decision Field Theory","title":"Diffusion Noise","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Diffusion noise is the amount of within trial noise in the evidence accumulation process. ","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"σ = .10","category":"page"},{"location":"mdft/#Non-Decision-Time","page":"Multi-attribute Decision Field Theory","title":"Non-Decision Time","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"τ = 0.30","category":"page"},{"location":"mdft/#Attention-Switching-Rates","page":"Multi-attribute Decision Field Theory","title":"Attention Switching Rates","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The rate at which attention shifts from one attribute to the other is controlled by the following rate parameters:","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"κ = [6, 5]","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The second rate is lower than the first rate to reflect more attention to the second dimension (i.e., salary).","category":"page"},{"location":"mdft/#MDFT-Constructor","page":"Multi-attribute Decision Field Theory","title":"MDFT Constructor","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Now that values have been asigned to the parameters, we will pass them to MDFT to generate the model object. We will begin with the choice between job A and job B.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"dist = MDFT(;\n    n_alternatives = 2,\n    σ,\n    α,\n    τ,\n    γ,\n    κ,\n    ϕ1,\n    ϕ2,\n    β,\n)","category":"page"},{"location":"mdft/#Simulate-Model","page":"Multi-attribute Decision Field Theory","title":"Simulate Model","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Now that the model is defined, we will generate 10,000 choices and reaction times using rand. ","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"M₂ = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n]\n    \nchoices,rts = rand(dist, 10_000, M₂; Δt = .001)\nprobs2 = map(c -> mean(choices .== c), 1:2)","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Here, we see that job A is prefered over job B. Also note, in the code block above, rand has a keyword argument Δt which controls the precision of the time discrete approximation. The default value is Δt = .001.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Next, we will simulate the choice between jobs A, B, and S.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"dist = MDFT(;\n    n_alternatives = 3,\n    σ,\n    α,\n    τ,\n    γ,\n    κ,\n    ϕ1,\n    ϕ2,\n    β,\n)\n\nM₃ = [\n    1.0 3.0 # A \n    3.0 1.0 # B\n    0.9 3.1 # S\n]\n\nchoices,rts = rand(dist, 10_000, M₃)\nprobs3 = map(c -> mean(choices .== c), 1:3)","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"In this case, the preferences have reversed: job B is now preferred over job A. ","category":"page"},{"location":"mdft/#Compute-Choice-Probability","page":"Multi-attribute Decision Field Theory","title":"Compute Choice Probability","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"cdf(dist, 1, Inf, M₃)","category":"page"},{"location":"mdft/#Plot-Simulation","page":"Multi-attribute Decision Field Theory","title":"Plot Simulation","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"The code below plots a histogram for each alternative.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"histogram(dist; model_args = (M₃,))","category":"page"},{"location":"mdft/#References","page":"Multi-attribute Decision Field Theory","title":"References","text":"","category":"section"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Evans, N. J., Holmes, W. R., & Trueblood, J. S. (2019). Response-time data provide critical constraints on dynamic models of multi-alternative, multi-attribute choice. Psychonomic Bulletin & Review, 26, 901-933.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Hotaling, J. M., Busemeyer, J. R., & Li, J. (2010). Theoretical developments in decision field theory: Comment on tsetsos, usher, and chater (2010). Psychological Review, 117 , 1294-1298.","category":"page"},{"location":"mdft/","page":"Multi-attribute Decision Field Theory","title":"Multi-attribute Decision Field Theory","text":"Roe, Robert M., Jermone R. Busemeyer, and James T. Townsend. \"Multi-attribute Decision Field Theory: A dynamic connectionst model of decision making.\" Psychological review 108.2 (2001): 370.","category":"page"},{"location":"predictive_distributions/#Prior-and-Posterior-Predictive-Distributions","page":"Predictive Distributions","title":"Prior and Posterior Predictive Distributions","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"This tutorial explains the steps required for constructing and plotting prior and posterior predictive distributions of a sequential sampling models (SSMs). The primary function we will be using is predict_distribution, which allows you to generate prior or posterior predictive distributions from a given model. ","category":"page"},{"location":"predictive_distributions/#Example","page":"Predictive Distributions","title":"Example","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"The first step is to load the required packages and set the seed for the random number generator.","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"using Distributions\nusing Plots\nusing Random\nusing SequentialSamplingModels\nusing Turing \nRandom.seed!(1124)","category":"page"},{"location":"predictive_distributions/#Generate-Simulated-Data","page":"Predictive Distributions","title":"Generate Simulated Data","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"We will use the Wald model as a simple example to illustrate how to create predictive distributions. The Wald model describes the evidence accumulation process underlying single detection decisions, such as respending when a stimulus appears. In the code block below, we will generate 50 data points.","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"n_samples = 50\nrts = rand(Wald(ν=1.5, α=.8, τ=.3), n_samples)","category":"page"},{"location":"predictive_distributions/#Define-Turing-Model","page":"Predictive Distributions","title":"Define Turing Model","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"Next, we will develop a Turing model for generating prior and posterior predictive distributions. You may develop the Turing model as usual, with one minor exception: you must return a NamedTuple of parameters. In the example below, nu and alpha are estimated, but tau is fixed. You may use any combination of estimated and fixed parameters.  ","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"@model function wald_model(rts)\n    ν ~ truncated(Normal(1.5, 1), 0, Inf)\n    α ~ truncated(Normal(.8, 1), 0, Inf)\n    τ = 0.3\n    rts ~ Wald(ν, α, τ)\n    return (;ν, α, τ)\nend","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"In the next code block, we will pass the data and create a model object.","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"model = wald_model(rts)","category":"page"},{"location":"predictive_distributions/#Generate-Prior-Predictive-Distribution","page":"Predictive Distributions","title":"Generate Prior Predictive Distribution","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"Generating a prior predictive distribution involves two steps: (1) sample from the prior, and (2) predict data or a statistic with the model evaluated at the prior samples. Below, we will sample 1,000 parameter vectors from the model. ","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"prior_chain = sample(model, Prior(), 1000)","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"For the next step, we will generate predictions from the model using the parameters sampled from the prior distribution. When Turing is loaded, SequentialSamplingModels automatically loads predict_distribution into your session. The signature for predict_distribution is as follows:","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"predict_distribution(dist, args...; model, func, n_samples, kwargs...)","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"func computes a statistic from simulated data of the model and has the general form func(sim_data, args...; kwargs...). Thus, the only constraint is that func must recieve the simulated data as its first argument. args... and kwargs... are optionally pased to func. The remaining inputs are the model type dist, the Turing model object model, and the number of simulated observations n_samples.","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"As a simple illustration, we will compute the prior predictive mean by calling the following two functions. The first function creates a new function to sample from the predictive distribution and the second function generated_quantities performs the sampling.","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"pred_model = predict_distribution(Wald; model, func=mean, n_samples)\nprior_preds = generated_quantities(pred_model, prior_chain)","category":"page"},{"location":"predictive_distributions/#Generate-Posterior-Predictive-Distribution","page":"Predictive Distributions","title":"Generate Posterior Predictive Distribution","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"Generating a posterior predictive distribution involves a similar process. First, we will estimate the parameters from the data to obtain a chain of posterior samples. Next, we will generate the posterior predictive distribution using generated_quantities:","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"post_chain = sample(model, NUTS(1000, .85), 1000)\npost_preds = generated_quantities(pred_model, post_chain)","category":"page"},{"location":"predictive_distributions/#Plot-the-Distributions","page":"Predictive Distributions","title":"Plot the Distributions","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"Now that we have generated the predictive distributions, we can compare them to the data by plotting them as a histogram. The histogram below reveals two insights: first, the data are centered near the prior and posterior predictive distributions, indicating they predict the data accurately; second, the posterior distribution is concentrated more closely around the data, indicating the information gain acquired during parameter estimation. ","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"histogram(prior_preds[:], xlims=(0,4), xlabel=\"Mean RT\", ylabel=\"Density\", norm=true, \n    color=:grey, label=\"prior\", grid=false)\nhistogram!(post_preds[:], alpha=.7, color=:darkred, norm=true, label=\"posterior\", grid=false)\nvline!([mean(rts)], linestyle=:dash, color=:black, linewidth=2, label=\"data\")","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"(Image: )","category":"page"},{"location":"predictive_distributions/#Posterior-Predictive-Distribution-of-Quantiles","page":"Predictive Distributions","title":"Posterior Predictive Distribution of Quantiles","text":"","category":"section"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"One goal of SSMs is to accurately characterize the distribution of reaction times. The previous example only evaluated one aspective of the model–-namely, the predicted mean. Given the interest in characterizing the shape of the RT distribution, we need a different method. One method for evaluating the model's ability to capture the shape of the distribution is to compare the quantiles. In the example below, the quantiles of the data and model are evaluated at the deciles: 12dots 9. If the model matches the data accurately, the quantiles will fall along the identity line.  ","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"pred_quantiles = predict_distribution(Wald; model, func=compute_quantiles, n_samples=20)\npost_quantile_preds = generated_quantities(pred_quantiles, post_chain)\nq_data = compute_quantiles(rts)\nplot_quantiles(q_data, post_quantile_preds)","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"(Image: )","category":"page"},{"location":"predictive_distributions/","page":"Predictive Distributions","title":"Predictive Distributions","text":"The posterior predictive quantile-quantile plot above shows that the model fits the reaction time distribution well. This close match is to be expected, as we generated the data from the same model. ","category":"page"},{"location":"stDDM/#Starting-time-Drift-Diffusion-Model-(stDDM)","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The relative starting time drift diffusion model (stDDM) characterizes the contributions of multiple unique attributes to the rate of evidence accumulation. Compared to the DDM, which assumes a constant evidence accumulation rate within each trial, the stDDM allows different attributes to enter the evidence accumulation process at various time points relative to one another. By doing so, the stDDM quantifies both the weights given to each attribute and their onset times (Amasino et al., 2019; Barakchian et al., 2021; Chen et al., 2022; Maier et al., 2020; Sullivan and Huettel, 2021).","category":"page"},{"location":"stDDM/#Example","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Example","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"In this example, we will demonstrate how to use the stDDM in a generic two-alternative forced-choice task with two arbitrary attributes.","category":"page"},{"location":"stDDM/#Load-Packages","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Load Packages","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"stDDM/#Create-Model-Object","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Create Model Object","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"In the code below, we will define parameters for the stDDM and create a model object to store the parameter values. ","category":"page"},{"location":"stDDM/#Drift-Rate","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Drift Rate","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The drift rate controls the speed and direction in which information accumulates. Here, each drift coefficient indicates the weighting strengths given to the first and second attributes (e.g., taste and health, payoff and delay, self and other), respectively, to the total drift rate in a given trial, where the drift rate accumulates relative evidence in favor of an option.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"ν = [2.5,2.0]","category":"page"},{"location":"stDDM/#Diffusion-Noise","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Diffusion Noise","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Diffusion noise is the amount of within trial noise in the evidence accumulation process. ","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"σ = 1.0","category":"page"},{"location":"stDDM/#starting-time","page":"Starting-time Drift Diffusion Model (stDDM)","title":"starting time","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The starting time parameter s denotes how much earlier one attribute begins to affect the evidence accumulation process relative to the other(s). If s is negative, attribute 1 evidence is accumulated before attribute 2 evidence; if s is positive, attribute 1 evidence is accumulated after attribute 2 evidence. The absolute value of s indicates the difference in starting times for the two attributes.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"s = 0.10 ","category":"page"},{"location":"stDDM/#Starting-Point","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting Point","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"An indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"z = 0.50","category":"page"},{"location":"stDDM/#Drift-Rates-Dispersion","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Drift Rates Dispersion","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Dispersion parameters of the drift rate are drawn from a multivariate normal distribution, with the mean vector ν describing the distribution of actual drift rates from specific trials. The standard deviation or across-trial variability is captured by the η vector, and the corresponding correlation between the two attributes is denoted by ρ.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"η = [1.0,1.0]\nρ = 0.3","category":"page"},{"location":"stDDM/#Threshold","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Threshold","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The threshold α represents the amount of evidence required to make a decision.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"α = 1.5","category":"page"},{"location":"stDDM/#Non-Decision-Time","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"τ = 0.30","category":"page"},{"location":"stDDM/#stDDM-Constructor","page":"Starting-time Drift Diffusion Model (stDDM)","title":"stDDM Constructor","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Now that values have been asigned to the parameters, we will pass them to stDDM to generate the model object.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"dist = stDDM(;ν, σ, s, z, η, ρ, α, τ,)","category":"page"},{"location":"stDDM/#Simulate-Model","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Simulate Model","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"stDDM/#Compute-Choice-Probability","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Compute Choice Probability","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"stDDM/#Plot-Simulation","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 3.0, length=100))","category":"page"},{"location":"stDDM/#References","page":"Starting-time Drift Diffusion Model (stDDM)","title":"References","text":"","category":"section"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Amasino, D.R., Sullivan, N.J., Kranton, R.E. et al. Amount and time exert independent influences on intertemporal choice. Nat Hum Behav 3, 383–392 (2019). https://doi.org/10.1038/s41562-019-0537-2","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Barakchian, Z., Beharelle, A.R. & Hare, T.A. Healthy decisions in the cued-attribute food choice paradigm have high test-retest reliability. Sci Rep, (2021). https://doi.org/10.1038/s41598-021-91933-6","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Chen, HY., Lombardi, G., Li, SC. et al. Older adults process the probability of winning sooner but weigh it less during lottery decisions. Sci Rep, (2022). https://doi.org/10.1038/s41598-022-15432-y","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Maier, S.U., Raja Beharelle, A., Polanía, R. et al. Dissociable mechanisms govern when and how strongly reward attributes affect decisions. Nat Hum Behav 4, 949–963 (2020). https://doi.org/10.1038/s41562-020-0893-y","category":"page"},{"location":"stDDM/","page":"Starting-time Drift Diffusion Model (stDDM)","title":"Starting-time Drift Diffusion Model (stDDM)","text":"Sullivan, N.J., Huettel, S.A. Healthful choices depend on the latency and rate of information accumulation. Nat Hum Behav 5, 1698–1706 (2021). https://doi.org/10.1038/s41562-021-01154-0","category":"page"},{"location":"turing_simple/#A-Simple-Turing-Model","page":"Simple Bayesian Model","title":"A Simple Turing Model","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"It is possible to use Turing.jl to perform Bayesian parameter estimation on models defined in SequentialSamplingModels.jl. Below, we show you how to estimate the parameters for the Linear Ballistic Accumulator (LBA) and to use it to estimate effects.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"Note that you can easily swap the LBA model from this example for other SSM models simply by changing the names of the parameters.","category":"page"},{"location":"turing_simple/#Load-Packages","page":"Simple Bayesian Model","title":"Load Packages","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing StatsPlots\nusing Random\n\nRandom.seed!(45461)","category":"page"},{"location":"turing_simple/#Generate-Data","page":"Simple Bayesian Model","title":"Generate Data","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"We will use the LBA distribution to simulate data (100 trials) with fixed parameters (those we want to recover only from the data using Bayesian modeling).","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Generate some data with known parameters\ndist = LBA(ν=[3.0, 2.0], A = .8, k = .2, τ = .3)\ndata = rand(dist, 100)","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The rand() function will sample random draws from the distribution, and store that into a named tuple of 2 vectors (one for choice and one for rt). The individual vectors can be accessed by their names using data.choice and data.rt.","category":"page"},{"location":"turing_simple/#Specify-Turing-Model","page":"Simple Bayesian Model","title":"Specify Turing Model","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The code snippet below defines a model in Turing. The model function accepts a tuple containing a vector of choices and a vector of reaction times. The sampling statements define the prior distributions for each parameter. The non-decision time parameter tau must be founded by the minimum reaction time, min_rt. The last sampling statement defines the likelihood of the data given the sampled parameter values.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Specify LBA model\n@model function model_lba(data; min_rt = minimum(data.rt))\n    # Priors\n    ν ~ MvNormal(zeros(2), I * 2)\n    A ~ truncated(Normal(.8, .4), 0.0, Inf)\n    k ~ truncated(Normal(.2, .2), 0.0, Inf)\n    τ  ~ Uniform(0.0, min_rt)\n\n    # Likelihood\n    data ~ LBA(;ν, A, k, τ )\nend","category":"page"},{"location":"turing_simple/#Estimate-the-Parameters","page":"Simple Bayesian Model","title":"Estimate the Parameters","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"Finally, we perform parameter estimation with sample(), which takes the model, and details about the sampling algorithm:","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"model(data): the Turing model with data passed\nNUTS(1000, .65): a sampler object for the No U-Turn Sampler for 1000 warmup samples.\nMCMCThreads(): instructs Turing to run each chain on a separate thread\nn_iterations: the number of iterations performed after warmup\nn_chains: the number of chains","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Estimate parameters\nchain = sample(model_lba(data), NUTS(1000, .85), MCMCThreads(), 1000, 4)","category":"page"},{"location":"turing_simple/#Posterior-Summary","page":"Simple Bayesian Model","title":"Posterior Summary","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"We can compute a description of the posterior distributions.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Summarize posteriors\nsummarystats(chain)","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"As you can see, based on the mean values of the posterior distributions, the original parameters (ν=[3.0, 2.0], A = .8, k = .2, τ = .3) are successfully recovered from the data (the accuracy would increase with more data).","category":"page"},{"location":"turing_simple/#Evaluation","page":"Simple Bayesian Model","title":"Evaluation","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"It is important to verify that the chains converged. We see that the chains converged according to hatr leq 105, and the trace plots below show that the chains look like \"hairy caterpillars\", which indicates the chains did not get stuck. As expected, the posterior distributions are close to the data generating parameter values.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"plot(chain)","category":"page"},{"location":"aDDM/#Attentional-Drift-Diffusion-Model","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion Model","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The attentional drift diffusion model (ADDM; Krajbich, Armel, & Rangel, 2010) describes how attentional processes drive drive decision making. In the ADDM, preference for the currently attended option accrues faster than preference for non-attended options. As with other sequential sampling models, the first option to hit a decision threshold determines the resulting choice and reaction time.","category":"page"},{"location":"aDDM/#Example","page":"Attentional Drift Diffusion (aDDM)","title":"Example","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"using SequentialSamplingModels\nusing StatsBase\nusing Plots\nusing Random","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"In this example, we will develope a ADDM for binary choice and generate its predictions. Unlike many other sequential sampling models, it is necessary to specify the attentional process, or supply fixation patterns from eye tracking data. ","category":"page"},{"location":"aDDM/#Load-Packages","page":"Attentional Drift Diffusion (aDDM)","title":"Load Packages","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"using SequentialSamplingModels\nusing StatsBase\nusing Plots\n\nRandom.seed!(5487)","category":"page"},{"location":"aDDM/#Define-Transition-Type","page":"Attentional Drift Diffusion (aDDM)","title":"Define Transition Type","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"To represent the transition of attention from one option to the other, we will definite a Transition type and constructor. The fields of the Transition type are:","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"state: an index for the current state\nn: the number of states\nmat: an ntimes n transition matrix","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The constructor accepts a transition matrix, extracts the number of states, and initializes the first state randomly with equal probability.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"mutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\nfunction Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end","category":"page"},{"location":"aDDM/#Define-Transition-Matrix","page":"Attentional Drift Diffusion (aDDM)","title":"Define Transition Matrix","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The transition matrix is defined below in the constructor for Transition. As shown in the table below, the model's attention can be in one of three states: option 1, option 2, or non-option, which is any area except the two options. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" option 1 option 2 non-option\noption 1 0.98 0.015 0.005\noption 2 0.015 0.98 0.005\nnon-option 0.45 0.45 0.10","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The transition matrix above embodies the following assumptions:","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Once the model attends to an option, it dwells on the option for some time.\nThere is not a bias for one option over the other.\nThe chance of fixating on a non-option is small, and such fixations are brief when they do occur.\nTransitions are Markovian in that they only depend on the previous state.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"tmat = Transition([.98 .015 .005;\n                    .015 .98 .005;\n                    .45 .45 .1])","category":"page"},{"location":"aDDM/#Attend-Function","page":"Attentional Drift Diffusion (aDDM)","title":"Attend Function","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The function below generates the next attention location based on the previous location. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" function fixate(transition)\n     (;mat,n,state) = transition\n     w = @view mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end","category":"page"},{"location":"aDDM/#Create-Model-Object","page":"Attentional Drift Diffusion (aDDM)","title":"Create Model Object","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The code snippets assign values to parameters of the ADDM and create a model object.","category":"page"},{"location":"aDDM/#Drift-Rate-Components","page":"Attentional Drift Diffusion (aDDM)","title":"Drift Rate Components","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The ADDM has two drift rates components corresponding to the utlity of each option. To form the drift rate, each component is weighted by non-attention bias and then a difference is computed.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"ν = [6.0,5.0]","category":"page"},{"location":"aDDM/#Threshold","page":"Attentional Drift Diffusion (aDDM)","title":"Threshold","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The threshold hold represents the amount of evidence required to make a decision. This parameter is typically fixed at alpha = 1.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"α = 1.0","category":"page"},{"location":"aDDM/#Starting-Point","page":"Attentional Drift Diffusion (aDDM)","title":"Starting Point","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The starting point of the evidence accumulation process is denoted z and is typically fixed to 0.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"z = 0.0","category":"page"},{"location":"aDDM/#Non-Attend-Bias","page":"Attentional Drift Diffusion (aDDM)","title":"Non-Attend Bias","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The non-attend bias parameter theta determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, theta=1. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"θ = 0.30","category":"page"},{"location":"aDDM/#Diffusion-Noise","page":"Attentional Drift Diffusion (aDDM)","title":"Diffusion Noise","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Diffusion noise, sigma represents intra-trial noise during the evidence accumulation process.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"σ = 0.02","category":"page"},{"location":"aDDM/#Drift-Rate-Scalar","page":"Attentional Drift Diffusion (aDDM)","title":"Drift Rate Scalar","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The drift rate scalar controls how quickly evidence accumulates for each option. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Δ = 0.0004 ","category":"page"},{"location":"aDDM/#Model-Object","page":"Attentional Drift Diffusion (aDDM)","title":"Model Object","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Finally, we pass the parameters to the aDDM constructor to initialize the model.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" model = aDDM(; ν, α, z, θ, σ, Δ)","category":"page"},{"location":"aDDM/#Simulate-Model","page":"Attentional Drift Diffusion (aDDM)","title":"Simulate Model","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. The rand function accepts the model object, the number of simulated trials, the fixate function, and the transition matrix object. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" choices,rts = rand(model, 10_000, tmat; fixate)","category":"page"},{"location":"aDDM/#Plot-Simulation","page":"Attentional Drift Diffusion (aDDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Finally, we can generate histograms of the reaction times for each decision option. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"histogram(model; model_args=(;tmat), model_kwargs=(;fixate))\nplot!(model; model_args=(;tmat), model_kwargs=(;fixate), t_range=range(0.0, 5, length=100), xlims=(0,5))","category":"page"},{"location":"aDDM/#References","page":"Attentional Drift Diffusion (aDDM)","title":"References","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Krajbich, I., Armel, C., & Rangel, A. (2010). Visual fixations and the computation and comparison of value in simple choice. Nature neuroscience, 13(10), 1292-1298.","category":"page"},{"location":"basic_plot_example/#Basic-Example","page":"Basic Example","title":"Basic Example","text":"","category":"section"},{"location":"basic_plot_example/","page":"Basic Example","title":"Basic Example","text":"SequentialSamplingModels.jl automatically loads plotting functionality for SSMs when Plots is active in your Julia session . As a simple starting point, the code block below illustrates some basic functionality:","category":"page"},{"location":"basic_plot_example/","page":"Basic Example","title":"Basic Example","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(85)\n\nν = [1.0,0.50]\nk = 0.50\nA = 1.0\nτ = 0.30\n\ndist = RDM(;ν, k, A, τ)\nhistogram(dist; xlims=(0,2.5))\nplot!(dist; t_range=range(.301, 2.5, length=100))","category":"page"},{"location":"basic_plot_example/","page":"Basic Example","title":"Basic Example","text":"You can overwrite the default plot options by passing keyword arguments. The code block below provides an example:","category":"page"},{"location":"basic_plot_example/","page":"Basic Example","title":"Basic Example","text":"using SequentialSamplingModels\nusing Plots\nusing Random \nRandom.seed!(85)\n\nν = [1.0,0.50]\nk = 0.50\nA = 1.0\nτ = 0.30\n\ndist = RDM(;ν, k, A, τ)\nhistogram(dist; xlims=(0,2.5))\nplot!(dist; t_range=range(.301, 2.5, length=100), color=:darkorange)","category":"page"},{"location":"poisson_race/#Poisson-Race","page":"Poisson Race","title":"Poisson Race","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The Poisson race model is one of the first sequential sampling models, with origins dating back to 1962. In this model, evidence accumulates in discrete steps until the first accumulator reaches a threshold. The time between increments follows an exponential distribution. The first passage time follows a gamma distribution because it is the sum of exponential random variables.  ","category":"page"},{"location":"poisson_race/#Example","page":"Poisson Race","title":"Example","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"In this example, we will demonstrate how to use the Poisson race model in a generic two alternative forced choice task.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"using SequentialSamplingModels\nusing Plots \nusing Random","category":"page"},{"location":"poisson_race/#Load-Packages","page":"Poisson Race","title":"Load Packages","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The first step is to load the required packages.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"using SequentialSamplingModels\nusing Plots \nusing Random\n\nRandom.seed!(65)","category":"page"},{"location":"poisson_race/#Create-Model-Object","page":"Poisson Race","title":"Create Model Object","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"In the code below, we will define parameters for the Poisson race and create a model object to store the parameter values.","category":"page"},{"location":"poisson_race/#Mean-processing-time","page":"Poisson Race","title":"Mean processing time","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The parameter nu represents the mean processing of each count. Note that nu = frac1lambda, where lambda is the rate parameter. ","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"ν = [.04, .05]","category":"page"},{"location":"poisson_race/#Threshold","page":"Poisson Race","title":"Threshold","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The parameter alpha is a vector of thresholds. Each threshold is an integer because it represents a discrete count.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"α = [4,4]","category":"page"},{"location":"poisson_race/#Non-Decision-Time","page":"Poisson Race","title":"Non-Decision Time","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"Non-decision time is an additive constant representing encoding and motor response time.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"τ = 0.30","category":"page"},{"location":"poisson_race/#Poisson-race-Constructor","page":"Poisson Race","title":"Poisson race Constructor","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"Now that values have been asigned to the parameters, we will pass them to LNR to generate the model object.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"dist = PoissonRace(;ν, α, τ)","category":"page"},{"location":"poisson_race/#Simulate-Model","page":"Poisson Race","title":"Simulate Model","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"poisson_race/#Compute-PDF","page":"Poisson Race","title":"Compute PDF","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"poisson_race/#Compute-Log-PDF","page":"Poisson Race","title":"Compute Log PDF","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"poisson_race/#Compute-Choice-Probability","page":"Poisson Race","title":"Compute Choice Probability","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The choice probability Pr(C=c) is computed by passing the model and choice index to cdf along with a large value for time as the second argument.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"cdf(dist, 1, Inf)","category":"page"},{"location":"poisson_race/#Plot-Simulation","page":"Poisson Race","title":"Plot Simulation","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 1, length=100))","category":"page"},{"location":"poisson_race/#References","page":"Poisson Race","title":"References","text":"","category":"section"},{"location":"poisson_race/","page":"Poisson Race","title":"Poisson Race","text":"LaBerge, D. A. (1962). A recruitment model of simple behavior. Psychometrika, 27, 375-395.","category":"page"},{"location":"#SequentialSamplingModels.jl","page":"Home","title":"SequentialSamplingModels.jl","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a unified interface for simulating and evaluating popular sequential sampling models (SSMs), which integrates with the following packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributions.jl: functions for probability distributions\nPigeons.jl: Bayesian parameter estimation and Bayes factors\nPlots.jl: extended plotting tools for SSMs\nTuring.jl: Bayesian parameter estimation","category":"page"},{"location":"#Background","page":"Home","title":"Background","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SSMs, also known as an evidence accumulation models, are a broad class of dynamic models of human decision making in which evidence for each option accumulates until the evidence for one option reaches a decision threshold. Models within this class make different assumptions about the nature of the evidence accumulation process. An example of the evidence accumulation process is illustrated below for the Leaking Competing Accumulator (LCA):","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Plots\nusing Random\nusing Colors\nusing SequentialSamplingModels\nusing SequentialSamplingModels: increment!\nRandom.seed!(8437)\n\nparms = (\n    α = 1.5,\n    β=0.20,\n    λ=0.10,\n    ν=[2.5,2.0],\n    τ=.30,\n    σ=1.0\n)\nmodel = LCA(; parms...)\ntime_steps,evidence = simulate(model)\nlca_plot = plot(time_steps, evidence, xlabel=\"Time (seconds)\", ylabel=\"Evidence\",\n    label=[\"option1\" \"option2\"], ylims=(0, 2.0), grid=false, linewidth = 2,\n    color =[RGB(148/255, 90/255, 147/255) RGB(90/255, 112/255, 148/255)])\nhline!(lca_plot, [model.α], color=:black, linestyle=:dash, label=\"threshold\", linewidth = 2)\nsavefig(\"lca_plot.png\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install a stable version of SequentialSamplingModels by running the following in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add SequentialSamplingModels","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package can then be loaded with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SequentialSamplingModels","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The example belows shows how to perform three common tasks:","category":"page"},{"location":"","page":"Home","title":"Home","text":"generate simulated data\nevaluate the log likelihood of data\nplot the predictions of the model","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\nRandom.seed!(2054)\n\n# Create LBA distribution with known parameters\ndist = LBA(; ν=[2.75,1.75], A=0.8, k=0.5, τ=0.25)\n# Sample 10,000 simulated data from the LBA\nsim_data = rand(dist, 10_000)\n# compute log likelihood of simulated data \nlogpdf(dist, sim_data)\n# Plot the RT distribution for each choice\nhistogram(dist)\nplot!(dist; t_range=range(.3,2.5, length=100), xlims=(0, 2.5))","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Evans, N. J. & Wagenmakers, E.-J. Evidence accumulation models: Current limitations and future directions. Quantitative Methods for Psychololgy 16, 73–90 (2020).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Forstmann, B. U., Ratcliff, R., & Wagenmakers, E. J. (2016). Sequential sampling models in cognitive neuroscience: Advantages, applications, and extensions. Annual Review of Psychology, 67, 641-666.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Jones, M., & Dzhafarov, E. N. (2014). Unfalsifiability and mutual translatability of major modeling schemes for choice reaction time. Psychological Review, 121(1), 1.","category":"page"},{"location":"developer_guide/#Style-Guide","page":"Developer Guide","title":"Style Guide","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"In most cases, code written in SequentialSamplingModels.jl follows the guidelines specified in the blue style guide for Julia. Please use the blue style guide or existing code as a guide, and deviate from the guides only when there is a compelling reason to do so.","category":"page"},{"location":"developer_guide/#Documentation","page":"Developer Guide","title":"Documentation","text":"","category":"section"},{"location":"developer_guide/#Docstrings","page":"Developer Guide","title":"Docstrings","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide docstrings for methods and types which are part of the API. For example, the doc strings for each model should adhere to the following format:","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"    LNR{T<:Real} <: SSM2D\n\nA lognormal race model object. \n\n# Parameters \n\n- `μ`: a vector of means in log-space\n- `σ`: a standard deviation parameter in log-space\n- `ϕ`: a encoding-response offset\n\n# Constructors\n\n    LNR(μ, σ, ϕ)\n\n    LNR(;μ, σ, ϕ)\n\n# Example\n\n```julia\nusing SequentialSamplingModels\ndist = LNR(μ=[-2,-3], σ=1.0, ϕ=.3)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n```\n# References\n\nRouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015). \nThe lognormal race: A cognitive-process model of choice and latency with desirable \npsychometric properties. Psychometrika, 80(2), 491-513.","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"For the benefit of other developers, err on the side of providing doc strings for internal methods. The doc strings should provide the function signature, a high level explanation of the function, and a description of arguments and keywords. Please include references as appropriate. ","category":"page"},{"location":"developer_guide/#Model-Example","page":"Developer Guide","title":"Model Example","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide a detailed model walk through for the online documentation under the section Models. The walk through should include a description of the model, an explanation of the model parameters, and a demonstration showing the pdf overlayed on the histogram (if applicable). Please use existing model examples as a template. ","category":"page"},{"location":"developer_guide/#API","page":"Developer Guide","title":"API","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Only export (make public) types and methods that are intended for users. Other methods are implementational details for interal use. ","category":"page"},{"location":"developer_guide/#Unit-tests","page":"Developer Guide","title":"Unit tests","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide unit tests for most (if not all) methods. When possible, programatically test a method over a wide range of inputs. If you find a bug, write a unit test for the bug to prevent regressions. When possible, compare methods to those defined in established and trusted packages in other languages.  ","category":"page"},{"location":"developer_guide/#Parameter-Naming-Conventions","page":"Developer Guide","title":"Parameter Naming Conventions","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"To ensure consistency across models, please use the following variable names:","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"use ν for drift rates\nuse α for decision boundaries\nuse Δt for a discrete time step\nuse σ for within-trial noise of drift rate  \nuse τ for non-decision time\nuse z for evidence starting point\nuse η for across-trial noise of drift rate","category":"page"}]
}
