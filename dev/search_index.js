var documenterSearchIndex = {"docs":
[{"location":"turing_advanced/#Estimate-Effect-on-Drift-Rate","page":"Advanced Model Specification","title":"Estimate Effect on Drift Rate","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"This advanced example illustrates how to estimate the effect of an experimental condition on the drift rate parameter. The drift rate could be manipulated in various ways. For example, the drift rate could be manipulated by varying the similarity of visual stimuli, or emphasizing speed or accuracy in task instructions.","category":"page"},{"location":"turing_advanced/#Generate-Data","page":"Advanced Model Specification","title":"Generate Data","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this example, we will get closer to real use-cases by starting with the data stored in a DataFrame. This dataframe will be a combination of data generated from two different distributions with different parameters, corresponding to two experimental conditions (e.g., Speed vs. Accuracy).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing Distributions\nusing DataFrames\nusing StatsPlots\nusing StatsModels\nusing KernelDensity\n\n# Generate data with different drifts for two conditions A vs. B\nRandom.seed!(6)\n\nn_obs = 50\ndf1 = DataFrame(rand(LBA(ν=[1.5, 0.5], A=0.5, k=0.2, τ=0.3), n_obs))\ndf2 = DataFrame(rand(LBA(ν=[2.5, 1.5], A=0.5, k=0.2, τ=0.3), n_obs))\ndf = vcat(df1, df2)\ndf.condition = repeat([\"A\", \"B\"], inner=n_obs)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"These 2 conditions A and B differ on their drift rates ([1.5, 0.5] vs. [2.5, 1.5]). In other words, the effect of condition B over condition A (the baseline condition, i.e., the intercept) is [1, 1] (because both drift rates increase by 1 between condition A and B).","category":"page"},{"location":"turing_advanced/#Exclude-Outliers","page":"Advanced Model Specification","title":"Exclude Outliers","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we are going to remove outliers, i.e., implausible RTs (RTs that likely do not reflect the processes of interest). In our case, we consider that RTs shorter than 0.2 seconds are too short for the cognitive process of interest to unfold, and that RTs longer than 3 seconds are too long to carry meaningful information.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Remove outliers\ndf = df[(df.rt .> 0.2).&(df.rt .< 3), :]\nfirst(df)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Note that standard outlier detection methods, such as z-scores (mean +- SD), are not necessarily appropriate for RTs, given the skewed nature of their distribution. Their asymmetric distribution is in fact accounted for by the models that we use. The outlier exclusion done here is more theory-driven (i.e., excluding extreme trials that likely do not reflect well the cognitive processes of interest) than data-driven (to better fit the model). That said, outlier exclusion should always be explicitly documented and justified.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"note: Note\nFor users coming from other languages, note the usage of the vectorization dot . in front of the < and > symbols. This means that we want to apply the logical test for all individual elements of the rt vector.","category":"page"},{"location":"turing_advanced/#Visualize-Data","page":"Advanced Model Specification","title":"Visualize Data","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can visualize the RT distribution for each response choice by looping through the conditions.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Make histogram\nhistogram(layout=(2, 1), xlabel=\"Reaction Time\", ylims=(0, 60), xlims=(0, 2), legend=false)\nfor (i, cond) in enumerate([\"A\", \"B\"])\n    histogram!(df.rt[(df.choice.==1).&(df.condition.==cond)], subplot=1, color=[:blue, :red][i], alpha=0.5, bins=range(0, 3, length=25))\n    histogram!(df.rt[(df.choice.==2).&(df.condition.==cond)], subplot=2, color=[:blue, :red][i], alpha=0.5, bins=range(0, 2, length=25))\nend\nplot!()","category":"page"},{"location":"turing_advanced/#Format-Predictors","page":"Advanced Model Specification","title":"Format Predictors","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"One additional step that we need to do here is to transform the dataframe into an input suited for modelling with Turing. For that, we will leverage the features of StatsModels to build an input matrix based on a formula.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Format input data\nf = @formula(rt ~ 1 + condition)\nf = apply_schema(f, schema(f, df))\n\n_, predictors = coefnames(f)\nX = modelmatrix(f, df)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this case, the model matrix is pretty simple: the key part is the second column that is simply a binary vector indicating whenever condition == \"B\". However, using formulas is a good way of dealing with more complex model specifications.","category":"page"},{"location":"turing_advanced/#Specify-Turing-Model","page":"Advanced Model Specification","title":"Specify Turing Model","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"In this model, the priors for the parameters that we want to vary between conditions are split, with one prior for their intercept (condition A) and another for the effect of condition B (relative to the intercept).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Because the drift parameters is a vector of length 2, the priors for both the intercept and condition effect drifts have themselves to be a vector of 2 distributions, which is done via filldist(prior_distribution, 2).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we need to specify these parameters as the result of a (linear) equation. Note that:","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We have added a keyword argument, condition, to let the user pass the condition data vector.\nSince we're computing parameters as the results of an equation, we need to use a for loop that loops through all the observations.\nBecause the priors for the drift is a filldist (i.e., a vector of distributions), we need to broadcast the addition (.+ instead of +).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"@model function model_lba(data; min_rt=0.2, condition=nothing)\n    # Priors for auxiliary parameters\n    A ~ truncated(Normal(0.8, 0.4), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    tau ~ Uniform(0.0, min_rt)\n\n    # Priors for coefficients\n    drift_intercept ~ filldist(Normal(0, 1), 2)\n    drift_condition ~ filldist(Normal(0, 1), 2)\n\n    for i in 1:length(data)\n        drifts = drift_intercept .+ drift_condition * condition[i]\n        data[i] ~ LBA(; τ=tau, A=A, k=k, ν=drifts)\n    end\nend","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Importantly, although we have the data as a dataframe, we will need to convert to a tuple, as it is the shape that the LBA() distribution expects. However, since we're iterating on each observation, we need to come up with an indexable version of the data: a vector of tuples.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Format the data to match the input type\ndata = [(choice=df.choice[i], rt=df.rt[i]) for i in 1:nrow(df)]","category":"page"},{"location":"turing_advanced/#Prior-Predictive-Check","page":"Advanced Model Specification","title":"Prior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/#Sample-from-Priors","page":"Advanced Model Specification","title":"Sample from Priors","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Before we fit the model, we want to inspect our priors to make sure that they are okay. To do that, we sample the model parameters from priors only. Note that condition is supplied as the 2nd column of the model matrix.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"chain = sample(model_lba(data; min_rt=minimum(df.rt), condition=X[:, 2]), Prior(), 1000)\nplot(chain; size=(800, 1200))","category":"page"},{"location":"turing_advanced/#Plot-Prior-Predictive-Check","page":"Advanced Model Specification","title":"Plot Prior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"The next step is to generate predictions from this model (i.e., from the priors). For this, we need to pass a dataset with empty (missing) values. Since the data used above was a vector (of tuples) of length 1000, we will create a vector of (missing) of the same length.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can then use the predict() method to generate predictions from this model. However, because the most of SequentialSamplingModels distributions return a tuple (choice and RT), the predicted output has the two types of variables mixed together. We can delineate the two by taking every 2nd values to get the predicted choice and RTs, respectively.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"datamissing = [(missing) for i in 1:nrow(df)]\n\npred = predict(model_lba(datamissing; min_rt=minimum(df.rt), condition=X[:, 2]), chain)\n\npriorpred_choice = Array(pred)[:, 1:2:end]\npriorpred_rt = Array(pred)[:, 2:2:end]","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"These objects have arrays of size 10,000 x 1000 : with 10,000 draws for each of the 1000 observations.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can plot the predicted distributions by looping through a number of draws (e.g., 100), and then plotting the density for each condition and each choice.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"plot(layout=(2, 1), xlabel=\"Reaction Time\", xlims = (0, 3), ylim=(0, 5), legend = false)\nfor i in 1:100\n    choice = priorpred_choice[i, :]\n    rt = priorpred_rt[i, :]\n    for (j, cond) in enumerate([0, 1])\n        U1 = kde(rt[(choice .== 1) .& (X[:, 2] .== cond)], boundary=(0, 5))\n        plot!(U1.x, U1.density, subplot=1, color = [:red, :blue][j], alpha=0.1)\n        U2 = kde(rt[(choice .== 2) .& (X[:, 2] .== cond)], boundary=(0, 5))\n        plot!(U2.x, U2.density, subplot=2, color = [:red, :blue][j], alpha=0.1)\n    end\nend\nplot!()","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"We can see that the bulk of the predicted RTs fall within 0 - 1.5 seconds, which is realistic, but that the same time it's all over the place, which means that the priors are not too informative.","category":"page"},{"location":"turing_advanced/#Parameters-Estimation","page":"Advanced Model Specification","title":"Parameters Estimation","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"chain = sample(model_lba(data; min_rt=minimum(df.rt), condition=X[:, 2]), NUTS(), 1000)\n\nsummarystats(chain)","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"plot(chain; size = (800,1200))","category":"page"},{"location":"turing_advanced/#Posterior-Predictive-Check","page":"Advanced Model Specification","title":"Posterior Predictive Check","text":"","category":"section"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we will run a posterior predictive check by first sampling from the posteriors. For that, we will re-use the code for the prior predictive check, including the datamissing empty data.","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Sample from posteriors\npred = predict(model_lba(datamissing; min_rt=minimum(df.rt), condition=X[:, 2]), chain)\npred_choice = Array(pred)[:, 1:2:end]\npred_rt = Array(pred)[:, 2:2:end]","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"Next, we will plot the predicted distributions on top of the observed distribution of data (the thick black lines).","category":"page"},{"location":"turing_advanced/","page":"Advanced Model Specification","title":"Advanced Model Specification","text":"# Observed density\nplot(layout=(2, 1), xlabel=\"Reaction Time\", xlims=(0, 2.5), legend=false)\nfor cond in [\"A\", \"B\"]\n    d_A = kde(df.rt[(df.choice.==1).&(df.condition.==cond)], boundary=(0, 5))\n    plot!(d_A.x, d_A.density, subplot=1, color=:black, linewidth=3)\n    d_B = kde(df.rt[(df.choice.==2).&(df.condition.==cond)], boundary=(0, 5))\n    plot!(d_B.x, d_B.density, subplot=2, color=:black, linewidth=3)\nend\n\n# Predicted densities\nfor i in 1:100\n    choice = pred_choice[i, :]\n    rt = pred_rt[i, :]\n    for (j, cond) in enumerate([0, 1])\n        U1 = kde(rt[(choice.==1).&(X[:, 2].==cond)], boundary=(0, 5))\n        plot!(U1.x, U1.density, subplot=1, color=[:red, :blue][j], alpha=0.05)\n        U2 = kde(rt[(choice.==2).&(X[:, 2].==cond)], boundary=(0, 5))\n        plot!(U2.x, U2.density, subplot=2, color=[:red, :blue][j], alpha=0.05)\n    end\nend\nplot!()","category":"page"},{"location":"Ratcliff_DDM/#Ratcliff-Diffusion-Model","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The Ratcliff Diffusion Model (Ratcliff DDM; Ratcliff et al., 2016) is similar to the DDM. Like the DDM, the model assumes that evidence accumulates over time, starting from a certain position, until it crosses one of two boundaries and triggers the corresponding response (Ratcliff & McKoon, 2008; Ratcliff & Rouder, 1998; Ratcliff & Smith, 2004). The drift rate (ν) determines the rate at which the accumulation process approaches a decision boundary, representing the relative evidence for or against a specific response. The distance between the two decision boundaries (referred to as the evidence threshold, α) influences the amount of evidence required before executing a response. Non-decision-related components, including perceptual encoding, movement initiation, and execution, are accounted for in the DDM and reflected in the τ parameter. Lastly, the model incorporates a bias in the evidence accumulation process through the parameter z, affecting the starting point of the drift process in relation to the two boundaries. The z parameter in DDM is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"However, the model differs in the inclusion of across-trial variability parameters. These parameters were developed to explain specific discrepancies between the DDM and experimental data (Anderson, 1960; Laming, 1968; Blurton et al., 2017). The data exhibited a difference in mean RT between correct and error responses that could not be captured by the DDM. As a result, two parameters for across-trial variability were introduced to explain this difference: across-trial variability in the starting point to explain fast errors (Laming, 1968), and across-trial variability in drift rate to explain slow errors (Ratcliff, 1978; Ratcliff and Rouder, 1998). Additionally, the DDM also showed a sharper rise in the leading edge of the response time distribution than observed in the data. To capture this leading edge effect, across-trial variability in non-decision time was introduced. ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Previous work has validated predictions of these across-trial variability parameters (Wagenmakers et al., 2009). When compared to the DDM, the Ratcliff DDM improves the fit to the data. Researchers now often assume that the core parameters of sequential sampling models, such as drift rates, non-decision times, and starting points vary between trials.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"One last parameter is the within-trial variability in drift rate (σ), or the diffusion coefficient. The diffusion coefficient is the standard deviation of the evidence accumulation process within one trial. It is a scaling parameter and by convention it is kept fixed. Following Navarro & Fuss, (2009), we use the σ = 1 version.","category":"page"},{"location":"Ratcliff_DDM/#Example","page":"Ratcliff Diffusion Model","title":"Example","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"In this example, we will demonstrate how to use the DDM in a generic two alternative forced choice task.","category":"page"},{"location":"Ratcliff_DDM/#Load-Packages","page":"Ratcliff Diffusion Model","title":"Load Packages","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The first step is to load the required packages.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"using SequentialSamplingModels\nusing Plots\nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"Ratcliff_DDM/#Create-Model-Object","page":"Ratcliff Diffusion Model","title":"Create Model Object","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"In the code below, we will define parameters for the DDM and create a model object to store the parameter values. ","category":"page"},{"location":"Ratcliff_DDM/#Drift-Rate","page":"Ratcliff Diffusion Model","title":"Drift Rate","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of drift rate. Standard deviation of a normal distribution with mean v describing the distribution of actual drift rates from specific trials. Values different from 0 can predict slow errors. Typical range: 0 < η < 2. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"ν=1.0\nη = 0.16","category":"page"},{"location":"Ratcliff_DDM/#Boundary-Separation","page":"Ratcliff Diffusion Model","title":"Boundary Separation","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The amount of information that is considered for a decision. Large values indicates response caution. Typical range: 0.5 < α < 2","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"α = 0.80","category":"page"},{"location":"Ratcliff_DDM/#Non-Decision-Time","page":"Ratcliff Diffusion Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of non-decisional components. Range of a uniform distribution with mean τ + st/2 describing the distribution of actual τ values across trials. Accounts for response times below t0. Reduces skew of predicted RT distributions. Typical range: 0 < τ < 0.2. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"τ = 0.30\nst = 0.10","category":"page"},{"location":"Ratcliff_DDM/#Starting-Point","page":"Ratcliff Diffusion Model","title":"Starting Point","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"An indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Across-trial-variability of starting point. Range of a uniform distribution with mean z describing the distribution of actual starting points from specific trials. Values different from 0 can predict fast errors. Typical range: 0 < sz < 0.5. Default is 0.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"z = 0.25\nsz = 0.05","category":"page"},{"location":"Ratcliff_DDM/#Ratcliff-Diffusion-Model-Constructor","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model Constructor","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Now that values have been assigned to the parameters, we will pass them to RatcliffDDM to generate the model object.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"dist = DDM(ν, α, τ, z)","category":"page"},{"location":"Ratcliff_DDM/#Simulate-Model","page":"Ratcliff Diffusion Model","title":"Simulate Model","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"Ratcliff_DDM/#Compute-PDF","page":"Ratcliff Diffusion Model","title":"Compute PDF","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"Ratcliff_DDM/#Compute-Log-PDF","page":"Ratcliff Diffusion Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"Ratcliff_DDM/#Plot-Simulation","page":"Ratcliff Diffusion Model","title":"Plot Simulation","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"# rts for option 1\nrts1 = rts[choices .== 1]\n# rts for option 2 \nrts2 = rts[choices .== 2]\n# probability of choosing 1\np1 = length(rts1) / length(rts)\nt_range = range(.30, 2, length=100)\n# pdf for choice 1\npdf1 = pdf.(dist, (1,), t_range)\n# pdf for choice 2\npdf2 = pdf.(dist, (2,), t_range)\n# histogram of retrieval times\nhist = histogram(layout=(2,1), leg=false, grid=false,\n     xlabel=\"Reaction Time\", ylabel=\"Density\", xlims = (0,1.5))\nhistogram!(rts1, subplot=1, color=:grey, bins = 200, norm=true, title=\"Choice 1\")\nplot!(t_range, pdf1, subplot=1, color=:darkorange, linewidth=2)\nhistogram!(rts2, subplot=2, color=:grey, bins = 150, norm=true, title=\"Choice 2\")\nplot!(t_range, pdf2, subplot=2, color=:darkorange, linewidth=2)\n# weight histogram according to choice probability\nhist[1][1][:y] *= p1\nhist[2][1][:y] *= (1 - p1)\nhist","category":"page"},{"location":"Ratcliff_DDM/#References","page":"Ratcliff Diffusion Model","title":"References","text":"","category":"section"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Navarro, D., & Fuss, I. (2009). Fast and accurate calculations for first-passage times in Wiener diffusion models. https://doi.org/10.1016/J.JMP.2009.02.003","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922. https://doi.org/10.1162/neco.2008.12-06-420","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347–356. https://doi.org/10.1111/1467-9280.00067","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., & Smith, P. L. (2004). A comparison of sequential sampling models for two-choice reaction time. Psychological Review, 111 2, 333–367. https://doi.org/10.1037/0033-295X.111.2.333","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Ratcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007","category":"page"},{"location":"Ratcliff_DDM/","page":"Ratcliff Diffusion Model","title":"Ratcliff Diffusion Model","text":"Wagenmakers, E.-J. (2009). Methodological and empirical developments for the Ratcliff diffusion model of response times and accuracy. European Journal of Cognitive Psychology, 21(5), 641-671.","category":"page"},{"location":"lba/#Linear-Ballistic-Accumulator","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The Linear Ballistic Accumulator (LBA; Brown & Heathcote, 2008) is a sequential sampling model in which evidence for options races independently. The LBA makes an additional simplification that evidence accumulates in a linear and ballistic fashion, meaning there is no intra-trial noise. Instead, evidence accumulates deterministically and linearly until it hits the threshold.","category":"page"},{"location":"lba/#Example","page":"Linear Ballistic Accumulator (LBA)","title":"Example","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"In this example, we will demonstrate how to use the LBA in a generic two alternative forced choice task. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"lba/#Load-Packages","page":"Linear Ballistic Accumulator (LBA)","title":"Load Packages","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The first step is to load the required packages.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lba/#Create-Model-Object","page":"Linear Ballistic Accumulator (LBA)","title":"Create Model Object","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"lba/#Drift-Rates","page":"Linear Ballistic Accumulator (LBA)","title":"Drift Rates","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The drift rates control the speed with which information accumulates. Typically, there is one drift rate per option. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"ν=[2.75,1.75]","category":"page"},{"location":"lba/#Maximum-Starting-Point","page":"Linear Ballistic Accumulator (LBA)","title":"Maximum Starting Point","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The starting point of each accumulator is sampled uniformly between 0A.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"A = 0.80","category":"page"},{"location":"lba/#Threshold-Maximum-Starting-Point","page":"Linear Ballistic Accumulator (LBA)","title":"Threshold - Maximum Starting Point","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Evidence accumulates until accumulator reaches a threshold alpha = k +A. The threshold is parameterized this way to faciliate parameter estimation and to ensure that A le alpha.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"k = 0.50","category":"page"},{"location":"lba/#Non-Decision-Time","page":"Linear Ballistic Accumulator (LBA)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"τ = 0.30","category":"page"},{"location":"lba/#LBA-Constructor","page":"Linear Ballistic Accumulator (LBA)","title":"LBA Constructor","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Now that values have been asigned to the parameters, we will pass them to LBA to generate the model object.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"dist = LBA(; ν, A, k, τ) ","category":"page"},{"location":"lba/#Simulate-Model","page":"Linear Ballistic Accumulator (LBA)","title":"Simulate Model","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lba/#Compute-PDF","page":"Linear Ballistic Accumulator (LBA)","title":"Compute PDF","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"lba/#Compute-Log-PDF","page":"Linear Ballistic Accumulator (LBA)","title":"Compute Log PDF","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"lba/#Plot-Simulation","page":"Linear Ballistic Accumulator (LBA)","title":"Plot Simulation","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"histogram(dist)\nplot!(dist; t_range=range(.3,2.5, length=100), xlims=(0, 2.5))\n","category":"page"},{"location":"lba/#References","page":"Linear Ballistic Accumulator (LBA)","title":"References","text":"","category":"section"},{"location":"lba/","page":"Linear Ballistic Accumulator (LBA)","title":"Linear Ballistic Accumulator (LBA)","text":"Brown, S. D., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive psychology, 57(3), 153-178.","category":"page"},{"location":"lca/#Leaky-Competing-Accumulator","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The Leaky Competing Accumulator (LCA; Usher & McClelland, 2001) is a sequential sampling model in which evidence for options races independently. The LBA makes an additional simplification that evidence accumulates in a linear and ballistic fashion, meaning there is no intra-trial noise. Instead, evidence accumulates deterministically and linearly until it hits the threshold.","category":"page"},{"location":"lca/#Example","page":"Leaky Competing Accumulator (LCA)","title":"Example","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"In this example, we will demonstrate how to use the LBA in a generic two alternative forced choice task. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"lca/#Load-Packages","page":"Leaky Competing Accumulator (LCA)","title":"Load Packages","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The first step is to load the required packages.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lca/#Create-Model-Object","page":"Leaky Competing Accumulator (LCA)","title":"Create Model Object","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"lca/#Drift-Rates","page":"Leaky Competing Accumulator (LCA)","title":"Drift Rates","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The drift rates control the speed with which information accumulates. Typically, there is one drift rate per option. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"ν = [2.5,2.0]","category":"page"},{"location":"lca/#Threshold","page":"Leaky Competing Accumulator (LCA)","title":"Threshold","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The threshold alpha represents the amount of evidence required to make a decision.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"α = 1.5","category":"page"},{"location":"lca/#Lateral-Inhibition","page":"Leaky Competing Accumulator (LCA)","title":"Lateral Inhibition","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The parameter beta inhibits evidence of competing options proportionally to their evidence value.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"β = 0.20","category":"page"},{"location":"lca/#Leak-Rate","page":"Leaky Competing Accumulator (LCA)","title":"Leak Rate","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The parameter lambda controls the rate with which evidence decays or \"leaks\".","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"λ = 0.10 ","category":"page"},{"location":"lca/#Diffusion-Noise","page":"Leaky Competing Accumulator (LCA)","title":"Diffusion Noise","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Diffusion noise is the amount of within trial noise in the evidence accumulation process. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"σ = 1.0","category":"page"},{"location":"lca/#Time-Step","page":"Leaky Competing Accumulator (LCA)","title":"Time Step","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The time step parameter Delta t is the precision of the discrete time approxmation. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Δt = .001","category":"page"},{"location":"lca/#Non-Decision-Time","page":"Leaky Competing Accumulator (LCA)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"τ = 0.30","category":"page"},{"location":"lca/#LCA-Constructor","page":"Leaky Competing Accumulator (LCA)","title":"LCA Constructor","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Now that values have been asigned to the parameters, we will pass them to LCA to generate the model object.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"dist = LCA(; ν, α, β, λ, τ, σ, Δt)","category":"page"},{"location":"lca/#Simulate-Model","page":"Leaky Competing Accumulator (LCA)","title":"Simulate Model","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lca/#Plot-Simulation","page":"Leaky Competing Accumulator (LCA)","title":"Plot Simulation","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"The code below plots a histogram for each option.","category":"page"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"histogram(dist)","category":"page"},{"location":"lca/#References","page":"Leaky Competing Accumulator (LCA)","title":"References","text":"","category":"section"},{"location":"lca/","page":"Leaky Competing Accumulator (LCA)","title":"Leaky Competing Accumulator (LCA)","text":"Usher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108 3, 550–592. https://doi.org/10.1037/0033-295X.108.3.550","category":"page"},{"location":"lnr/#Lognormal-Race-Model","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The Lognormal Race model (LNR) assumes evidence for each option races independently and that the first passage time for each option is lognormally distributed. One way in which the LNR has been used is to provide a likelihood function for the ACT-R cognitive architecture. An example of such an application can be found in ACTRModels.jl. We will present a simplified version below.","category":"page"},{"location":"lnr/#Example","page":"Lognormal Race Model (LNR)","title":"Example","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"In this example, we will demonstrate how to use the LNR in a generic two alternative forced choice task.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"lnr/#Load-Packages","page":"Lognormal Race Model (LNR)","title":"Load Packages","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The first step is to load the required packages.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"lnr/#Create-Model-Object","page":"Lognormal Race Model (LNR)","title":"Create Model Object","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values.","category":"page"},{"location":"lnr/#Mean-Log-Time","page":"Lognormal Race Model (LNR)","title":"Mean Log Time","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The parameter nu represents the mean processing time of each accumulator in log space.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"ν = [-1,-1.5]","category":"page"},{"location":"lnr/#Log-Standard-Deviation","page":"Lognormal Race Model (LNR)","title":"Log Standard Deviation","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The parameter sigma represents the standard deviation of processing time in log space.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"σ = 0.50","category":"page"},{"location":"lnr/#Non-Decision-Time","page":"Lognormal Race Model (LNR)","title":"Non-Decision Time","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Non-decision time is an additive constant representing encoding and motor response time.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"τ = 0.30","category":"page"},{"location":"lnr/#LNR-Constructor","page":"Lognormal Race Model (LNR)","title":"LNR Constructor","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Now that values have been asigned to the parameters, we will pass them to LNR to generate the model object.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"dist = LNR(ν, σ, τ)","category":"page"},{"location":"lnr/#Simulate-Model","page":"Lognormal Race Model (LNR)","title":"Simulate Model","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"lnr/#Compute-PDF","page":"Lognormal Race Model (LNR)","title":"Compute PDF","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"lnr/#Compute-Log-PDF","page":"Lognormal Race Model (LNR)","title":"Compute Log PDF","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"lnr/#Plot-Simulation","page":"Lognormal Race Model (LNR)","title":"Plot Simulation","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 1, length=100))","category":"page"},{"location":"lnr/#References","page":"Lognormal Race Model (LNR)","title":"References","text":"","category":"section"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Heathcote, A., & Love, J. (2012). Linear deterministic accumulator models of simple choice. Frontiers in psychology, 3, 292.","category":"page"},{"location":"lnr/","page":"Lognormal Race Model (LNR)","title":"Lognormal Race Model (LNR)","text":"Rouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015). The lognormal race: A cognitive-process model of choice and latency with desirable psychometric properties. Psychometrika, 80, 491-513.","category":"page"},{"location":"DDM/#Diffusion-Decision-Model","page":"Drift Diffusion Model (DDM)","title":"Diffusion Decision Model","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The Diffusion Decision Model (DDM; Ratcliff et al., 2016) is a model of speeded decision-making in two-choice tasks. The DDM assumes that evidence accumulates over time, starting from a certain position, until it crosses one of two boundaries and triggers the corresponding response (Ratcliff & McKoon, 2008; Ratcliff & Rouder, 1998; Ratcliff & Smith, 2004). Like other Sequential Sampling Models, the DDM comprises psychologically interpretable parameters that collectively form a generative model for reaction time distributions of both responses.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The drift rate (ν) determines the rate at which the accumulation process approaches a decision boundary, representing the relative evidence for or against a specific response. The distance between the two decision boundaries (referred to as the evidence threshold, α) influences the amount of evidence required before executing a response. Non-decision-related components, including perceptual encoding, movement initiation, and execution, are accounted for in the DDM and reflected in the τ parameter. Lastly, the model incorporates a bias in the evidence accumulation process through the parameter z, affecting the starting point of the drift process in relation to the two boundaries. The z parameter in DDM is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"One last parameter is the within-trial variability in drift rate (σ), or the diffusion coefficient. The diffusion coefficient is the standard deviation of the evidence accumulation process within one trial. It is a scaling parameter and by convention it is kept fixed. Following Navarro & Fuss, (2009), we use the σ = 1 version.","category":"page"},{"location":"DDM/#Example","page":"Drift Diffusion Model (DDM)","title":"Example","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"In this example, we will demonstrate how to use the DDM in a generic two alternative forced choice task.","category":"page"},{"location":"DDM/#Load-Packages","page":"Drift Diffusion Model (DDM)","title":"Load Packages","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"DDM/#Create-Model-Object","page":"Drift Diffusion Model (DDM)","title":"Create Model Object","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"In the code below, we will define parameters for the DDM and create a model object to store the parameter values. ","category":"page"},{"location":"DDM/#Drift-Rate","page":"Drift Diffusion Model (DDM)","title":"Drift Rate","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"ν=1.0","category":"page"},{"location":"DDM/#Boundary-Separation","page":"Drift Diffusion Model (DDM)","title":"Boundary Separation","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The amount of information that is considered for a decision. Large values indicates response caution. Typical range: 0.5 < α < 2","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"α = 0.80","category":"page"},{"location":"DDM/#Non-Decision-Time","page":"Drift Diffusion Model (DDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 ","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"τ = 0.30","category":"page"},{"location":"DDM/#Starting-Point","page":"Drift Diffusion Model (DDM)","title":"Starting Point","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"An indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"z = 0.50","category":"page"},{"location":"DDM/#DDM-Constructor","page":"Drift Diffusion Model (DDM)","title":"DDM Constructor","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Now that values have been assigned to the parameters, we will pass them to DDM to generate the model object.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"dist = DDM(ν, α, τ, z)","category":"page"},{"location":"DDM/#Simulate-Model","page":"Drift Diffusion Model (DDM)","title":"Simulate Model","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"DDM/#Compute-PDF","page":"Drift Diffusion Model (DDM)","title":"Compute PDF","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"DDM/#Compute-Log-PDF","page":"Drift Diffusion Model (DDM)","title":"Compute Log PDF","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"DDM/#Plot-Simulation","page":"Drift Diffusion Model (DDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"histogram(dist)\nplot!(dist; t_range=range(.301, 1, length=100))","category":"page"},{"location":"DDM/#References","page":"Drift Diffusion Model (DDM)","title":"References","text":"","category":"section"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Navarro, D., & Fuss, I. (2009). Fast and accurate calculations for first-passage times in Wiener diffusion models. https://doi.org/10.1016/J.JMP.2009.02.003","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922. https://doi.org/10.1162/neco.2008.12-06-420","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347–356. https://doi.org/10.1111/1467-9280.00067","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., & Smith, P. L. (2004). A comparison of sequential sampling models for two-choice reaction time. Psychological Review, 111 2, 333–367. https://doi.org/10.1037/0033-295X.111.2.333","category":"page"},{"location":"DDM/","page":"Drift Diffusion Model (DDM)","title":"Drift Diffusion Model (DDM)","text":"Ratcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007","category":"page"},{"location":"wald_mixture/#Wald-Mixture-Model","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The Wald mixture model is a sequential sampling model for single choice decisions. It extends the Wald model by allowing the drift rate to vary randomly across trials. ","category":"page"},{"location":"wald_mixture/#Example","page":"Wald Mixture Model","title":"Example","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"In this example, we will demonstrate how to use the Wald mixture model in a generic single choice decision task. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"wald_mixture/#Load-Packages","page":"Wald Mixture Model","title":"Load Packages","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The first step is to load the required packages.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"wald_mixture/#Create-Model-Object","page":"Wald Mixture Model","title":"Create Model Object","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"wald_mixture/#Drift-Rate","page":"Wald Mixture Model","title":"Drift Rate","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter nu represents the evidence accumulation rate.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"ν = 3.0","category":"page"},{"location":"wald_mixture/#Drift-Rate-2","page":"Wald Mixture Model","title":"Drift Rate","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter nu represents the evidence accumulation rate.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"ν = 3.0","category":"page"},{"location":"wald_mixture/#Drift-Rate-Variability","page":"Wald Mixture Model","title":"Drift Rate Variability","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter sigma represents the standard deviation of the evidence accumulation rate across trials.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"σ = 0.20","category":"page"},{"location":"wald_mixture/#Threshold","page":"Wald Mixture Model","title":"Threshold","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The parameter alpha the amount of evidence required to make a decision.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"α = 0.50","category":"page"},{"location":"wald_mixture/#Non-Decision-Time","page":"Wald Mixture Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"τ = 0.130","category":"page"},{"location":"wald_mixture/#Wald-Constructor","page":"Wald Mixture Model","title":"Wald Constructor","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Now that values have been asigned to the parameters, we will pass them to WaldMixture to generate the model object.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"dist = WaldMixture(ν, σ, α, τ)","category":"page"},{"location":"wald_mixture/#Simulate-Model","page":"Wald Mixture Model","title":"Simulate Model","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"rts = rand(dist, 1000)","category":"page"},{"location":"wald_mixture/#Compute-PDF","page":"Wald Mixture Model","title":"Compute  PDF","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"pdf.(dist, rts)","category":"page"},{"location":"wald_mixture/#Compute-Log-PDF","page":"Wald Mixture Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"logpdf.(dist, rts)","category":"page"},{"location":"wald_mixture/#Plot-Simulation","page":"Wald Mixture Model","title":"Plot Simulation","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"The code below overlays the PDF on reaction time histogram.","category":"page"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"histogram(dist)\nplot!(dist; t_range=range(.130, 1, length=100))","category":"page"},{"location":"wald_mixture/#References","page":"Wald Mixture Model","title":"References","text":"","category":"section"},{"location":"wald_mixture/","page":"Wald Mixture Model","title":"Wald Mixture Model","text":"Steingroever, H., Wabersich, D., & Wagenmakers, E. J. (2021). Modeling across-trial variability in the Wald drift rate parameter. Behavior Research Methods, 53, 1060-1076.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [SequentialSamplingModels]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#SequentialSamplingModels.AbstractLBA","page":"API","title":"SequentialSamplingModels.AbstractLBA","text":"AbstractLBA <: SSM2D\n\nAn abstract type for the linear ballistic accumulator model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractLCA","page":"API","title":"SequentialSamplingModels.AbstractLCA","text":"AbstractLCA <: SSM2D\n\nAn abstract type for the leaky competing accumulator model\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractLNR","page":"API","title":"SequentialSamplingModels.AbstractLNR","text":"AbstractLNR <: SSM2D\n\nAn abstract type for the lognormal race model\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractRDM","page":"API","title":"SequentialSamplingModels.AbstractRDM","text":"AbstractRDM <: SSM2D\n\nAn abstract type for the racing diffusion model.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractWald","page":"API","title":"SequentialSamplingModels.AbstractWald","text":"AbstractWald <: SSM1D\n\nAn abstract type for the Wald model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.AbstractaDDM","page":"API","title":"SequentialSamplingModels.AbstractaDDM","text":"AbstractaDDM <: SSM2D\n\nAn abstract type for the drift diffusion model.  \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.DDM","page":"API","title":"SequentialSamplingModels.DDM","text":"DDM{T<:Real} <: SSM2D\n\nModel object for the standard Drift Diffusion Model.\n\nParameters\n\nν: drift rate. Average slope of the information accumulation process. The drift gives information about the speed and direction of the accumulation of information. Typical range: -5 < ν < 5\nα: boundary threshold separation. The amount of information that is considered for a decision. Typical range: 0.5 < α < 2\nτ: non-decision time. The duration for a non-decisional processes (encoding and response execution). Typical range: 0.1 < τ < 0.5 \nz: starting point. Indicator of an an initial bias towards a decision. The z parameter is relative to a (i.e. it ranges from 0 to 1).\n\nConstructors\n\nDDM(ν, α, τ, z)\n\nDDM(; ν = 1.0,\n    α = 0.8,\n    τ = 0.3\n    z = 0.25)\n\nExample\n\nusing SequentialSamplingModels\ndist = DDM(ν = 1.0, α = 0.8, τ = 0.3, z = 0.25) \nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nRatcliff, R., & McKoon, G. (2008). The Diffusion Decision Model: Theory and Data for Two-Choice Decision Tasks. Neural Computation, 20(4), 873–922.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LBA","page":"API","title":"SequentialSamplingModels.LBA","text":"LBA{T<:Real} <: AbstractLBA\n\nA model object for the linear ballistic accumulator.\n\nParameters\n\nν: a vector of drift rates\nA: max start point\nk: A + k = b, where b is the decision threshold\nσ=fill(1.0, length(ν)): drift rate standard deviation\nτ: a encoding-response offset\n\nConstructors\n\nLBA(ν, A, k, τ, σ)\n\nLBA(;τ=.3, A=.8, k=.5, ν=[2.0,1.75], σ=[1.0,1.0])\n\nExample\n\nusing SequentialSamplingModels\ndist = LBA(ν=[3.0,2.0], A = .8, k = .2, τ = .3) \nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nBrown, S. D., & Heathcote, A. (2008). The simplest complete model of choice response time: Linear ballistic accumulation. Cognitive psychology, 57(3), 153-178.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LCA","page":"API","title":"SequentialSamplingModels.LCA","text":"LCA{T<:Real} <: AbstractLCA\n\nA model type for the Leaky Competing Accumulator. \n\nParameters\n\nν = [2.5,2.0]: drift rates \nα = 1.5: evidence threshold \nβ = .20: lateral inhabition \nλ = .10: leak rate\nτ = .30: non-decision time \nσ = 1.0: diffusion noise \nΔt = .001: time step \n\nConstructors\n\nLCA(ν, α, β, λ, τ, σ, Δt)\n\nLCA(;ν = [2.5,2.0], \n    α = 1.5, \n    β = .20, \n    λ = .10, \n    τ = .30, \n    σ = 1.0, \n    Δt = .001)\n\nExample\n\nusing SequentialSamplingModels \nν = [2.5,2.0]\nα = 1.5\nβ = 0.20\nλ = 0.10 \nσ = 1.0\nτ = 0.30\nΔt = .001\n\ndist = LCA(; ν, α, β, λ, τ, σ, Δt)\nchoices,rts = rand(dist, 500)\n\nReferences\n\nUsher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108 3, 550–592. https://doi.org/10.1037/0033-295X.108.3.550\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.LNR","page":"API","title":"SequentialSamplingModels.LNR","text":"LNR{T<:Real} <: AbstractLNR\n\nA lognormal race model object. \n\nParameters\n\nν: a vector of means in log-space\nσ: a standard deviation parameter in log-space\nτ: a encoding-response offset\n\nConstructors\n\nLNR(ν, σ, τ)\n\nLNR(;ν=[-1,-2], σ=1, τ=.20)\n\nExample\n\nusing SequentialSamplingModels\ndist = LNR(ν=[-2,-3], σ=1.0, τ=.3)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nRouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015).  The lognormal race: A cognitive-process model of choice and latency with desirable  psychometric properties. Psychometrika, 80(2), 491-513.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.RDM","page":"API","title":"SequentialSamplingModels.RDM","text":"RDM{T<:Real} <: AbstractRDM\n\nAn object for the racing diffusion model.\n\nConstructors\n\nRDM(ν, k, A, τ)\n\nRDM(;ν=[1,2], k=.3, A=.7, τ=.2)\n\nParameters\n\nν: a vector of drift rates\nk: k = b - A where b is the decision threshold, and A is the maximum starting point\nA: the maximum starting point diffusion process, sampled from Uniform distribution\nτ: a encoding-motor time offset\n\nExample\n\nusing SequentialSamplingModels\ndist = RDM(;ν=[1,2], k=.3, A=.7, τ=.2)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n\nReferences\n\nTillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability:  The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27, 911-936.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.SSM1D","page":"API","title":"SequentialSamplingModels.SSM1D","text":"SSM1D <: ContinuousUnivariateDistribution\n\nAn abstract type for sequential sampling models characterized by a single choice reaction time distribution. Sub-types of SSM1D output a vector of reaction times.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.SSM2D","page":"API","title":"SequentialSamplingModels.SSM2D","text":"SSM2D = Distribution{Multivariate, Mixed}\n\nAn abstract type for sequential sampling models characterized by a multivariate choice-reaction time distribution. Sub-types of SSM2D output a NamedTuple consisting of a vector of choices and reaction times. \n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.Wald","page":"API","title":"SequentialSamplingModels.Wald","text":"Wald{T<:Real} <: AbstractWald\n\nA model object for the Wald model, also known as the inverse Gaussian model.\n\nParameters\n\nν: drift rate\nα: decision threshold\nτ: a encoding-response offset\n\nConstructors\n\nWald(ν, α, τ)\n\nWald(;ν=1.5, α=.50, τ=0.20)\n\nExample\n\nusing SequentialSamplingModels\ndist = Wald(ν=3.0, α=.5, τ=.130)\nrt = rand(dist, 10)\nlike = pdf.(dist, rt)\nloglike = logpdf.(dist, rt)\n\nReferences\n\nAnders, R., Alario, F., & Van Maanen, L. (2016). The shifted Wald distribution for response time data analysis. Psychological methods, 21(3), 309.\n\nFolks, J. L., & Chhikara, R. S. (1978). The inverse Gaussian distribution and its statistical application—a review. Journal of the Royal Statistical Society Series B: Statistical Methodology, 40(3), 263-275.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.WaldMixture","page":"API","title":"SequentialSamplingModels.WaldMixture","text":"WaldMixture{T<:Real} <: AbstractWald\n\nParameters\n\nυ: drift rate\nη: standard deviation of drift rate\nα: decision threshold\nτ: a encoding-response offset\n\nConstructors\n\nWaldMixture(ν, η, α, τ)\n\nWaldMixture(;ν=3.0, η=.2, α=.5, τ=.130)\n\nExample\n\nusing SequentialSamplingModels\ndist = WaldMixture(;ν=3.0, η=.2, α=.5, τ=.130)\nrt = rand(dist, 10)\nlike = pdf.(dist, rt)\nloglike = logpdf.(dist, rt)\n\nReferences\n\nSteingroever, H., Wabersich, D., & Wagenmakers, E. J. (2020).  Modeling across-trial variability in the Wald drift rate parameter.  Behavior Research Methods, 1-17.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.aDDM","page":"API","title":"SequentialSamplingModels.aDDM","text":"aDDM{T<:Real} <: AbstractaDDM\n\nAn object for the attentional diffusion model. \n\nParameters\n\nν=[5.0,4.0]: relative decision values (i.e., drift rates)\nα=1.0: evidence threshold \nz=0.0: initial evidence \nθ=.3: bias towards attended alternative (lower indicates more bias)\nσ=.02: standard deviation of noise in evidence accumulation\nΔ=.0004: constant of evidence accumulation speed (evidence per ms)\nτ=0.0: non-decision time\n\nConstructors\n\naDDM(ν, α, z, θ, σ, Δ, τ)\n\naDDM(;ν=[5.0,4.0], α=1.0, z=α*.5, θ=.3, σ=.02, Δ=.0004, τ=0.0)\n\nExample\n\nusing SequentialSamplingModels\nusing StatsBase\n\nmutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\n function Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end\n \n function attend(transition)\n     (;mat,n,state) = transition\n     w = mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end\n\n model = aDDM()\n \n tmat = Transition([.98 .015 .005;\n                    .015 .98 .005;\n                    .45 .45 .1])\n\n choices,rts = rand(model, 100, attend, tmat)\n\nReferences\n\nKrajbich, I., Armel, C., & Rangel, A. (2010). Visual fixations and the computation and comparison of  value in simple choice. Nature neuroscience, 13(10), 1292-1298.\n\n\n\n\n\n","category":"type"},{"location":"api/#SequentialSamplingModels.maaDDM","page":"API","title":"SequentialSamplingModels.maaDDM","text":"maaDDM{T<:Real} <: AbstractaDDM\n\nAn object for the multi-attribute attentional drift diffusion model. \n\nConstructors\n\nmaaDDM(ν, α, z, θ, ϕ, ω, σ, Δ, τ)\n\nmaaDDM(; \n    ν = [4.0 5.0; 5.0 4.0],\n    α = 1.0, \n    z = 0.0, \n    θ = .3, \n    ϕ = .50, \n    ω = .70, \n    σ = .02, \n    Δ = .0004,\n    τ = 0.0)\n\nConstructor for multialternative attentional diffusion model object. \n\nIn this version of the model, the non-attended attribute of the non-attended alternative is doubly discounted. For example, the mean drift rate for the attribute 1 of alternative 1 is given by:\n\n    Δ * (ω * (ν[1,1] - θ * ν[2,1]) + (1 - ω) * ϕ * (ν[1,2] - θ * ν[2,2]))\n\nKeywords\n\nν = [4.0 5.0; 5.0 4.0]: drift rates where rows are alternatives and columns are attributes\nα=1.0: evidence threshold \nz=0.0: initial evidence \nθ=.3: bias away from unattended alternative (lower indicates more bias)\nϕ=.50: bias away from unattended attribute \nω=.70: attribute weight\nσ=.02: standard deviation of noise in evidence accumulation\nΔ=.0004: constant of evidence accumulation speed (evidence per ms)\nτ=0.0: non-decision time\n\nExample\n\nusing SequentialSamplingModels\nusing StatsBase\n\nmutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\n function Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end\n \n function attend(transition)\n     (;mat,n,state) = transition\n     w = mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end\n\nν = [4.0 5.0; 5.0 4.0]\nα = 1.0 \nz = 0.0\nθ = .3\nϕ = .50\nω = .70\nσ = .02\nΔ = .0004\nτ = 0.0\n\ndist = maaDDM(; ν, α, z, θ, ϕ, ω, σ, Δ, τ)\n\ntmat = Transition([.98 .015 .0025 .0025;\n                .015 .98 .0025 .0025;\n                .0025 .0025 .98 .015;\n                .0025 .0025 .015 .98])\n\n choices,rts = rand(dist, 100, attend, tmat)\n\nReferences\n\nYang, X., & Krajbich, I. (2023). A dynamic computational model of gaze and choice in multi-attribute decisions.  Psychological Review, 130(1), 52.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractLCA, Int64}","page":"API","title":"Base.rand","text":"rand(dist::AbstractLCA, n_sim::Int)\n\nGenerate n_sim random choice-rt pairs for the Leaky Competing Accumulator.\n\nArguments\n\ndist: model object for the Leaky Competing Accumulator.\nn_sim::Int: the number of simulated choice-rt pairs  \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractLCA}","page":"API","title":"Base.rand","text":"rand(dist::AbstractLCA)\n\nGenerate a random choice-rt pair for the Leaky Competing Accumulator.\n\nArguments\n\ndist: model object for the Leaky Competing Accumulator. \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractaDDM, Any, Vararg{Any}}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, dist::AbstractaDDM, fixation, args...; kwargs...)\n\nGenerate a single simulated trial from the attention diffusion model.\n\nArguments\n\nrng: a random number generator\ndist: an attentional diffusion model object\nfixation: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs...: optional positional arguments for the fixation function\n\nKeywords\n\nkwargs...: optional keyword arguments for the fixation function\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, AbstractaDDM, Int64, Any, Vararg{Any}}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, dist::AbstractaDDM, n_sim::Int, fixation, args...; rand_state! = _rand_state!, kwargs...)\n\nGenerate n_sim simulated trials from the attention diffusion model.\n\nArguments\n\nrng: a random number generator\ndist: an attentional diffusion model object\nn_sim::Int: the number of simulated trials\nfixation: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs...: optional positional arguments for the fixation function\n\nKeywords\n\nrand_state! = _rand_state!: initialize first state with equal probability \n\nkwargs...: optional keyword arguments for the fixation function\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, DDM}","page":"API","title":"Base.rand","text":"rand(dist::DDM)\n\nGenerate a random rt for the Diffusion Decision Model (negative coding)\n\nArguments\n\ndist: model object for the Diffusion Decision Model. \n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, Distributions.MultivariateDistribution{SequentialSamplingModels.Mixed}, Int64}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, d::SSM2D, N::Int)\n\nDefault method for Generating n_sim random choice-rt pairs from a sequential sampling model  with more than one choice option.\n\nArguments\n\nd::SSM2D: a 2D sequential sampling model.\nn_sim::Int: the number of simulated choices and rts  \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{Distributions.MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple}","page":"API","title":"Distributions.logpdf","text":"logpdf(d::SSM2D, data::NamedTuple)\n\nComputes the likelihood for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{Distributions.MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple}","page":"API","title":"Distributions.pdf","text":"pdf(d::SSM2D, data::NamedTuple)\n\nComputes the probability density for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{DDM}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::DDM)\n\nReturns 2 for the number of choice options\n\nArguments\n\nd::DDM: a model object for the drift diffusion model\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{Distributions.MultivariateDistribution{SequentialSamplingModels.Mixed}}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::SSM2D)\n\nReturns the number of choice options based on the length of the drift rate vector ν.\n\nArguments\n\nd::SSM2D: a sub-type of SSM2D\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.n_options-Tuple{SSM1D}","page":"API","title":"SequentialSamplingModels.n_options","text":"n_options(dist::SSM1D)\n\nReturns 1 for the number of choice options\n\nArguments\n\nd::SSM1D: a sub-type of SSM1D\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractLBA}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractLBA; n_steps=100)\n\nReturns a matrix containing evidence samples of the LBA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractLBA: a subtype of AbstractLBA\n\nKeywords\n\nn_steps=100: number of time steps at which evidence is recorded\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractLCA}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractLCA; _...)\n\nReturns a matrix containing evidence samples of the LCA decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstrctLCA: an LCA model object\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractRDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractRDM)\n\nReturns a matrix containing evidence samples of the racing diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractRDM: an racing diffusion model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{AbstractaDDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::AbstractaDDM; fixation, m_args=(), m_kwargs=())\n\nReturns a matrix containing evidence samples from a subtype of an attentional drift diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::AbstractaDDM: an drift diffusion  model object\n\nKeywords\n\nattend: a function of the visual fixation process which returns 1 for alternative    and 2 for alternative 2\nargs=(): a set of optional positional arguments for the attend function \nkwargs=(): a set of optional keyword arguments for the attend function \n\nrand_state! = _rand_state!: initialize first state with equal probability \n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{DDM}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::DDM; Δt=.001)\n\nReturns a matrix containing evidence samples of the drift diffusion model decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::DDM: an drift diffusion  model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{WaldMixture}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::WaldMixture; Δt=.001)\n\nReturns a matrix containing evidence samples of the Wald mixture decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::Wald: an Wald mixture model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#SequentialSamplingModels.simulate-Tuple{Wald}","page":"API","title":"SequentialSamplingModels.simulate","text":"simulate(model::Wald; Δt=.001)\n\nReturns a matrix containing evidence samples of the Wald decision process. In the matrix, rows  represent samples of evidence per time step and columns represent different accumulators.\n\nArguments\n\nmodel::Wald: an Wald model object\n\nKeywords\n\nΔt=.001: size of time step of decision process in seconds\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.loglikelihood-Tuple{Distributions.MultivariateDistribution{SequentialSamplingModels.Mixed}, NamedTuple}","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(d::SSM2D, data::NamedTuple)\n\nComputes the summed log likelihood for a 2D sequential sampling model. \n\nArguments\n\nd::SSM2D: an object for a 2D sequential sampling model \ndata::NamedTuple: a NamedTuple of data containing choice and reaction time \n\n\n\n\n\n","category":"method"},{"location":"wald/#Wald-Model","page":"Wald Model","title":"Wald Model","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The Wald model, also known as the inverse Gaussian, a sequential sampling model for single choice decisions. It is formally equivalent to a drift diffusion model with one decision threshold and no starting point or across trial drift rate variability.","category":"page"},{"location":"wald/#Example","page":"Wald Model","title":"Example","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"In this example, we will demonstrate how to use the Wald model in a generic single choice decision task. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random","category":"page"},{"location":"wald/#Load-Packages","page":"Wald Model","title":"Load Packages","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The first step is to load the required packages.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"using SequentialSamplingModels\nusing SSMPlots \nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"wald/#Create-Model-Object","page":"Wald Model","title":"Create Model Object","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"In the code below, we will define parameters for the LBA and create a model object to store the parameter values. ","category":"page"},{"location":"wald/#Drift-Rate","page":"Wald Model","title":"Drift Rate","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The parameter nu represents the evidence accumulation rate.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"ν = 3.0","category":"page"},{"location":"wald/#Threshold","page":"Wald Model","title":"Threshold","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The parameter alpha the amount of evidence required to make a decision.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"α = 0.50","category":"page"},{"location":"wald/#Non-Decision-Time","page":"Wald Model","title":"Non-Decision Time","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Non-decision time is an additive constant representing encoding and motor response time. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"τ = 0.130","category":"page"},{"location":"wald/#Wald-Constructor","page":"Wald Model","title":"Wald Constructor","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Now that values have been asigned to the parameters, we will pass them to Wald to generate the model object.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"dist = Wald(ν, α, τ)","category":"page"},{"location":"wald/#Simulate-Model","page":"Wald Model","title":"Simulate Model","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. ","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"rts = rand(dist, 1000)","category":"page"},{"location":"wald/#Compute-PDF","page":"Wald Model","title":"Compute  PDF","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"pdf.(dist, rts)","category":"page"},{"location":"wald/#Compute-Log-PDF","page":"Wald Model","title":"Compute Log PDF","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"logpdf.(dist, rts)","category":"page"},{"location":"wald/#Plot-Simulation","page":"Wald Model","title":"Plot Simulation","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"The code below overlays the PDF on reaction time histogram.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"histogram(dist)\nplot!(dist; t_range=range(.130, 1, length=100))","category":"page"},{"location":"wald/#References","page":"Wald Model","title":"References","text":"","category":"section"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Anders, R., Alario, F., & Van Maanen, L. (2016). The shifted Wald distribution for response time data analysis. Psychological methods, 21(3), 309.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Folks, J. L., & Chhikara, R. S. (1978). The inverse Gaussian distribution and its statistical application—a review. Journal of the Royal Statistical Society: Series B (Methodological), 40(3), 263-275.","category":"page"},{"location":"wald/","page":"Wald Model","title":"Wald Model","text":"Steingroever, H., Wabersich, D., & Wagenmakers, E. J. (2021). Modeling across-trial variability in the Wald drift rate parameter. Behavior Research Methods, 53, 1060-1076.","category":"page"},{"location":"maaDDM/#Attentional-Drift-Diffusion-Model","page":"Muti-attribute attentional drift diffusion Model","title":"Attentional Drift Diffusion Model","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The multi-attribute attentional drift diffusion model (MAADDM; Yang & Krajbich, 2023) describes how attentional processes drive drive decision making. Much like the ADDM, in the MAADDM preference for the currently attended option accrues faster than preference for non-attended options. However, the MAADDM has been extended to model shifts in attention for alternatives with two attributes. As with other sequential sampling models, the first option to hit a decision threshold determines the resulting choice and reaction time.","category":"page"},{"location":"maaDDM/#Example","page":"Muti-attribute attentional drift diffusion Model","title":"Example","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"using SequentialSamplingModels\nusing StatsBase\nusing SSMPlots \nusing Random","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"In this example, we will develope a MAADDM for binary choice and generate its predictions. Unlike many other sequential sampling models, it is necessary to specify the attentional process, or supply fixation patterns from eye tracking data. ","category":"page"},{"location":"maaDDM/#Load-Packages","page":"Muti-attribute attentional drift diffusion Model","title":"Load Packages","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The first step is to load the required packages.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"using SequentialSamplingModels\nusing StatsBase\nusing SSMPlots \n\nRandom.seed!(9854)","category":"page"},{"location":"maaDDM/#Define-Transition-Type","page":"Muti-attribute attentional drift diffusion Model","title":"Define Transition Type","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"To represent the transition of attention from one option to the other, we will definite a Transition type and constructor. The fields of the Transition type are:","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"state: an index for the current state\nn: the number of states\nmat: an ntimes n transition matrix","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The constructor accepts a transition matrix, extracts the number of states, and initializes the first state randomly with equal probability.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"mutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\nfunction Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end","category":"page"},{"location":"maaDDM/#Define-Transition-Matrix","page":"Muti-attribute attentional drift diffusion Model","title":"Define Transition Matrix","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The transition matrix is defined below in the constructor for Transition. As shown in the table below, the model's attention can be in one of three states: option 1, option 2, or non-option, which is any area except the two options.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"  Option 1  Option 1 \n  Attribute 1 Attribute 2 Attribute 1 Attribute 2\nOption 1 Attribute 1 0.980 0.015 0.0025 0.0025\n Attribute 2 0.015 0.980 0.0025 0.0025\nOption 1 Attribute 1 0.0025 0.0025 0.980 0.015\n Attribute 2 0.0025 0.0025 0.015 0.980","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The transition matrix above embodies the following assumptions:","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Once the model attends to an option, it dwells on the option for some time.\nThere is not a bias for one option over the other.\nThere is a larger chance of transitioning between attributes within the same alternative than transitioning between alternatives\nTransitions are Markovian in that they only depend on the previous state.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":" tmat = Transition([.98 .015 .0025 .0025;\n                    .015 .98 .0025 .0025;\n                    .0025 .0025 .98 .015;\n                    .0025 .0025 .015 .98])\n","category":"page"},{"location":"maaDDM/#Attend-Function","page":"Muti-attribute attentional drift diffusion Model","title":"Attend Function","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The function below generates the next attention location based on the previous location. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":" function attend(transition)\n     (;mat,n,state) = transition\n     w = mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end","category":"page"},{"location":"maaDDM/#Create-Model-Object","page":"Muti-attribute attentional drift diffusion Model","title":"Create Model Object","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The code snippets assign values to parameters of the MAADDM and create a model object.","category":"page"},{"location":"maaDDM/#Drift-Rate-Components","page":"Muti-attribute attentional drift diffusion Model","title":"Drift Rate Components","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"In the decision making task, there are two alternatives with two attributes each. This leads to four components of the drift rates: nu_11 nu_12nu_21nu_22 where the first index corresponds to alternative and the second index corresponds to attribute.  To form the drift rate, each component is weighted by non-attention bias and then a difference is computed.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"ν = [4.0 5.0; 5.0 4.0]","category":"page"},{"location":"maaDDM/#Threshold","page":"Muti-attribute attentional drift diffusion Model","title":"Threshold","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The threshold hold represents the amount of evidence required to make a decision. This parameter is typically fixed at alpha = 1.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"α = 1.0","category":"page"},{"location":"maaDDM/#Starting-Point","page":"Muti-attribute attentional drift diffusion Model","title":"Starting Point","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The starting point of the evidence accumulation process is denoted z and is typically fixed to 0.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"z = 0.0","category":"page"},{"location":"maaDDM/#Non-Attend-Bias-Alternative","page":"Muti-attribute attentional drift diffusion Model","title":"Non-Attend Bias Alternative","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The non-attend bias parameter theta determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, theta=1. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"θ = 0.30","category":"page"},{"location":"maaDDM/#Non-Attend-Bias-Attribute","page":"Muti-attribute attentional drift diffusion Model","title":"Non-Attend Bias Attribute","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The non-attend bias parameter psi determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, psi=1. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"ϕ = .50","category":"page"},{"location":"maaDDM/#Attribute-Weight","page":"Muti-attribute attentional drift diffusion Model","title":"Attribute Weight","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The parameter omega denotes the weight of the first attribute.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"ω = .70","category":"page"},{"location":"maaDDM/#Diffusion-Noise","page":"Muti-attribute attentional drift diffusion Model","title":"Diffusion Noise","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Diffusion noise, sigma represents intra-trial noise during the evidence accumulation process.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"σ = 0.02","category":"page"},{"location":"maaDDM/#Drift-Rate-Scalar","page":"Muti-attribute attentional drift diffusion Model","title":"Drift Rate Scalar","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"The drift rate scalar controls how quickly evidence accumulates for each option. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Δ = 0.0004 ","category":"page"},{"location":"maaDDM/#Model-Object","page":"Muti-attribute attentional drift diffusion Model","title":"Model Object","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Finally, we pass the parameters to the maaDDM constructor to initialize the model.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"dist = maaDDM(; ν, α, z, θ, ϕ, ω, σ, Δ)","category":"page"},{"location":"maaDDM/#Simulate-Model","page":"Muti-attribute attentional drift diffusion Model","title":"Simulate Model","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. The rand function accepts the model object, the number of simulated trials, the attend function, and the transition matrix object. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":" choices,rts = rand(dist, 10_000, attend, tmat)","category":"page"},{"location":"maaDDM/#Plot-Simulation","page":"Muti-attribute attentional drift diffusion Model","title":"Plot Simulation","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Finally, we can generate histograms of the reaction times for each decision option. ","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"m_args = (attend,tmat)\nhistogram(dist; m_args)\nplot!(dist; m_args, t_range=range(.130, 5, length=100), xlims=(0,7))","category":"page"},{"location":"maaDDM/#References","page":"Muti-attribute attentional drift diffusion Model","title":"References","text":"","category":"section"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Yang, X., & Krajbich, I. (2023). A dynamic computational model of gaze and choice in multi-attribute decisions. Psychological Review, 130(1), 52.","category":"page"},{"location":"maaDDM/","page":"Muti-attribute attentional drift diffusion Model","title":"Muti-attribute attentional drift diffusion Model","text":"Fisher, G. (2021). A multiattribute attentional drift diffusion model. Organizational Behavior and Human Decision Processes, 165, 167-182.","category":"page"},{"location":"rdm/#Racing-Diffusion-Model","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The Racing Diffusion Model (RDM; Tillman, Van Zandt, & Logan, 2020) is a sequential sampling model in which evidence for options races independently. The RDM is similar to the Linear Ballistic Accumulator, except it assumes noise occurs during the within-trial evidence accumulation process, but the drift rate is constant across trials.","category":"page"},{"location":"rdm/#Example","page":"Racing Diffusion Model (RDM)","title":"Example","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"In this example, we will demonstrate how to use the RDM in a generic two alternative forced choice task.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"using SequentialSamplingModels\nusing SSMPlots\nusing Random","category":"page"},{"location":"rdm/#Load-Packages","page":"Racing Diffusion Model (RDM)","title":"Load Packages","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"using SequentialSamplingModels\nusing SSMPlots\nusing Random\n\nRandom.seed!(8741)","category":"page"},{"location":"rdm/#Create-Model-Object","page":"Racing Diffusion Model (RDM)","title":"Create Model Object","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"In the code below, we will define parameters for the RDM and create a model object to store the parameter values.","category":"page"},{"location":"rdm/#Drift-Rates","page":"Racing Diffusion Model (RDM)","title":"Drift Rates","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The drift rates control the speed with which information accumulates. Typically, there is one drift rate per option.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"ν = [1.0,0.50]","category":"page"},{"location":"rdm/#Maximum-Starting-Point","page":"Racing Diffusion Model (RDM)","title":"Maximum Starting Point","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The starting point of each accumulator is sampled uniformly between 0A.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"A = 0.80","category":"page"},{"location":"rdm/#Threshold-Maximum-Starting-Point","page":"Racing Diffusion Model (RDM)","title":"Threshold - Maximum Starting Point","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Evidence accumulates until accumulator reaches a threshold alpha = k +A. The threshold is parameterized this way to faciliate parameter estimation and to ensure that A le alpha.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"k = 0.50","category":"page"},{"location":"rdm/#Non-Decision-Time","page":"Racing Diffusion Model (RDM)","title":"Non-Decision Time","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Non-decision time is an additive constant representing encoding and motor response time.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"τ  = 0.30","category":"page"},{"location":"rdm/#RDM-Constructor","page":"Racing Diffusion Model (RDM)","title":"RDM Constructor","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Now that values have been assigned to the parameters, we will pass them to RDM to generate the model object.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"dist = RDM(;ν, k, A, τ)","category":"page"},{"location":"rdm/#Simulate-Model","page":"Racing Diffusion Model (RDM)","title":"Simulate Model","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":" choices,rts = rand(dist, 10_000)","category":"page"},{"location":"rdm/#Compute-PDF","page":"Racing Diffusion Model (RDM)","title":"Compute PDF","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The PDF for each observation can be computed as follows:","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"pdf.(dist, choices, rts)","category":"page"},{"location":"rdm/#Compute-Log-PDF","page":"Racing Diffusion Model (RDM)","title":"Compute Log PDF","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Similarly, the log PDF for each observation can be computed as follows:","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"logpdf.(dist, choices, rts)","category":"page"},{"location":"rdm/#Plot-Simulation","page":"Racing Diffusion Model (RDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"The code below overlays the PDF on reaction time histograms for each option.","category":"page"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"histogram(dist; xlims=(0,2.5))\nplot!(dist; t_range=range(.301, 2.5, length=100))","category":"page"},{"location":"rdm/#References","page":"Racing Diffusion Model (RDM)","title":"References","text":"","category":"section"},{"location":"rdm/","page":"Racing Diffusion Model (RDM)","title":"Racing Diffusion Model (RDM)","text":"Tillman, G., Van Zandt, T., & Logan, G. D. (2020). Sequential sampling models without random between-trial variability: The racing diffusion model of speeded decision making. Psychonomic Bulletin & Review, 27, 911-936.","category":"page"},{"location":"turing_hierarchical/#Hierarchical-Models","page":"Hierarchical Models","title":"Hierarchical Models","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"In this example, we will fit a model with random factors and estimate individual parameters. This tutorial will build on the previous ones, so make sure you have followed them first. Let's start by loading all the packages and setting a reproducible seed.","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing Distributions\nusing DataFrames\nusing StatsPlots\nusing StatsModels\nusing CSV\nusing Optim\n\nRandom.seed!(6)","category":"page"},{"location":"turing_hierarchical/#Generate-Data","page":"Hierarchical Models","title":"Generate Data","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We will use the LBA distribution to simulate data for 10 participants in two conditions with 100 trials per condition (repeated measures design). The drift rates for condition A are sampled from normal distributions, and the drift rates for condition B are set by sampling a departure from (i.e., the difference with) condition A. In other words, each participant has different drift rates for condition A (the intercept, i.e., the baseline condition) and a different \"effect\" magnitude of condition B (the offset from condition A to condition B).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"# Generate data with different drifts for two conditions A vs. B\ndf = DataFrame()\nparams = DataFrame()\nfor participant in 1:10\n    # Intercept (condition A)\n    drifts = [rand(Normal(1.5, 0.2)), rand(Normal(0.5, 0.1))]\n    param = join(round.(drifts, digits=2), \", \")  # Format and save params\n    df1 = DataFrame(rand(LBA(ν=drifts, A=0.5, k=0.5, τ=0.3), 100))\n    df1[!, :condition] = repeat([\"A\"], nrow(df1))\n    df1[!, :participant] = repeat([participant], nrow(df1))\n\n    # Effect of condition B\n    drifts2 = [rand(Normal(0.5, 0.15)), rand(Normal(0.5, 0.05))]\n    param = [param, join(round.(drifts2, digits=2), \", \")]\n    df2 = DataFrame(rand(LBA(ν=drifts .+ drifts2, A=0.5, k=0.5, τ=0.3), 100))\n    df2[!, :condition] = repeat([\"B\"], nrow(df2))\n    df2[!, :participant] = repeat([participant], nrow(df1))\n\n    # Assemble and store parameters (to compare with estimation)\n    df = vcat(df, df1, df2)\n    params = vcat(params, DataFrame(permutedims(param), [:drift_intercept, :drift_condition]))\nend","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We can visualize the individual distributions for the two type of responses and for the conditions (condition A in red and B in blue).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"density(layout=(2, 1), ylims=(0, 5), xlims=(0, 3), legend=false)\nfor p in unique(df.participant)\n    for (i, cond) in enumerate([\"A\", \"B\"])\n        density!(df.rt[(df.choice.==1).&(df.condition.==cond).&(df.participant.==p)],\n            subplot=1, color=[:blue, :red][i], title=\"Choice = 1\")\n        density!(df.rt[(df.choice.==2).&(df.condition.==cond).&(df.participant.==p)],\n            subplot=2, color=[:blue, :red][i], title=\"Choice = 2\", xlabel=\"Reaction Time (s)\")\n    end\nend\nplot!()","category":"page"},{"location":"turing_hierarchical/#Model-Specification","page":"Hierarchical Models","title":"Model Specification","text":"","category":"section"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"First, we will transform our predictor data into an model matrix. This essentially transform our favor column with \"A\" and \"B\" to a binary vector.","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"We will also transform our outcome data (RTs and choice) into a list of tuples (see this example for more explanation).","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"# Format input data\nf = @formula(rt ~ 1 + condition)\nf = apply_schema(f, schema(f, df))\n_, predictors = coefnames(f)\nX = modelmatrix(f, df)\n\n# Format the data to match the input type\ndata = [(choice=df.choice[i], rt=df.rt[i]) for i in 1:nrow(df)]","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"Now, the model is a bit more complex:","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"@model function model_lba(data; min_rt=0.2, condition=nothing, participant=nothing)\n\n    # Priors for auxiliary parameters\n    A ~ truncated(Normal(0.8, 0.4), 0.0, Inf)\n    k ~ truncated(Normal(0.2, 0.2), 0.0, Inf)\n    tau ~ Uniform(0.0, min_rt)\n\n    # Priors for population-level coefficients\n    drift_intercept_1 ~ Normal(0, 1)\n    drift_intercept_2 ~ Normal(0, 1)\n    drift_condition_1 ~ Normal(0, 1)\n    drift_condition_2 ~ Normal(0, 1)\n\n    # Prior for random intercepts (requires thoughtful specification)\n    # Group-level intercepts' SD\n    drift_intercept_random_sd ~ truncated(Cauchy(0, 0.1), 0.0, Inf)\n    # Group-level intercepts\n    drift_intercept_random_1 ~ filldist(\n        Normal(0, drift_intercept_random_sd),\n        length(unique(participant))\n    )\n    drift_intercept_random_2 ~ filldist(\n        Normal(0, drift_intercept_random_sd),\n        length(unique(participant))\n    )\n\n    for i in 1:length(data)\n        # Formula for intercept\n        drifts_intercept_1 = drift_intercept_1 .+ drift_intercept_random_1[participant[i]]\n        drifts_intercept_2 = drift_intercept_2 .+ drift_intercept_random_2[participant[i]]\n\n        # Combine with condition\n        drifts_1 = drift_intercept_1 + drift_condition_1 * condition[i]\n        drifts_2 = drift_intercept_2 + drift_condition_2 * condition[i]\n        data[i] ~ LBA(; τ=tau, A=A, k=k, ν=[drifts_1, drifts_2])\n    end\nend","category":"page"},{"location":"turing_hierarchical/","page":"Hierarchical Models","title":"Hierarchical Models","text":"Note that for now, these types of model are very slow to run in Turing.","category":"page"},{"location":"turing_simple/#A-Simple-Turing-Model","page":"Simple Bayesian Model","title":"A Simple Turing Model","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"It is possible to use Turing.jl to perform Bayesian parameter estimation on models defined in SequentialSamplingModels.jl. Below, we show you how to estimate the parameters for the Linear Ballistic Accumulator (LBA) and to use it to estimate effects.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"Note that you can easily swap the LBA model from this example for other SSM models simply by changing the names of the parameters.","category":"page"},{"location":"turing_simple/#Load-Packages","page":"Simple Bayesian Model","title":"Load Packages","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"using Turing\nusing SequentialSamplingModels\nusing Random\nusing LinearAlgebra\nusing StatsPlots\nusing Random\n\nRandom.seed!(45461)","category":"page"},{"location":"turing_simple/#Generate-Data","page":"Simple Bayesian Model","title":"Generate Data","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"We will use the LBA distribution to simulate data (100 trials) with fixed parameters (those we want to recover only from the data using Bayesian modeling).","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Generate some data with known parameters\ndist = LBA(ν=[3.0, 2.0], A = .8, k = .2, τ = .3)\ndata = rand(dist, 100)","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The rand() function will sample random draws from the distribution, and store that into a named tuple of 2 vectors (one for choice and one for rt). The individual vectors can be accessed by their names using data.choice and data.rt.","category":"page"},{"location":"turing_simple/#Specify-Turing-Model","page":"Simple Bayesian Model","title":"Specify Turing Model","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The code snippet below defines a model in Turing. The model function accepts a tuple containing a vector of choices and a vector of reaction times. The sampling statements define the prior distributions for each parameter. The non-decision time parameter tau must be founded by the minimum reaction time, min_rt. The last sampling statement defines the likelihood of the data given the sampled parameter values.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Specify LBA model\n@model function model_lba(data; min_rt = minimum(data.rt))\n    # Priors\n    ν ~ MvNormal(zeros(2), I * 2)\n    A ~ truncated(Normal(.8, .4), 0.0, Inf)\n    k ~ truncated(Normal(.2, .2), 0.0, Inf)\n    τ  ~ Uniform(0.0, min_rt)\n\n    # Likelihood\n    data ~ LBA(;ν, A, k, τ )\nend","category":"page"},{"location":"turing_simple/#Estimate-the-Parameters","page":"Simple Bayesian Model","title":"Estimate the Parameters","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"Finally, we perform parameter estimation with sample(), which takes the model, and details about the sampling algorithm:","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"model(data): the Turing model with data passed\nNUTS(1000, .65): a sampler object for the No U-Turn Sampler for 1000 warmup samples.\nMCMCThreads(): instructs Turing to run each chain on a separate thread\nn_iterations: the number of iterations performed after warmup\nn_chains: the number of chains","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Estimate parameters\nchain = sample(model_lba(data), NUTS(1000, .85), MCMCThreads(), 1000, 4)","category":"page"},{"location":"turing_simple/#Posterior-Summary","page":"Simple Bayesian Model","title":"Posterior Summary","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"We can compute a description of the posterior distributions.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Summarize posteriors\nsummarystats(chain)","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"As you can see, based on the mean values of the posterior distributions, the original parameters (ν=[3.0, 2.0], A = .8, k = .2, τ = .3) are successfully recovered from the data (the accuracy would increase with more data).","category":"page"},{"location":"turing_simple/#Evaluation","page":"Simple Bayesian Model","title":"Evaluation","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"It is important to verify that the chains converged. We see that the chains converged according to hatr leq 105, and the trace plots below show that the chains look like \"hairy caterpillars\", which indicates the chains did not get stuck. As expected, the posterior distributions are close to the data generating parameter values.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"plot(chain)","category":"page"},{"location":"turing_simple/#Posterior-Predictive-Distribution","page":"Simple Bayesian Model","title":"Posterior Predictive Distribution","text":"","category":"section"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"The next step is to generate predictions from the posterior distributions. For this, we need to pass a dataset with empty (missing) values (so that Turing knows what to predict).","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"We can then use the predict() method to generate predictions from this model. However, because the most of SequentialSamplingModels distributions return a tuple (choice and RT), the predicted output has the two types of variables mixed together. We can delineate the two by taking every 2nd values to get the predicted choice and RTs, respectively.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"predictions = predict(model_lba(missing; min_rt = minimum(data.rt)), chain)\n\npred_choice = Array(predictions)[:, 1:2:end]\npred_rt = Array(predictions)[:, 2:2:end]","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"In the following code block, we plot the predictive distributions for each choice.","category":"page"},{"location":"turing_simple/","page":"Simple Bayesian Model","title":"Simple Bayesian Model","text":"# Get RTs for option 1 and 2\nrts1 = pred_rt[pred_choice .== 1]\nrts2 = pred_rt[pred_choice .== 2]\n\n# Specify plot layout\nhistogram(layout=(2, 1), xlabel=\"RT\", ylabel=\"Density\", legend=false, xlims=(0, 1), ylim=(0, 150))\n# Add data\nhistogram!(rts1, subplot=1, color=:grey, norm=false, title=\"Choice 1\", bins=0:0.01:1)\nhistogram!(rts2, subplot=2, color=:grey, norm=false, title=\"Choice 2\", bins=0:0.01:1)","category":"page"},{"location":"aDDM/#Attentional-Drift-Diffusion-Model","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion Model","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The attentional drift diffusion model (ADDM; Krajbich, Armel, & Rangel, 2010) describes how attentional processes drive drive decision making. In the ADDM, preference for the currently attended option accrues faster than preference for non-attended options. As with other sequential sampling models, the first option to hit a decision threshold determines the resulting choice and reaction time.","category":"page"},{"location":"aDDM/#Example","page":"Attentional Drift Diffusion (aDDM)","title":"Example","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"using SequentialSamplingModels\nusing StatsBase\nusing SSMPlots\nusing Random","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"In this example, we will develope a ADDM for binary choice and generate its predictions. Unlike many other sequential sampling models, it is necessary to specify the attentional process, or supply fixation patterns from eye tracking data. ","category":"page"},{"location":"aDDM/#Load-Packages","page":"Attentional Drift Diffusion (aDDM)","title":"Load Packages","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The first step is to load the required packages.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"using SequentialSamplingModels\nusing StatsBase\nusing SSMPlots\n\nRandom.seed!(5487)","category":"page"},{"location":"aDDM/#Define-Transition-Type","page":"Attentional Drift Diffusion (aDDM)","title":"Define Transition Type","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"To represent the transition of attention from one option to the other, we will definite a Transition type and constructor. The fields of the Transition type are:","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"state: an index for the current state\nn: the number of states\nmat: an ntimes n transition matrix","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The constructor accepts a transition matrix, extracts the number of states, and initializes the first state randomly with equal probability.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"mutable struct Transition\n    state::Int \n    n::Int\n    mat::Array{Float64,2} \n end\n\nfunction Transition(mat)\n    n = size(mat,1)\n    state = rand(1:n)\n    return Transition(state, n, mat)\n end","category":"page"},{"location":"aDDM/#Define-Transition-Matrix","page":"Attentional Drift Diffusion (aDDM)","title":"Define Transition Matrix","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The transition matrix is defined below in the constructor for Transition. As shown in the table below, the model's attention can be in one of three states: option 1, option 2, or non-option, which is any area except the two options. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" option 1 option 2 non-option\noption 1 0.98 0.015 0.005\noption 2 0.015 0.98 0.005\nnon-option 0.45 0.45 0.10","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The transition matrix above embodies the following assumptions:","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Once the model attends to an option, it dwells on the option for some time.\nThere is not a bias for one option over the other.\nThe chance of fixating on a non-option is small, and such fixations are brief when they do occur.\nTransitions are Markovian in that they only depend on the previous state.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"tmat = Transition([.98 .015 .005;\n                    .015 .98 .005;\n                    .45 .45 .1])","category":"page"},{"location":"aDDM/#Attend-Function","page":"Attentional Drift Diffusion (aDDM)","title":"Attend Function","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The function below generates the next attention location based on the previous location. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" function attend(transition)\n     (;mat,n,state) = transition\n     w = mat[state,:]\n     next_state = sample(1:n, Weights(w))\n     transition.state = next_state\n     return next_state\n end","category":"page"},{"location":"aDDM/#Create-Model-Object","page":"Attentional Drift Diffusion (aDDM)","title":"Create Model Object","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The code snippets assign values to parameters of the ADDM and create a model object.","category":"page"},{"location":"aDDM/#Drift-Rate-Components","page":"Attentional Drift Diffusion (aDDM)","title":"Drift Rate Components","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The ADDM has two drift rates components corresponding to the utlity of each option. To form the drift rate, each component is weighted by non-attention bias and then a difference is computed.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"ν = [6.0,5.0]","category":"page"},{"location":"aDDM/#Threshold","page":"Attentional Drift Diffusion (aDDM)","title":"Threshold","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The threshold hold represents the amount of evidence required to make a decision. This parameter is typically fixed at alpha = 1.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"α = 1.0","category":"page"},{"location":"aDDM/#Starting-Point","page":"Attentional Drift Diffusion (aDDM)","title":"Starting Point","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The starting point of the evidence accumulation process is denoted z and is typically fixed to 0.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"z = 0.0","category":"page"},{"location":"aDDM/#Non-Attend-Bias","page":"Attentional Drift Diffusion (aDDM)","title":"Non-Attend Bias","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The non-attend bias parameter theta determines how much the non-attended option contributes to the  evidence accumulation process. In the standard DDM, theta=1. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"θ = 0.30","category":"page"},{"location":"aDDM/#Diffusion-Noise","page":"Attentional Drift Diffusion (aDDM)","title":"Diffusion Noise","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Diffusion noise, sigma represents intra-trial noise during the evidence accumulation process.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"σ = 0.02","category":"page"},{"location":"aDDM/#Drift-Rate-Scalar","page":"Attentional Drift Diffusion (aDDM)","title":"Drift Rate Scalar","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"The drift rate scalar controls how quickly evidence accumulates for each option. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Δ = 0.0004 ","category":"page"},{"location":"aDDM/#Model-Object","page":"Attentional Drift Diffusion (aDDM)","title":"Model Object","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Finally, we pass the parameters to the aDDM constructor to initialize the model.","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" model = aDDM(; ν, α, z, θ, σ, Δ)","category":"page"},{"location":"aDDM/#Simulate-Model","page":"Attentional Drift Diffusion (aDDM)","title":"Simulate Model","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Now that the model is defined, we will generate 10000 choices and reaction times using rand. The rand function accepts the model object, the number of simulated trials, the attend function, and the transition matrix object. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":" choices,rts = rand(model, 10_000, attend, tmat)","category":"page"},{"location":"aDDM/#Plot-Simulation","page":"Attentional Drift Diffusion (aDDM)","title":"Plot Simulation","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Finally, we can generate histograms of the reaction times for each decision option. ","category":"page"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"m_args = (attend,tmat)\nhistogram(model; m_args)\nplot!(model; m_args, t_range=range(0.0, 5, length=100), xlims=(0,5))","category":"page"},{"location":"aDDM/#References","page":"Attentional Drift Diffusion (aDDM)","title":"References","text":"","category":"section"},{"location":"aDDM/","page":"Attentional Drift Diffusion (aDDM)","title":"Attentional Drift Diffusion (aDDM)","text":"Krajbich, I., Armel, C., & Rangel, A. (2010). Visual fixations and the computation and comparison of value in simple choice. Nature neuroscience, 13(10), 1292-1298.","category":"page"},{"location":"plotting/#Plotting","page":"Plotting","title":"Plotting","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"SSMPlots.jl contains plotting functionality for sequential sampling models (SSMs). The code block below provides a simple example of plotting the predictions of SSMs:","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using SequentialSamplingModels\nusing SSMPlots \n\ndist = RDM(;ν=[1,2,3], k=.30, A=.70, τ=.20)\nhistogram(dist)\nplot!(dist)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"More details on plotting SSMs can be found in the documentation of SSMPlots.jl.","category":"page"},{"location":"developer_guide/#Style-Guide","page":"Developer Guide","title":"Style Guide","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"In most cases, code written in SequentialSamplingModels.jl follows the guidelines specified in the blue style guide for Julia. Please use the blue style guide or existing code as a guide, and deviate from the guides only when there is a compelling reason to do so.","category":"page"},{"location":"developer_guide/#Documentation","page":"Developer Guide","title":"Documentation","text":"","category":"section"},{"location":"developer_guide/#Docstrings","page":"Developer Guide","title":"Docstrings","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide docstrings for methods and types which are part of the API. For example, the doc strings for each model should adhere to the following format:","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"    LNR{T<:Real} <: SSM2D\n\nA lognormal race model object. \n\n# Parameters \n\n- `μ`: a vector of means in log-space\n- `σ`: a standard deviation parameter in log-space\n- `ϕ`: a encoding-response offset\n\n# Constructors\n\n    LNR(μ, σ, ϕ)\n\n    LNR(;μ, σ, ϕ)\n\n# Example\n\n```julia\nusing SequentialSamplingModels\ndist = LNR(μ=[-2,-3], σ=1.0, ϕ=.3)\nchoice,rt = rand(dist, 10)\nlike = pdf.(dist, choice, rt)\nloglike = logpdf.(dist, choice, rt)\n```\n# References\n\nRouder, J. N., Province, J. M., Morey, R. D., Gomez, P., & Heathcote, A. (2015). \nThe lognormal race: A cognitive-process model of choice and latency with desirable \npsychometric properties. Psychometrika, 80(2), 491-513.","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"For the benefit of other developers, err on the side of providing doc strings for internal methods. The doc strings should provide the function signature, a high level explanation of the function, and a description of arguments and keywords. Please include references as appropriate. ","category":"page"},{"location":"developer_guide/#Model-Example","page":"Developer Guide","title":"Model Example","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide a detailed model walk through for the online documentation under the section Models. The walk through should include a description of the model, an explanation of the model parameters, and a demonstration showing the pdf overlayed on the histogram (if applicable). Please use existing model examples as a template. ","category":"page"},{"location":"developer_guide/#API","page":"Developer Guide","title":"API","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Only export (make public) types and methods that are intended for users. Other methods are implementational details for interal use. ","category":"page"},{"location":"developer_guide/#Unit-tests","page":"Developer Guide","title":"Unit tests","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"Provide unit tests for most (if not all) methods. When possible, programatically test a method over a wide range of inputs. If you find a bug, write a unit test for the bug to prevent regressions. When possible, compare methods to those defined in established and trusted packages in other languages.  ","category":"page"},{"location":"developer_guide/#Parameter-Naming-Conventions","page":"Developer Guide","title":"Parameter Naming Conventions","text":"","category":"section"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"To ensure consistency across models, please use the following variable names:","category":"page"},{"location":"developer_guide/","page":"Developer Guide","title":"Developer Guide","text":"use ν for drift rates\nuse α for decision boundaries\nuse Δt for a discrete time step\nuse σ for within-trial noise of drift rate  \nuse τ for non-decision time\nuse z for evidence starting point\nuse η for across-trial noise of drift rate","category":"page"},{"location":"#SequentialSamplingModels.jl","page":"Home","title":"SequentialSamplingModels.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sequential sampling models (SSM), also known as an evidence accumulation models, are a broad class of dynamic models of human decision making in which evidence for each option accumulates until the evidence for one option reaches a decision threshold. Models within this class make different assumptions about the nature of the evidence accumulation process (see the references below for a broad overview).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Despite their usefulness in psychology, models such as Drift-Diffusion Models and their variants are notoriously hard to implement, with packages such as Python's HDDM and PyDDM, or R's fddm, sometimes lacking coverage (implementing only specific model subtypes) or flexibility (hard to use in bespoke real-life cases).","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides a unified interface for all the popular sequential sampling models (such as DDM, LBA, LNR, LCA, ...) in Julia, based on the Distributions.jl API, that can be used with Turing for Bayesian estimation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"An example of the evidence accumulation process is illustrated below for the Leaking Competing Accumulator (LCA):","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Plots\nusing Random\nusing Colors\nusing SequentialSamplingModels\nusing SequentialSamplingModels: increment!\nRandom.seed!(8437)\n\nparms = (α = 1.5,\n            β=0.20,\n             λ=0.10,\n             ν=[2.5,2.0],\n             Δt=.001,\n             τ=.30,\n             σ=1.0)\nmodel = LCA(; parms...)\ntime_steps,evidence = simulate(model)\nlca_plot = plot(time_steps, evidence, xlabel=\"Time (seconds)\", ylabel=\"Evidence\",\n    label=[\"option1\" \"option2\"], ylims=(0, 2.0), grid=false, linewidth = 2,\n    color =[RGB(148/255, 90/255, 147/255) RGB(90/255, 112/255, 148/255)])\nhline!(lca_plot, [model.α], color=:black, linestyle=:dash, label=\"threshold\", linewidth = 2)\nsavefig(\"lca_plot.png\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install a stable version of SequentialSamplingModels by running the following in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add SequentialSamplingModels","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package can then be loaded with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SequentialSamplingModels","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package implements sequential sampling models as distributions, that we can use you estimate the likelihood, or generate data from. In the example below, we instantiate a Linear Ballistic Accumulator (LBA) model, and generate data from it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SequentialSamplingModels\nusing StatsPlots\nusing Random\n\nRandom.seed!(2054)\n\n# Create LBA distribution with known parameters\ndist = LBA(; ν=[2.75,1.75], A=0.8, k=0.5, τ=0.25)\n# Sample 10,000 random data points from this distribution\nchoice, rt = rand(dist, 10_000)\n\n# Plot the RT distribution for each choice\nhistogram(layout=(2, 1), xlabel=\"Reaction Time\", ylabel=\"Frequency\", xlims = (0,1),\n    grid=false, ylims = (0, 650))\nhistogram!(rt[choice.==1], subplot=1, color=:grey, leg=false, bins=200)\nhistogram!(rt[choice.==2], subplot=2, color=:grey, leg=false, bins=200)","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Evans, N. J. & Wagenmakers, E.-J. Evidence accumulation models: Current limitations and future directions. Quantitative Methods for Psychololgy 16, 73–90 (2020).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Forstmann, B. U., Ratcliff, R., & Wagenmakers, E. J. (2016). Sequential sampling models in cognitive neuroscience: Advantages, applications, and extensions. Annual Review of Psychology, 67, 641-666.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Jones, M., & Dzhafarov, E. N. (2014). Unfalsifiability and mutual translatability of major modeling schemes for choice reaction time. Psychological Review, 121(1), 1.","category":"page"}]
}
